{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNkbX214UvbxxAsmyb3riN5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##**COURS INTERACTIF**\n","###**Génération de langage naturel en Python**\n","+ *4 heures*\n","+ *13 vidéos*\n","+ *52 exercices*\n","\n","####**Description du cours**\n","+ Vous êtes-vous déjà demandé comment Gmail autocomplète vos phrases ou comment fonctionnent les suggestions de WhatsApp lorsque vous tapez un message ? \n","\n","+ La technologie derrière ces conseils d'écriture utiles est l'apprentissage automatique. \n","\n","+ Dans ce cours, vous allez construire et entraîner des modèles d'apprentissage automatique pour différentes tâches de génération de langage naturel. \n","\n","+ Par exemple, vous formerez un modèle sur les œuvres littéraires de Shakespeare et générerez du texte dans le style de son écriture. \n","\n","+ Vous apprendrez également à créer un modèle de traduction neuronal pour traduire des phrases anglaises en français. \n","\n","+ Enfin, vous formerez un modèle seq2seq pour générer vos propres phrases autocomplétées en langage naturel, tout comme Gmail !\n","\n","####**1) Introduction aux données séquentielles**\n","\n","+ L'ordre des mots dans les phrases est important (sauf si on vous appelle Yoda).\n","\n","+ C'est pourquoi dans ce chapitre, vous apprendrez à représenter vos données de manière séquentielle et à utiliser l'architecture des réseaux de neurones pour modéliser vos données textuelles.\n","\n","+ Vous apprendrez à créer et à entraîner un réseau récurrent pour générer un nouveau texte, caractère par caractère.\n","\n","+ Vous utiliserez également le jeu de données de noms pour créer votre propre générateur de noms de bébés, à l'aide d'un réseau neuronal récurrent très simple et du package Keras.\n","\n","\n","|OBJECTIFS|\n","|---------|\n","Traitement des données séquentielles\n","Prétraitement de l'ensemble de données sur les noms\n","Prétraitement du jeu de données des noms (suite)\n","Introduction au réseau de neurones récurrent\n","Créer des tenseurs d'entrée et de cible\n","Initialiser les vecteurs d'entrée et de cible avec des valeurs\n","Construire et compiler un réseau RNN\n","Inférence à l'aide d'un réseau neuronal récurrent\n","Entraînement du modèle RNN et début des prédictions\n","Générer des noms de bébé\n","\n","####**2) Écrire comme Shakespeare**\n","+ Dans ce chapitre, vous découvrirez comment surmonter les limites des réseaux neuronaux récurrents lorsque les séquences d'entrée couvrent de longs intervalles. \n","\n","+ Pour éviter les problèmes de disparition et d'explosion du gradient, vous découvrirez les réseaux à mémoire à long terme (LSTM), qui sont plus efficaces lorsqu'ils travaillent avec des dépendances à long terme. \n","\n","+ Vous travaillerez sur un projet amusant où vous construirez et entraînerez un modèle LSTM simple en utilisant des œuvres littéraires sélectionnées de Shakespeare pour générer un nouveau texte dans le style d'écriture unique de Shakespeare.\n","\n","|OBJECTIFS|\n","|---------|\n","Limites des réseaux neuronaux récurrents\n","Gradients disparaissants et explosifs\n","Réseau simple utilisant Keras\n","Gradients évanescents\n","Introduction à la mémoire à long terme et à court terme\n","Vocabulaire et correspondance entre caractères et nombres entiers\n","Ensemble de données d'entrée et de cible\n","Créer et initialiser les vecteurs d'entrée et de cible\n","Création d'un modèle LSTM dans keras\n","Inférence utilisant la mémoire à long terme\n","Entraînement du modèle LSTM\n","Prédire le caractère suivant à partir d'une séquence\n","Générer un texte imitant Shakespeare.\n","\n","####**3) Traduire des mots dans une autre langue**\n","\n","+ Dans ce chapitre, vous découvrirez l'architecture de l'encodeur-décodeur et la manière dont elle peut être utilisée pour modéliser des ensembles de données de séquence à séquence, en convertissant les informations d'un domaine à un autre. \n","\n","+ Vous utiliserez ces connaissances pour construire un modèle de traduction automatique neuronale, en entraînant votre modèle à traduire des phrases anglaises en français.\n","\n","\n","|OBJECTIFS|\n","|---------|\n","Introduction aux modèles de séquence à séquence\n","Créer l'eng-fra le jeu de données\n","Obtenir les vocabulaires\n","Mappage de caractères en entiers et vice-versa\n","Traduction automatique neuronale\n","Définir les vecteurs d'entrée et de cible\n","Initialiser les vecteurs d'entrée et de cible\n","Construction de l'encodeur et du décodeur\n","Entraîner le réseau de l'encodeur et du décodeur\n","Modèle d'inférence pour l'encodeur et le décodeur\n","Construire des modèles d'inférence pour l'encodeur et le décodeur\n","Prédire le premier caractère\n","Prédire le second caractère\n","Générer une phrase entièrement traduite\n","\n","####**4) Autocomplétion de vos phrases**\n","\n","+ Dans ce chapitre, vous allez construire votre propre modèle d'apprentissage automatique seq2seq. \n","\n","+ Vous utiliserez des messages réels de l'ensemble de données de courriers électroniques Enron pour entraîner un modèle encodeur-décodeur. \n","\n","+ Ce modèle vous permettra de prédire la fin correcte d'une phrase incomplète.\n","\n","|OBJECTIFS|\n","|---------|\n","Conversion des données d'e-mail en seq2seq\n","Divisez les phrases en préfixes et suffixes.\n","Créez le vocabulaire et les mappings\n","Définir les vecteurs d'entrée et de cible\n","Initialiser les vecteurs d'entrée et de cible\n","Autocomplétion de phrases à l'aide de l'encodeur-décodeur\n","Construction de l'encodeur\n","Construire le décodeur\n","Entraîner l'encodeur et le décodeur\n","Autocomplétion de phrases à l'aide de modèles d'inférence\n","Construction des modèles d'inférence\n","Prédire le premier caractère à l'aide de modèles d'inférence\n","Prédire le deuxième caractère\n","Phrases autocomplétées\n","\n"],"metadata":{"id":"QHSXM5eD_oH2"}},{"cell_type":"markdown","source":["###**Traitement des données séquentielles**\n","\n","####**1. Traitement des données séquentielles**\n","+ Salut. Je m’appelle Biswanath. \n","\n","+ Je suis un scientifique des données ayant environ neuf ans d’expérience pertinente dans l’industrie dans des entreprises comme Oracle, Microsoft et Adobe. \n","\n","+ ***Bienvenue au cours sur la génération de langage naturel qui est un sous-thème du traitement du langage naturel, ou NLP***. \n","\n","+ **La génération de langage naturel traite des tâches qui génèrent automatiquement des textes.**\n","\n","####**2. Génération de langage naturel**\n","\n","+ ***Les systèmes de génération de langage naturel peuvent produire du texte pour différentes applications, par exemple, la génération de phrases dans un certain style, la traduction automatique qui n’est rien d’autre que la génération de texte dans une langue différente, l’auto-complétion d’une phrase donnée à une partie de cette phrase en entrée, la génération de résumés textuels, les chatbots automatisés, etc. Dans ce cours, vous apprendrez à créer des systèmes pour certaines de ces tâches.***\n","\n","####**3. Introduction aux données séquentielles**\n","+ Ce chapitre présente les données séquentielles et les façons d’utiliser ces données pour la génération de texte. \n","\n","+ Tout au long de ce chapitre, vous utiliserez également ce que vous avez appris pour générer des noms de bébé à partir de zéro. \n","\n","+ Les données séquentielles sont tout type de données où la commande compte. Des exemples de ceux-ci pourraient être des données de séries chronologiques, des données textuelles provenant de documents, des séquences d’ADN, etc. Nous traiterons des données textuelles dans ce cours.\n","\n","####**4. Données textuelles ou linguistiques**\n","\n","+ Les données textuelles font référence à toutes les données utilisées dans la langue parlée ou écrite et l’ordre dans lequel les mots apparaissent est important. \n","\n","+ Par exemple, considérez cette phrase « J’apprends les mathématiques » où chaque mot a sa place et si l’ordre est changé au hasard, le sens change ou cela pourrait devenir quelque chose de complètement absurde. \n","\n","+ Ainsi, chaque mot d’une phrase dépend de tous les mots précédents utilisés. \n","\n","+ Cela est également vrai pour les personnages.\n","\n","####**5. Un exemple de jeu de données textuel**\n","\n","+ Passons maintenant en revue un exemple de jeu de données textuel séquentiel. \n","\n","+ L’ensemble de données que nous utiliserons tout au long de ce chapitre est le jeu de données de noms qui contient les noms de personnes où chaque mot est un nom. \n","\n","+ Les noms sont indépendants, mais les caractères à l’intérieur des noms sont ordonnés. \n","\n","+ Chaque nom peut être considéré comme une séquence ordonnée de caractères qui suit un modèle inconnu. \n","\n","+ **Plus formellement, la séquence de caractères suit une certaine distribution de probabilité qui ne nous est pas connue**. \n","\n","+ **Notre objectif est de deviner cette distribution à partir des noms existants et de générer de nouveaux noms similaires aux noms de cet ensemble de données.**\n","\n","####**6. Jeu de données de noms**\n","+ Le jeu de données de noms est un DataFrame avec une seule colonne ayant un nom dans chaque ligne, comme illustré ici.\n","\n","####**7. Délimiteurs de mots**\n","\n","+ ***Notre objectif est d’entraîner un modèle qui prédira un nouveau personnage en fonction d’un ensemble de caractères en entrée.*** \n","\n","+ Ainsi, le modèle doit comprendre quand un nom commence et se termine. \n","\n","+ Vous pouvez utiliser des caractères spéciaux qui ne sont utilisés dans aucun nom de ce jeu de données pour marquer le début et la fin. \n","\n","+ Ceux-ci sont appelés respectivement le jeton de début et de fin. \n","\n","+ Nous utiliserons le caractère 'tab' et le caractère 'newline' à cette fin.\n","\n","####**8. Insérer le jeton de démarrage**\n","+ Le jeton de démarrage peut être ajouté au début de chaque nom à l’aide d’une fonction lambda, comme illustré ici.\n","\n","###**9. Ajouter un jeton de fin**\n","+ Le jeton de fin peut être ajouté de la même manière.\n","\n","####**10. Vocabulaire pour l’ensemble de données de noms**\n","+ Les modèles d’apprentissage automatique traitent des chiffres. \n","\n","+ Nous devons donc convertir ces séquences de caractères en représentations entières appropriées. \n","\n","+ Pour cela, nous devons créer le vocabulaire qui est un ensemble de tous les caractères uniques utilisés dans tous les noms. \n","\n","+ Nous n’avons que des lettres minuscules dans notre jeu de données de noms. \n","\n","+ Notre vocabulaire se compose donc de toutes les lettres minuscules plus le jeton de début et de fin. \n","\n","+ **La fonction get_vocabulary()** ici est d’itérer sur tous les caractères de chaque nom et d’ajouter le caractère au vocabulaire s’il n’est pas déjà là.\n","\n","####**11. Mappage caractère-entier**\n","\n","+ Une astuce pour mapper ces caractères à des entiers est de trier le vocabulaire et d’attribuer des nombres dans l’ordre. \n","\n","+ Ainsi, le caractère de tabulation peut être mappé à 0, la nouvelle ligne peut être mappée à 1, 'a' peut être mappé à 2, b' peut être mappé à 3 et ainsi de suite. \n","\n","+ Nous pouvons utiliser la fonction triée sur le vocabulaire pour obtenir une liste triée de caractères. \n","\n","+ Ensuite, nous pouvons énumérer sur la liste triée qui générera des tuples de paires d’index et de caractères.\n","\n","+ Cela peut être enregistré dans un dictionnaire comme indiqué.\n","\n","####**12. Mappage d’entiers à caractères**\n","+ Nous pouvons également enregistrer le mappage inverse où 2 est mappé à a, 3 est mappé à b et ainsi de suite dans un autre dictionnaire.\n","\n","####**13. Entraînons-nous!**\n","+ Maintenant que vous avez appris à prétraiter les données textuelles, mettons-les en pratique."],"metadata":{"id":"wYbv_UTfD0AX"}},{"cell_type":"markdown","source":["###**EXERCICE**\n","\n","####**Prétraitement du jeu de données des noms**\n","\n","\n","+ Pour générer des noms de bébés à partir de rien, il faut disposer d'un système qui génère rapidement des textes courts.\n","\n","+ Ces textes doivent avoir un style unique et pourraient effectivement servir de noms pour les nouveau-nés. \n","\n","+ Dans cet exercice, vous allez commencer à prétraiter un jeu de données de noms de personnes afin de pouvoir l'utiliser pour entraîner un tel système.\n","\n","+ Le jeu de données est déjà importé dans un DataFrame pandas appelé **names_df** qui possède une colonne en entrée contenant les noms uniques en lettres minuscules. \n","\n","+ Vous ajouterez le jeton de début au début de chaque nom et mettrez à jour cette colonne en place. \n","\n","+ Vous allez également créer une autre colonne dans laquelle le jeton de fin sera ajouté à chaque nom. numpy et pandas sont importés sous les noms np et pd.\n","\n","####**Instructions**\n","\n","+ Insérez une tabulation \\t devant chaque nom de l'ensemble de données.\n","+ Ajoutez une nouvelle ligne \\n à la fin de chaque nom de l'ensemble de données."],"metadata":{"id":"Qe0bnAAh899B"}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"3cns614TCgdg","executionInfo":{"status":"ok","timestamp":1672912231967,"user_tz":-60,"elapsed":311,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["names_df = pd.read_csv(\"/content/names.txt\")"],"metadata":{"id":"2IlzIhj2-aBw","executionInfo":{"status":"ok","timestamp":1672912233500,"user_tz":-60,"elapsed":306,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["names_df.rename(columns = {\"John\" : \"input\"}, inplace = True)"],"metadata":{"id":"g2hnAQfMClOQ","executionInfo":{"status":"ok","timestamp":1672912234665,"user_tz":-60,"elapsed":3,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["names_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"hAcvNiPZVYsn","executionInfo":{"status":"ok","timestamp":1672912236835,"user_tz":-60,"elapsed":262,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"36eee0cf-8a0e-4cf2-dfd0-d7fb35b6415c"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     input\n","0  William\n","1    James\n","2  Charles\n","3   George\n","4    Frank"],"text/html":["\n","  <div id=\"df-acf38f4f-07d9-486b-b708-85a19b6d69b2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>William</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>James</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Charles</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>George</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Frank</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acf38f4f-07d9-486b-b708-85a19b6d69b2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-acf38f4f-07d9-486b-b708-85a19b6d69b2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-acf38f4f-07d9-486b-b708-85a19b6d69b2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Insert a tab in front of all the names\n","names_df['input'] = names_df['input'].apply(lambda x : '\\t' + x)\n","\n","# Append a newline at the end of every name\n","# We already appended a tab in front, so the target word should start at index 1\n","names_df['target'] = names_df['input'].apply(lambda x : x[1:len(x)] + '\\n')"],"metadata":{"id":"cN0ZZdlOFSzm","executionInfo":{"status":"ok","timestamp":1672912240574,"user_tz":-60,"elapsed":313,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["names_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"XyoOI5E3C5kP","executionInfo":{"status":"ok","timestamp":1672912243030,"user_tz":-60,"elapsed":7,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"a0dbc226-483f-4fbc-da63-30f0f8426467"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       input     target\n","0  \\tWilliam  William\\n\n","1    \\tJames    James\\n\n","2  \\tCharles  Charles\\n\n","3   \\tGeorge   George\\n\n","4    \\tFrank    Frank\\n"],"text/html":["\n","  <div id=\"df-eb22e946-9443-4955-9a51-258be4b66010\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\\tWilliam</td>\n","      <td>William\\n</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\\tJames</td>\n","      <td>James\\n</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\\tCharles</td>\n","      <td>Charles\\n</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\\tGeorge</td>\n","      <td>George\\n</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\\tFrank</td>\n","      <td>Frank\\n</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb22e946-9443-4955-9a51-258be4b66010')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-eb22e946-9443-4955-9a51-258be4b66010 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-eb22e946-9443-4955-9a51-258be4b66010');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["####**Prétraitement du jeu de données des noms (suite)**\n","+ Vous avez maintenant un DataFrame avec deux colonnes contenant les noms avec les tokens de début et de fin ajoutés. \n","\n","+ L'étape suivante consiste à encoder ces noms sous forme de valeurs numériques, car les modèles d'apprentissage automatique n'acceptent que les entrées numériques.\n","\n","+ Dans cet exercice, vous créerez deux dictionnaires, **char_to_idx et idx_to_char**, qui contiendront les correspondances entre les caractères et les entiers, par exemple, \n","    + {'\\t' : 0, '\\n' : 1, 'a' : 2, 'b' : 3, ...} \n","et les correspondances inverses des entiers aux caractères, par exemple \n","  + {0 : '\\t', 1 : '\\n', 2 : 'a', 3 : 'b', ...}.\n","\n","+ *L'ensemble de données est disponible dans names_df.*\n","\n","+ *Nous avons également défini une fonction d'aide get_vocabulary() qui prend une liste de mots en entrée et renvoie le vocabulaire qui est l'ensemble de tous les caractères disponibles dans le jeu de données. Nous avons utilisé cette fonction et enregistré le résultat dans la variable vocabulaire.*\n","\n","####**Instructions**\n","\n","+ Trier le vocabulaire.\n","+ Créez un dictionnaire char_to_idx mettant en correspondance chaque caractère avec son index dans le vocabulaire trié.\n","+ Créez un dictionnaire idx_to_char faisant correspondre chaque index à son caractère dans le vocabulaire trié."],"metadata":{"id":"j4bt2JuODVLY"}},{"cell_type":"code","source":["# Get vocabulary of Names dataset\n","def get_vocabulary(names):  \n","    # Define vocabulary to be set\n","    all_chars=set()\n","    \n","    # Add the start and end token to the vocabulary\n","    all_chars.add('\\t')\n","    all_chars.add('\\n')  \n","    \n","    # Iterate for each name\n","    for name in names:\n","\n","        # Iterate for each character of the name\n","        for c in name:\n","\n","            if c not in all_chars:\n","            # If the character is not in vocabulary, add it\n","                all_chars.add(c)\n","\n","    # Return the vocabulary\n","    return all_chars"],"metadata":{"id":"J3arVt7QDn83","executionInfo":{"status":"ok","timestamp":1672912259686,"user_tz":-60,"elapsed":381,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Get the vocabulary\n","vocabulary = get_vocabulary(names_df['input'])\n","\n","# Sort the vocabulary\n","vocabulary_sorted = sorted(vocabulary)\n","\n","# Create the mapping of the vocabulary chars to integers\n","char_to_idx = { char : idx for idx, char in enumerate(vocabulary_sorted) }\n","\n","# Create the mapping of the integers to vocabulary chars\n","idx_to_char = { idx : char for idx, char in enumerate(vocabulary_sorted) }\n","\n","# Print the dictionaries\n","print(char_to_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KxJ6yIWDoAH","executionInfo":{"status":"ok","timestamp":1672912274748,"user_tz":-60,"elapsed":374,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"54381696-c18e-4fed-f9e0-bee843f3dba4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["{'\\t': 0, '\\n': 1, 'A': 2, 'B': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7, 'G': 8, 'H': 9, 'I': 10, 'J': 11, 'K': 12, 'L': 13, 'M': 14, 'N': 15, 'O': 16, 'P': 17, 'Q': 18, 'R': 19, 'S': 20, 'T': 21, 'U': 22, 'V': 23, 'W': 24, 'X': 25, 'Y': 26, 'Z': 27, 'a': 28, 'b': 29, 'c': 30, 'd': 31, 'e': 32, 'f': 33, 'g': 34, 'h': 35, 'i': 36, 'j': 37, 'k': 38, 'l': 39, 'm': 40, 'n': 41, 'o': 42, 'p': 43, 'q': 44, 'r': 45, 's': 46, 't': 47, 'u': 48, 'v': 49, 'w': 50, 'x': 51, 'y': 52, 'z': 53}\n"]}]},{"cell_type":"code","source":["print(idx_to_char)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zw9xNBkhEZM4","executionInfo":{"status":"ok","timestamp":1672912310893,"user_tz":-60,"elapsed":281,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"5ce70b02-2045-4a22-dca9-f444b729f164"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: '\\t', 1: '\\n', 2: 'A', 3: 'B', 4: 'C', 5: 'D', 6: 'E', 7: 'F', 8: 'G', 9: 'H', 10: 'I', 11: 'J', 12: 'K', 13: 'L', 14: 'M', 15: 'N', 16: 'O', 17: 'P', 18: 'Q', 19: 'R', 20: 'S', 21: 'T', 22: 'U', 23: 'V', 24: 'W', 25: 'X', 26: 'Y', 27: 'Z', 28: 'a', 29: 'b', 30: 'c', 31: 'd', 32: 'e', 33: 'f', 34: 'g', 35: 'h', 36: 'i', 37: 'j', 38: 'k', 39: 'l', 40: 'm', 41: 'n', 42: 'o', 43: 'p', 44: 'q', 45: 'r', 46: 's', 47: 't', 48: 'u', 49: 'v', 50: 'w', 51: 'x', 52: 'y', 53: 'z'}\n"]}]},{"cell_type":"markdown","source":["###**Introduction aux réseaux neuronaux récurrents**\n","\n","####**1. Introduction aux réseaux neuronaux récurrents**\n","\n","+ Bienvenue. Dans cette leçon, vous découvrirez les réseaux neuronaux récurrents spécialement conçus pour utiliser les informations d’ordre présentes dans les données séquentielles.\n","\n","####**2. Réseau neuronal de feed-forward**\n","\n","+ Les réseaux neuronaux de feedforward acceptent une entrée de taille fixe et produisent une sortie de taille fixe en utilisant un nombre fixe de couches cachées entre les deux. \n","\n","+ Ils supposent que les échantillons d’entrée sont indépendants les uns des autres. \n","\n","+ Ils sont évidemment un mauvais choix pour les données séquentielles. \n","\n","+ Si vous voulez prédire le caractère suivant dans un mot, vous devez savoir quels caractères l’ont précédé dans la séquence. Les réseaux neuronaux récurrents répondent à cette préoccupation.\n","\n","####**3. Introduction de la récurrence**\n","+ Ils sont appelés récurrents parce qu’ils effectuent les mêmes calculs pour chaque élément de la séquence et que la sortie dépend des éléments précédents. \n","\n","+ À chaque pas de temps, un neurone récurrent produit une sortie avec un état caché. \n","\n","+ L’état peut être considéré comme une mémoire du réseau. \n","\n","+ Il consolide toutes les informations d’historique à partir des données d’entrée. \n","\n","+ L’historique et l’entrée actuelle sont utilisés ensemble pour prédire la sortie. \n","\n","+ Ainsi, l’entrée actuelle et l’état masqué du pas de temps précédent servent d’entrée pour ce pas de temps.\n","\n","####**4. RNN pour générateur de prénom bébé**\n","\n","\n","+ Rappelez-vous la dernière leçon où nous avons commencé à traiter le jeu de données de noms pour pouvoir générer de nouveaux noms à partir de zéro. \n","\n","+ L’idée est de générer le personnage suivant en fonction du personnage actuel et de l’historique en entrée. \n","\n","+ Supposons que nous voulions générer le nom « john ». \n","\n","+ Dans le premier pas de temps, nous devons entrer le caractère de tabulation qui devrait générer j en sortie. \n","\n","+ Dans le deuxième pas de temps, nous devons entrer j dans le réseau qui devrait retourner o et l’état gardera une trace que les caractères tab et j sont déjà rencontrés. \n","\n","+ Cela continuera jusqu’à ce que chaque caractère du nom soit traité. \n","\n","+ Les entrées, les sorties et les états sont représentés par des vecteurs. \n","\n","+ À chaque pas de temps, le réseau transforme le vecteur d’entrée en vecteur de sortie et le vecteur d’état est mis à jour pour refléter les caractères déjà rencontrés.\n","\n","####**5. Encodage des caractères**\n","+ N’oubliez pas le mappage caractère-entier que nous avons créé dans la dernière leçon et la façon dont les modèles Machine Learning consomment des valeurs numériques. \n","\n","+ Chaque caractère peut être représenté par un vecteur de longueur égal à la taille du vocabulaire. \n","\n","+ Le vecteur aura un 1 à l’index qui est le mappage de ce caractère. \n","\n","+ Toutes les autres positions auront des zéros. \n","\n","+ **C’est ce qu’on appelle l’encodage à chaud unique.** \n","\n","+ Les vecteurs de chaque caractère ressembleront à ceci.\n","\n","####**6. Nombre de pas de temps**\n","+ Le nombre de pas de temps sera la longueur du nom. \n","\n","+ Comme les noms ont des longueurs différentes, le pas de temps peut être égal à la longueur du nom le plus long avec des noms plus courts complétés par zéro après la nouvelle ligne. \n","\n","+ ***La fonction get_max_len détermine ici la longueur du nom le plus long en itérant sur tous les noms, en enregistrant les longueurs des noms dans une liste et en trouvant le maximum.***\n","\n","####**7. Vecteurs d’entrée et cibles**\n","+ Les vecteurs d’entrée et cible sont tridimensionnels. \n","\n","+ La première dimension est le nombre de noms dans le jeu de données, la seconde étant le nombre de pas de temps qui est la longueur du nom le plus long. \n","\n","+ La troisième dimension est la taille de chaque vecteur codé à chaud qui est la taille du vocabulaire.\n","\n","####**8. Initialiser le vecteur d’entrée**\n","\n","\n","+ Définissons d’abord le vecteur d’entrée comme un vecteur zéro à 3 dimensions. \n","\n","+ La première dimension de ce vecteur est le nombre de noms dans l’ensemble de données, la deuxième dimension est la longueur du nom le plus long qui définit notre taille de pas et la troisième dimension est la taille du vocabulaire. \n","\n","+ Pour remplir ce vecteur avec des données, nous devons convertir chaque caractère de chaque nom en son vecteur codé à chaud. \n","\n","+ La boucle ici itére d’abord sur chaque nom, puis sur chaque caractère de chaque nom et convertit chaque caractère en son codage à chaud à l’aide des mappages de caractères en entiers.\n","\n","####**9. Initialiser le vecteur cible**\n","+ Le vecteur cible peut également être défini et initialisé de la même manière, comme illustré ici.\n","\n","####**10. Construire et compiler un réseau neuronal récurrent**\n","+ Maintenant, construisons le réseau à l’aide de Keras. \n","\n","+ Tout d’abord, nous créons un modèle séquentiel, puis ajoutons une couche RNN de 50 unités. \n","\n","+ Nous définissons les séquences de retour sur true pour nous assurer que la couche RNN génère une séquence et pas seulement un seul vecteur. \n","\n","+ Cette séquence de sortie est ensuite transmise à une couche dense avec activation softmax pour générer la sortie. \n","\n","+ L’activation Softmax prédit les valeurs de probabilité pour chaque caractère du vocabulaire. \n","\n","+ Le calque wrapper TimeDistributed est utilisé pour s’assurer que les calques denses peuvent gérer l’entrée tridimensionnelle. \n","\n","+ Nous pouvons maintenant compiler ce modèle en utilisant la perte d’entropie croisée catégorielle et l’optimiseur d’adam. \n","\n","+ La perte d’entropie croisée catégorique est utilisée lorsque nous avons plus de deux étiquettes. \n","\n","+ Ici, la sortie sera un caractère du vocabulaire et donc, le nombre d’étiquettes est la taille du vocabulaire. \n","\n","+ Adam est un optimiseur avancé qui converge plus rapidement.\n","\n","####**11. Vérifier le résumé du modèle**\n","+ Nous pouvons vérifier l’architecture de notre modèle à l’aide du résumé du modèle comme indiqué ici.\n","\n","####**12. Entraînons-nous!**\n","+ Excellent travail! Vous avez appris à construire et compiler un réseau neuronal récurrent à l’aide de Keras. \n","\n","+ Il est temps de mettre cela en pratique."],"metadata":{"id":"XEXB3SnoEl-w"}},{"cell_type":"markdown","source":["###**EXERCICE**\n","\n","####**Créer des tenseurs d'entrée et de cible**\n","\n","+ Dans cet exercice, vous allez créer deux tenseurs pour coder les séquences d'entrée et de cible. \n","\n","+ L'entrée est une liste contenant tous les noms de l'ensemble de données. \n","\n","+ Ainsi, la première dimension du tenseur d'entrée sera le nombre de noms dans l'ensemble de données. \n","\n","+ Chaque nom peut être considéré comme une chaîne dont la longueur est égale à la longueur du nom le plus long et chaque caractère de chaque nom est un vecteur codé à un coup de la taille du vocabulaire. \n","\n","+ Ainsi, les deuxième et troisième dimensions du tenseur d'entrée seront la longueur du nom le plus long et la taille du vocabulaire. \n","\n","+ Il en va de même pour le tenseur cible.\n","\n","+ *Le jeu de données et le vocabulaire sont disponibles dans names_df et vocabulary. Une fonction nommée get_max_len() est disponible. Elle prend une liste de noms en entrée et retourne la longueur du nom le plus long.*\n","\n","####**Instructions**\n","+ Utilisez la fonction prédéfinie get_max_len() pour trouver la longueur du nom le plus long.\n","+ Utilisez np.zeros() pour définir deux tenseurs de zéro de forme appropriée pour les données d'entrée et les données cibles."],"metadata":{"id":"pQfDyExvMauh"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"9iS6A8kHM1vn","executionInfo":{"status":"ok","timestamp":1672912459511,"user_tz":-60,"elapsed":213,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def get_max_len(names):\n","    \"\"\"\n","    Function to return length of the longest name.\n","    Input: list of names\n","    Output: length of the longest name\n","    \"\"\"\n","\n","    # create a list to contain all the name lengths\n","    length_list=[]\n","\n","    # Iterate over all names and save the name length in the list.]\n","    for l in names:\n","        length_list.append(len(l))\n","\n","    # Find the maximum length\n","    max_len = np.max(length_list)\n","\n","    # return maximum length\n","    return max_len"],"metadata":{"id":"P2I4pvxTIHA6","executionInfo":{"status":"ok","timestamp":1672912461444,"user_tz":-60,"elapsed":308,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Get the vocabulary\n","vocabulary = get_vocabulary(names_df['input'])\n","vocabulary = sorted(set(vocabulary))\n","#Character to integer and the reverse mapping.\n","char_to_idx = dict((char, idx) for idx, char in enumerate(vocabulary))\n","idx_to_char = dict((idx, char) for idx, char in enumerate(vocabulary))"],"metadata":{"id":"UL29XgJUIHHA","executionInfo":{"status":"ok","timestamp":1672912433796,"user_tz":-60,"elapsed":417,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Find the length of longest name\n","max_len = get_max_len(names_df['input'])\n","\n","# Initialize the input vector\n","input_data = np.zeros((len(names_df['input']), max_len+1, len(vocabulary)), dtype='float32')\n","\n","# Initialize the target vector\n","target_data = np.zeros((len(names_df['input']), max_len+1, len(vocabulary)), dtype='float32')"],"metadata":{"id":"JcztmD5iIHI-","executionInfo":{"status":"ok","timestamp":1672912466050,"user_tz":-60,"elapsed":921,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["max_len"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RyVnFEztIHLn","executionInfo":{"status":"ok","timestamp":1672912470054,"user_tz":-60,"elapsed":527,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"9a9e1be1-8ac6-4a50-b248-cb40bece2349"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["input_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_POOEVDgNzsR","executionInfo":{"status":"ok","timestamp":1672912472194,"user_tz":-60,"elapsed":306,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"57464691-5b3d-4f7a-8c1c-69cb183e47a2"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       ...,\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["target_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNfImpQKN5bw","executionInfo":{"status":"ok","timestamp":1672912477616,"user_tz":-60,"elapsed":199,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"b3c2c065-d745-405e-be31-fd3eea440b42"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       ...,\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["####**Initialiser les vecteurs d'entrée et de cible avec des valeurs**\n","+ Dans le dernier exercice, vous avez créé les tenseurs d'entrée et de cible de forme appropriée contenant tous les zéros. \n","\n","+ Maintenant, vous allez les remplir avec des valeurs réelles. \n","\n","+ Les tenseurs d'entrée et de cible contiennent tous les noms de l'ensemble de données. \n","\n","+ Chaque nom peut être considéré comme une chaîne dont la longueur est égale à la longueur du nom le plus long et chaque caractère de chaque nom est un vecteur codé à un coup de taille vocabulaire.\n","\n","+ Les tenseurs peuvent être remplis comme suit : \n","  + input_data[n_idx, p_idx, char_to_idx[char]] sera mis à 1 chaque fois que l'index du nom dans l'ensemble de données est n_idx et qu'il contient le caractère char en position p_idx.\n","\n","+ *L'ensemble de données et le mappage des caractères en nombres entiers sont disponibles dans names_df et char_to_idx. Les tenseurs zéro du dernier exercice sont disponibles dans input_data et target_data.*\n","\n","####**Instructions**\n","\n","+ Remplissez le tenseur d'entrée en itérant sur chaque caractère de chaque nom dans names_df['input'] et en le convertissant en un vecteur codé à un coup.\n","+ Remplissez le tenseur cible en itérant sur chaque caractère de chaque nom dans names_df['target'] et en le convertissant en un vecteur encodé à un coup."],"metadata":{"id":"oCokX0A1OQ5J"}},{"cell_type":"code","source":["input_data = np.zeros((len(names_df), max_len+1, len(vocabulary)),dtype='float32')"],"metadata":{"id":"b5kHHEkyR2JX","executionInfo":{"status":"ok","timestamp":1672912486939,"user_tz":-60,"elapsed":920,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["target_data = np.zeros((len(names_df), max_len+1, len(vocabulary)),dtype='float32')"],"metadata":{"id":"mlfL-qhaUwl4","executionInfo":{"status":"ok","timestamp":1672912488597,"user_tz":-60,"elapsed":429,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Iterate for each name in the dataset\n","for n_idx, name in enumerate(names_df['input']):\n","  # Iterate over each character and convert it to a one-hot encoded vector\n","  for c_idx, char in enumerate(name):\n","    input_data[n_idx, c_idx, char_to_idx[char]] = 1"],"metadata":{"id":"tGj1RbYyOfm5","executionInfo":{"status":"ok","timestamp":1672912495240,"user_tz":-60,"elapsed":1232,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["+ l'index 36 est hors limites pour l'axe 2 de taille 28"],"metadata":{"id":"ADAUCI-bXeim"}},{"cell_type":"code","source":["# Iterate for each name in the dataset\n","for n_idx, name in enumerate(names_df['target']):\n","  # Iterate over each character and convert it to a one-hot encoded vector\n","  for c_idx, char in enumerate(name):\n","    target_data[n_idx, c_idx, char_to_idx[char]] = 1"],"metadata":{"id":"bTtItM2DOfq3","executionInfo":{"status":"ok","timestamp":1672912499229,"user_tz":-60,"elapsed":923,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["####**Construction et compilation du réseau RNN***\n","+ Maintenant que les vecteurs d'entrée et de cible sont prêts, il est temps de construire le réseau neuronal récurrent. Vous allez créer une petite architecture de réseau qui comportera 50 nœuds RNN simples dans la première couche, suivis d'une couche dense. \n","\n","+ La couche dense va générer une distribution de probabilité sur le vocabulaire pour le prochain caractère. \n","\n","+ Ainsi, la taille de la couche dense sera la même que la taille du vocabulaire.\n","\n","+ L'ensemble de données est disponible sous forme de noms de DataFrame. La longueur du nom le plus long est enregistrée dans la variable max_len. \n","\n","+ Le vocabulaire est disponible dans la variable vocabulary. \n","\n","+ Les couches SimpleRNN, Dense, Activation, TimeDistributed sont déjà importées de tensorflow.keras.layers et le modèle Sequential est déjà importé de tensorflow.keras.models.\n","\n","####**Instructions**\n","\n","+ Ajoutez une couche SimpleRNN de taille 50 avec return_sequences comme True.\n","+ Ajoutez une couche Dense distribuée dans le temps de taille égale au vocabulaire.\n","+ Ajouter une couche d'activation softmax.\n","+ Compilez le modèle en utilisant la perte \"categorical_crossentropy\" et l'optimiseur \"adam\"."],"metadata":{"id":"WNXF4YCPWS9g"}},{"cell_type":"code","source":["from keras.layers import LSTM, SimpleRNN\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Input, TimeDistributed, Flatten"],"metadata":{"id":"QFKLbNIsZoCw","executionInfo":{"status":"ok","timestamp":1672912507220,"user_tz":-60,"elapsed":2769,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Create a Sequential model\n","model = Sequential()\n","\n","# Add SimpleRNN layer of 50 units\n","model.add(SimpleRNN(50, input_shape=(max_len+1, len(vocabulary)), return_sequences=True))\n","\n","# Add a TimeDistributed Dense layer of size same as the vocabulary\n","model.add(TimeDistributed(Dense(len(vocabulary), activation='softmax')))\n","\n","# Compile the model\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n","\n","# Print the model summary\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vvLWMTtWWfrg","executionInfo":{"status":"ok","timestamp":1672912555070,"user_tz":-60,"elapsed":236,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"433325b4-a819-4ec8-aebb-69c05f415a8f"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," simple_rnn (SimpleRNN)      (None, 13, 50)            5250      \n","                                                                 \n"," time_distributed (TimeDistr  (None, 13, 54)           2754      \n"," ibuted)                                                         \n","                                                                 \n","=================================================================\n","Total params: 8,004\n","Trainable params: 8,004\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["###**Inférence à l’aide d’un réseau neuronal récurrent**\n","\n","####**1. Inférence à l’aide d’un réseau neuronal récurrent**\n","+ Dans la dernière série d’exercices, nous avons construit un réseau neuronal récurrent en utilisant Keras et l’avons compilé. \n","\n","+ Nos vecteurs d’entrée et cibles sont également prêts.\n","\n","+ Dans cette leçon, vous allez apprendre à former ce réseau et à obtenir des prédictions à partir du modèle formé.\n","\n","####**2. Comprendre la formation**\n","+ Vous devez vous demander ce que signifie la formation. \n","\n","+ ***Vous pouvez considérer n’importe quel modèle de réseau neuronal comme une boîte noire qui, étant donnée une entrée, produit une sortie, souvent appelée prédiction***. \n","\n","+ Chaque paire d’entrée et de cible $x,y$ indique au réseau que la sortie idéale devrait être $y$ lorsque l’entrée est $x$. \n","\n","+ Lorsque vous fournissez une entrée $x$, le réseau effectue des calculs internes et produit une sortie, disons $z$, qui sera différente de $y$. \n","\n","+ Le but de la formation est de réduire cette différence ou erreur en ajustant les paramètres internes du réseau. \n","\n","+ Une fois que le réseau a itéré plusieurs fois sur l’ensemble du jeu de données, il commence à produire une sortie similaire à celle présente dans les exemples cibles.\n","\n","####**3. Vecteurs d’entrée et cibles pour la formation**\n","\n","\n","+ Rappelez-vous que nos vecteurs d’entrée et cibles sont des vecteurs tridimensionnels dont la première dimension est le nombre d’échantillons ou de noms dans l’ensemble de données, la deuxième dimension est le nombre de pas de temps que nous avons définis comme la longueur du nom le plus long et la troisième dimension est la taille des vecteurs codés à chaud qui est la taille du vocabulaire. \n","\n","+ Nous devons utiliser ces vecteurs pour former le modèle que nous avons construit.\n","\n","####**4. Réseau récurrent de trains**\n","\n","+ Nous pouvons utiliser la fonction Keras fit pour entraîner le modèle. \n","\n","+ Nous devons transmettre les données d’entrée et les données cibles. \n","\n","+ De plus, nous devons spécifier la taille du lot et le nombre d’époques. \n","\n","+ Il est efficace d’ajuster les paramètres du réseau après avoir accumulé l’erreur sur un ensemble d’échantillons que d’ajuster après chaque échantillon. \n","\n","+ Le nombre d’échantillons après lequel le modèle ajuste les paramètres est spécifié par la taille du lot. \n","\n","\n","+ Nous devons également itérer sur l’ensemble des données un certain nombre de fois pour obtenir le meilleur résultat. \n","+ ***Epoch spécifie le nombre de fois que le jeu de données complet sera itéré***.\n","\n","####**5. Prédire le premier caractère**\n","\n","+ Maintenant que le modèle est formé, nous pouvons l’utiliser pour les prédictions. \n","\n","+ Nous avons entraîné le modèle de manière à ce qu’il produise le caractère suivant étant donné le caractère actuel en entrée. \n","\n","+ Et, le premier caractère est le caractère de tabulation qui est le jeton de départ. \n","\n","+ Nous pouvons alimenter le caractère de l’onglet sur le réseau et obtenir le caractère suivant le plus probable en sortie. \n","\n","+ Nous pouvons créer un vecteur zéro tridimensionnel pour la séquence de sortie et l’initialiser pour contenir le caractère de tabulation. \n","\n","+ Nous pouvons utiliser la méthode « predict proba » pour obtenir la distribution de probabilité pour le caractère suivant dans la séquence. \n","\n","+ Comme nous voulons générer le premier caractère après l’onglet, nous devons découper la liste de distribution de probabilité pour obtenir la distribution de probabilité pour le premier caractère. \n","\n","+ Maintenant, nous pouvons trouver le caractère suivant en échantillonnant le vocabulaire au hasard en utilisant cette distribution de probabilité.\n","\n","####**6. Prédire le deuxième caractère en utilisant le premier**\n","+ Nous pouvons utiliser le premier caractère généré pour prédire le deuxième caractère de la séquence. \n","\n","+ Le même processus peut être utilisé pour prédire le deuxième caractère le plus probable étant donné l’onglet et le premier caractère.\n","\n","####**7. Générez des noms de bébé**\n","+ Nous pouvons continuer à générer des personnages de cette manière jusqu’à ce que le jeton de fin ou la nouvelle ligne soit rencontré. \n","\n","+ Nous pouvons également mettre une contrainte sur la longueur maximale des noms et arrêter lorsque le nombre de caractères générés atteint ce maximum. \n","\n","+ Nous pouvons créer une fonction qui le fait à l’intérieur d’une boucle while, comme indiqué. Dans cette fonction, la longueur maximale est définie sur 10. \n","\n","+ Ainsi, il continue à générer les caractères jusqu’à ce qu’une nouvelle ligne soit rencontrée ou qu’elle atteigne un maximum de 10 caractères. \n","\n","+ Nous pouvons également mettre tout cela dans une autre boucle pour générer plus de noms.\n","\n","####**8. Noms de bébé cool**\n","+ Ce sont les dix noms générés par notre modèle. \n","\n","+ Vérifiez à quel point certains d’entre eux peuvent être similaires à des noms humains réels.\n","\n","+  Vous pouvez entraîner le modèle pour plus d’époques à l’aide d’un jeu de données plus volumineux pour le rendre encore plus précis.\n","\n","####**9. Entraînons-nous!**\n","+ Maintenant que vous avez appris à former et à obtenir des prédictions à partir de réseaux neuronaux récurrents, il est temps pour vous de mettre cela en pratique."],"metadata":{"id":"cXyMLqNTH1Fw"}},{"cell_type":"markdown","source":["###**EXERCICES**\n","\n","####**Entraîner le modèle RNN et démarrer les prédictions**\n","+ Dans cette leçon, vous entraînerez le modèle que vous avez créé la dernière fois sur les Tensors d'entrée et cible et utiliserez le modèle entraîné pour les prédictions.\n","\n","+ Le nom de sortie sera généré caractère par caractère. Le premier caractère de chaque nom était le jeton de début \\t. \n","\n","+ Vous alimenterez le jeton de début avec le modèle formé pour produire une distribution de probabilité sur le vocabulaire qui peut être échantillonnée pour générer le caractère suivant.\n","\n","+ Le modèle compilé, les vecteurs d'entrée et cible sont enregistrés dans model, input_data et target_data. \n","\n","+ Les tenseurs d'entrée et cible ont la forme (nombre de mots) x (longueur du mot le plus long + 1) x (nombre de caractères dans le vocabulaire)\n","\n","+ Le mappage caractère-entier, le vocabulaire et la longueur du nom le plus long sont enregistrés dans char_to_idx, vocabulaire et max_len.\n","\n","####**Des instructions**\n","+ Ajustez le modèle pour 5 époques en utilisant une taille de lot de 128.\n","+ Définissez un tenseur nul output_seq contenant un seul mot codé de la même manière que les mots dans input_data.\n","+ Initialisez le premier caractère de output_seq avec le jeton de début codé."],"metadata":{"id":"xggL1d43J0ql"}},{"cell_type":"code","source":["# Fit the model for 5 epochs using a batch size of 128 \n","model.fit(input_data, target_data, batch_size=128, epochs=5)\n","\n","# Create a 3-D zero vector and initialize it with the start token\n","output_seq = np.zeros((1, max_len+1, len(vocabulary)))\n","output_seq[0, 0, char_to_idx['\\t']] = 1"],"metadata":{"id":"G8RNUICxWfvK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672912704457,"user_tz":-60,"elapsed":140098,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"0e1ed892-247a-451e-9bec-e6da6e8ce326"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","2016/2016 [==============================] - 35s 17ms/step - loss: 1.1458\n","Epoch 2/5\n","2016/2016 [==============================] - 26s 13ms/step - loss: 1.0065\n","Epoch 3/5\n","2016/2016 [==============================] - 26s 13ms/step - loss: 0.9660\n","Epoch 4/5\n","2016/2016 [==============================] - 26s 13ms/step - loss: 0.9408\n","Epoch 5/5\n","2016/2016 [==============================] - 27s 13ms/step - loss: 0.9233\n"]}]},{"cell_type":"markdown","source":["+ Utilisez la fonction .predict() du modèle formé pour obtenir la distribution de probabilité pour le premier caractère.\n","+ Utilisez **np.random.choice()** pour échantillonner le vocabulaire en fonction de la distribution de probabilité de sortie pour obtenir le premier caractère."],"metadata":{"id":"PdhRetNEK4nt"}},{"cell_type":"code","source":["# Fit the model for 5 epochs using a batch size of 128 \n","model.fit(input_data, target_data, batch_size=128, epochs=5)\n","\n","# Create a 3-D zero vector and initialize it with the start token\n","output_seq = np.zeros((1, max_len+1, len(vocabulary)))\n","output_seq[0, 0, char_to_idx['\\t']] = 1\n","\n","# Get the probabilities for the first character\n","probs = model.predict(output_seq, verbose=0)[:,1,:]\n","\n","# Sample vocabulary to get first character\n","first_char = np.random.choice(sorted(vocabulary), replace=False, p=probs.reshape(len(vocabulary)))\n","\n","# Print the character generated\n","print(first_char)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VdpuIN5jLAiF","executionInfo":{"status":"ok","timestamp":1672912836544,"user_tz":-60,"elapsed":131087,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"91820619-46e3-4e19-eebf-bc5026b0fec9"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","2016/2016 [==============================] - 27s 14ms/step - loss: 0.9106\n","Epoch 2/5\n","2016/2016 [==============================] - 26s 13ms/step - loss: 0.9006\n","Epoch 3/5\n","2016/2016 [==============================] - 25s 13ms/step - loss: 0.8925\n","Epoch 4/5\n","2016/2016 [==============================] - 25s 13ms/step - loss: 0.8859\n","Epoch 5/5\n","2016/2016 [==============================] - 27s 13ms/step - loss: 0.8805\n","a\n"]}]},{"cell_type":"markdown","source":["####**Générer des noms de bébé**\n","+ Dans l'exercice précédent, vous avez entraîné le modèle et utilisé le modèle entraîné pour générer le premier caractère à partir duquel le jeton de début a été attribué comme caractère de départ. \n","\n","+ Dans cet exercice, vous allez générer le deuxième personnage en transmettant à nouveau le jeton de départ et le premier personnage généré au réseau formé. \n","\n","+ Vous générerez également des noms complets à partir du jeton de début et répéterez ce processus jusqu'à ce que le jeton de fin soit trouvé.\n","\n","+ Le output_seq défini dans le dernier exercice est disponible pour utilisation. Il contient déjà le jeton de début \\t. \n","\n","+ Le premier caractère généré précédemment est enregistré dans la variable first_char. \n","\n","+ Le modèle formé est disponible dans le modèle comme d'habitude. Le vocabulaire et le mappage caractère-entier sont disponibles respectivement dans vocabulaire et char_to_idx.\n","\n","####**Des instructions**\n","+ Mettez à jour output_seq pour contenir l'encodage du premier caractère first_char.\n","+ Obtenez les probabilités pour le deuxième caractère.\n","+ Utilisez np.random.choice pour échantillonner le vocabulaire afin d'obtenir le deuxième caractère."],"metadata":{"id":"GgD-0PCHLcbq"}},{"cell_type":"code","source":["# Print the first character which we got last time\n","print(first_char)\n","\n","# Update the vector to contain first the character\n","output_seq[0, 1, char_to_idx[first_char]] = 1\n","\n","# Get the probabilities for the second character\n","probs = model.predict(output_seq, verbose=0)[:,2,:]\n","\n","# Sample vocabulary to get second character\n","second_char = np.random.choice(sorted(vocabulary), replace=False, \n","                               p=probs.reshape(len(vocabulary)))\n","\n","# Print the second character\n","print(second_char)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgfFqyiVLo-D","executionInfo":{"status":"ok","timestamp":1672913105254,"user_tz":-60,"elapsed":235,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"34bed651-eb54-405b-c72a-35646906011f"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["a\n","g\n"]}]},{"cell_type":"markdown","source":["####**Question**\n","+ Vous pouvez continuer à générer des caractères dans une boucle jusqu'à ce que le caractère de saut de ligne soit trouvé ou que vous atteigniez une longueur maximale prédéfinie pour le nom. \n","\n","+ De cette façon, vous pouvez générer un nouveau nom caractère par caractère à partir du jeton de début. \n","\n","+ Une fonction generate_baby_names() est disponible qui prend un nombre n en entrée et génère n nouveaux noms de bébé en utilisant cette technique.\n","\n","+ Expérimentez avec cette fonction dans la console et générez des noms. Vérifiez à quel point certains de ces noms ressemblent à des noms humains. Mais il y a encore place à l'amélioration. Pouvez-vous penser à des moyens d'améliorer les performances du modèle ?\n","\n","####**Des réponses possibles**\n","\n","+ Entraînez-vous à l'aide d'un ensemble de données plus volumineux.\n","\n","+ Entraînez-vous pour plus d'époques.\n","\n","+ Augmentez la complexité du modèle en ajoutant davantage de couches masquées.\n","\n","+ ***Tout ce qui précède***"],"metadata":{"id":"ccR5gClnObur"}},{"cell_type":"markdown","source":["###**Limites des réseaux neuronaux récurrents**\n","\n","####**1. Limites des réseaux neuronaux récurrents**\n","+ Bienvenue. Dans le chapitre précédent, vous avez découvert les réseaux neuronaux récurrents et leur fonctionnement sur des données séquentielles. \n","\n","+ Cependant, ***les réseaux neuronaux récurrents ne sont pas très efficaces pour les séquences plus longues et nous avons besoin d’un type de récurrence différent pour gérer les longues séquences***. \n","\n","+ Dans ce chapitre, ***vous apprendrez à connaître les limites des réseaux neuronaux récurrents simples et à vous familiariser avec la mémoire à long terme à court terme.***\n","\n","####**2. Réseaux neuronaux simples**\n","+ ***Les réseaux neuronaux peuvent être considérés comme un ensemble de nœuds ou de neurones disposés en couches et les nœuds de différentes couches sont reliés par des poids.*** \n","\n","+ La première couche obtient les données d’entrée. \n","\n","+ Les autres couches reçoivent des entrées de la couche précédente.\n","\n","####**3. Calculs dans les réseaux neuronaux**\n","\n","+ Chaque nœud multiplie les entrées entrantes avec les poids et les additionne. \n","\n","+ Il s’agit d’une transformation linéaire. \n","\n","+ Une transformation non linéaire est ensuite appliquée pour générer la sortie appelée activation à partir de ce nœud. \n","\n","+ En théorie, les combinaisons de transformations linéaires et non linéaires peuvent se rapprocher de n’importe quelle fonction qui rend les réseaux neuronaux très puissants.\n","\n","####**4. Gradient et formation**\n","\n","+ Nous pouvons définir l’erreur à chaque nœud de sortie comme étant fonction de la sortie attendue et de la sortie prévue. \n","\n","+ In peut être représenté par la différence au carré entre la production réelle et la sortie prévue. \n","\n","+ Dans chaque itération d’apprentissage, les erreurs de tous les échantillons d’apprentissage sont ajoutées. \n","\n","+ L’entraînement d’un réseau neuronal n’est rien d’autre que l’ajustement des valeurs des poids afin de réduire l’erreur. \n","\n","+ ***Le gradient est défini comme étant le taux de variation de l’erreur par rapport aux pondérations***. \n","\n","+ Si nous pouvons calculer les gradients pour chaque poids, nous pouvons ajuster le poids pour réduire l’erreur. \n","\n","+ Les valeurs de gradient sont multipliées par une petite fraction, puis soustraites des poids. \n","\n","+ ***Cette fraction est appelée le taux d’apprentissage qui influence la façon dont les valeurs de poids convergeront vers la valeur optimale***. \n","\n","+ L’ajustement des valeurs de poids peut être effectué pour plusieurs itérations jusqu’à ce que nous atteignions une précision suffisante.\n","\n","####**5. Règle de chaîne**\n","+ Les valeurs de gradient pour chaque poids sont calculées à l’aide d’une règle simple du calcul appelée règle de chaîne qui dit que si $z$ est une fonction de $y$ et $y$ est, à son tour, une fonction de $x$, alors la dérivée de $z$ par rapport à $x$ peut être trouvée en multipliant la dérivée de $z$ par rapport à $y$ et la dérivée de $y$ par rapport à $x$. \n","\n","+ Les valeurs de dégradé dans la couche de sortie peuvent être trouvées par différenciation. \n","\n","+ Pour les couches intermédiaires, la règle de chaîne est appliquée.\n","\n","####**6. Rétropropagation**\n","+ Les valeurs de gradient pour chaque couche peuvent être trouvées par rétropropagation. \n","\n","+ Ce cours vous donnera l’intuition derrière la rétropropagation et l’apprentissage profond sans trop approfondir les mathématiques. \n","\n","+ Le gradient dans la couche de sortie peut être calculé en différenciant l’erreur par rapport aux poids. \n","\n","+ Selon la règle de chaîne, le dégradé des autres calques sera le produit des valeurs de dégradé des calques suivants. \n","\n","+ Intuitivement, le gradient est calculé au niveau de la couche de sortie et rétropropagé vers la couche d’entrée.\n","\n","####**7. Disparition et explosion des gradients**\n","\n","+ Ce qu’il faut retenir ici, c’est que le gradient d’un poids interne est le produit de nombreuses valeurs de gradient des couches suivantes. \n","\n","+ Comme les réseaux neuronaux récurrents fonctionnent sur de nombreux pas de temps, les valeurs de gradient sont propagées du dernier pas de temps vers le premier. \n","\n","+ Si les valeurs du gradient sont de petites fractions, ce qu’elles sont habituellement, alors le gradient deviendra de plus en plus petit à mesure que vous vous dirigerez vers le premier pas de temps à partir du dernier et finira par devenir nul et le neurone cessera d’apprendre. \n","\n","+ C’est ce qu’on appelle le problème du gradient de fuite. \n","\n","+ Inversement, si les valeurs de gradient sont supérieures à un, alors à mesure que nous reculons d’un pas dans le temps, il deviendra de plus en plus grand, ce qui entraînera une explosion du gradient.\n","\n","####**8. Recours**\n","+ Nous pouvons définir un nombre fixe de pas de temps jusqu’auxquels nous voulons nous propager pour réduire l’effet des gradients de fuite. \n","\n","+ Pour remédier aux problèmes d’explosion du gradient, nous pouvons couper les valeurs de dégradé à chaque nœud. \n","\n","+ Ces deux solutions de contournement entraîneront une formation sous-optimale et réduiront les performances de prédiction.\n","\n","####**9. Entraînons-nous!**\n","+ Vous venez d’apprendre comment les dégradés qui disparaissent et explosent posent problème pour les séquences plus longues. \n","\n","+ Travaillons quelques exemples pour voir cela dans la pratique."],"metadata":{"id":"a4jP2h3uSXfR"}},{"cell_type":"markdown","source":["###**EXERCICE**\n","####**Disparition et explosion des gradients**\n","+ ***Les réseaux neuronaux récurrents simples souffrent du problème des gradients de fuite où les gradients des poids deviennent de plus en plus petits et finissent par devenir nuls lorsque nous reculons du dernier pas de temps vers le premier pas de temps***.\n","\n","+ ***Les réseaux neuronaux récurrents souffrent également du problème du gradient explosif où les valeurs de gradient des poids deviennent de plus en plus grandes à mesure que nous reculons-se propageons vers le premier pas de temps.***\n","\n","+ *Dans cet exercice, vous plongerez en profondeur dans les problèmes de gradient qui disparaissent et explosent et réfléchirez aux causes possibles et aux remèdes de ces problèmes.*\n","\n","####**Instructions**\n","\n","+ Faites la distinction entre les causes et les remèdes des problèmes de gradient de disparition et d’explosion.\n","\n","![image.png](data:image/png;base64,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)"],"metadata":{"id":"bFA0-qy_VOsq"}},{"cell_type":"markdown","source":["####**Réseau simple utilisant Keras**\n","+ À présent, vous avez une compréhension intuitive de la façon dont les valeurs de gradient deviennent de moins en moins importantes au fur et à mesure de la rétropropagation. \n","\n","+ Dans cet exercice, vous allez travailler sur un exemple pour illustrer ce problème de gradient de fuite. \n","\n","+ Vous allez créer un réseau simple de couches denses à l'aide de Keras et vérifier les valeurs de gradient des poids pour une itération de rétropropagation.\n","\n","+ ***Le modèle séquentiel et les couches denses et d'activation sont déjà importés de Keras***. \n","\n","+ ***Le backend du module Keras est également importé. Cela a une méthode .gradients() qui peut être utilisée pour obtenir les valeurs de gradient des poids***.\n","\n","####**Des instructions**\n","+ Créez un modèle séquentiel.\n","+ Ajoutez une couche Dense de 12 unités avec une activation 'relu' et une dimension d'entrée de 8.\n","+ Ajoutez une couche Dense de 8 unités avec activation 'relu'.\n","+ Ajoutez une couche de sortie Dense de 1 unité avec activation « sigmoïde ».\n","+ Compilez le modèle à l'aide de la perte 'binary_crossentropy' et de l'optimiseur 'adam'.\n","+ Utilisez backend.gradients() du backend pour obtenir les vecteurs de dégradé."],"metadata":{"id":"kheAGKkDWVuo"}},{"cell_type":"code","source":["# Create a sequential model\n","model = Sequential()\n","\n","# Create a dense layer of 12 units\n","model.add(Dense(units = 12, input_dim=8, activation='relu'))\n","\n","# Create a dense layer of 8 units\n","model.add(Dense(units = 8, activation='relu'))"],"metadata":{"id":"RzwP6_jOOocT","executionInfo":{"status":"ok","timestamp":1672913118173,"user_tz":-60,"elapsed":248,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0FmBd1y9VjC6","executionInfo":{"status":"ok","timestamp":1672913120994,"user_tz":-60,"elapsed":299,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"48c192aa-cf06-4c6d-950a-cdb1431cbe2d"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.engine.sequential.Sequential at 0x7fde4d737760>"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["from tensorflow.keras import backend as K\n","import tensorflow as tf"],"metadata":{"id":"adstmTkbXy0v","executionInfo":{"status":"ok","timestamp":1672913126829,"user_tz":-60,"elapsed":309,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["+ https://faroit.com/keras-docs/1.2.0/backend/\n","+ https://www.tensorflow.org/guide/autodiff\n","+ http://neuralnetworksanddeeplearning.com/chap1.html\n","+ https://github.com/BinRoot/TensorFlow-Book\n","+ https://www.datacamp.com/tutorial/tensorflow-tutorial"],"metadata":{"id":"Xm-JVaO7ZDOO"}},{"cell_type":"code","source":["help(K)"],"metadata":{"id":"cWemdClOX2Vn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a sequential model\n","model = Sequential()\n","\n","# Create a dense layer of 12 units\n","model.add(Dense(12, input_dim=8, activation='relu'))\n","\n","# Create a dense layer of 8 units\n","model.add(Dense(8, activation='relu'))\n","\n","# Create a dense layer of 1 unit\n","model.add(Dense(units = 1, activation='sigmoid'))\n","\n","# Compile the model and get gradients\n","model.compile(loss='binary_crossentropy', optimizer='adam')\n","gradients = tf.GradientTape(model.output, model.trainable_weights)"],"metadata":{"id":"DIN3WMdcVjLm","executionInfo":{"status":"ok","timestamp":1672913130213,"user_tz":-60,"elapsed":311,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["gradients"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tQUDHFxXMIx","executionInfo":{"status":"ok","timestamp":1672913143724,"user_tz":-60,"elapsed":522,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"bdf1ab51-d8de-4604-b411-d20daaa7274d"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.eager.backprop.GradientTape at 0x7fde488039d0>"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["####**Dégradés de fuite**\n","+ Avant de commencer à travailler sur des applications plus robustes de génération de langage, il est préférable d'apprendre à identifier quand vous souffrez du problème du gradient de fuite. \n","\n","+ Dans cet exercice, vous entraînerez le réseau que vous avez créé lors du dernier exercice à l'aide de données d'entraînement aléatoires.\n","\n","+ Pour exécuter un modèle dans Tensorflow, \n","  + **vous devez d'abord initialiser une session tensorflow à l'aide de la fonction InteractiveSession()**. \n","  + Ensuite, **vous initialiserez toutes les variables à l'aide de la fonction global_variables_initializer()**.\n","\n","+ *Vous exécuterez le nœud de dégradés que vous avez créé dans le dernier exercice de cette session tensorflow. Vous vérifierez également certaines des valeurs de dégradé de différentes couches. Notez que tensorflow a été importé en tant que tf. Pour savoir comment exécuter un modèle de base dans Tensorflow, consultez cet article (https://www.datacamp.com/tutorial/tensorflow-tutorial).*\n","\n","####**Des instructions**\n","+ Utilisez np.random.random() pour créer un vecteur d'entrée de taille (1,8).\n","+ Utilisez **InteractiveSession() pour créer une session Tensorflow**.\n","+ Exécutez global_variables_initializer() dans la session tensorflow.\n","+ Utilisez .run () sur l'objet de session et alimentez input_vector à l'entrée du modèle pour obtenir les valeurs de gradient."],"metadata":{"id":"eGFR5Z-Yfx_D"}},{"cell_type":"code","source":["tf.compat.v1.disable_eager_execution()\n","\n","# Create a dummy input vector\n","input_vector = np.random.random((1,8))\n","\n","# Create a tensorflow session to run the network\n","sess = tf.compat.v1.Session() #tf.compat.v1.InteractiveSession()\n","\n","# Initialize all the variables\n","sess.run(tf.compat.v1.global_variables_initializer())"],"metadata":{"id":"cIcA_iVigH7L","executionInfo":{"status":"ok","timestamp":1672917652761,"user_tz":-60,"elapsed":295,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["help(sess.run)"],"metadata":{"id":"FP69Ifnrkyb6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the gradients using the training examples\n","evaluated_gradients = sess.run(gradients, feed_dict = {model.input:input_vector})\n","\n","# Print gradient values from third layer and two nodes of the second layer\n","print(evaluated_gradients[4])"],"metadata":{"id":"qVYbeiGXgH--"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(evaluated_gradients[2][4])"],"metadata":{"id":"OwToMxptgcpE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Introduction à la mémoire à long terme**\n","\n","####**1. Introduction à la mémoire à long terme**\n","+ Dans la leçon précédente, vous avez appris à quel point les réseaux neuronaux récurrents simples luttent lors de la modélisation de longues séquences.\n","\n","+ Dans cette leçon, **vous découvrirez une mémoire à long et court terme qui ne souffre pas de problèmes de gradient de disparition et d’explosion et, par conséquent, peut gérer efficacement des séquences plus longues**.\n","\n","####**2. Dépendances à long terme**\n","+ **Nous savons déjà que les RNN peuvent apprendre l’avenir du contexte du passé**. \n","\n","+ Parfois, nous n’avons besoin que du passé récent pour prédire l’avenir. \n","\n","+ Par exemple, supposons que nous voulions prédire le dernier mot de la phrase « Les oiseaux volent dans le ciel ». \n","\n","+ Ici, pour prédire le mot « ciel », il suffit de se souvenir des derniers mots. \n","\n","+ Le plus souvent, nous avons besoin du contexte d’un passé plus lointain pour prédire l’avenir. \n","\n","+ Considérez un texte où les premiers mots sont « Je suis né en Allemagne » suivi de beaucoup d’autres mots ou phrases et enfin il se termine par « Je peux parler allemand ». \n","\n","+ D’après le contexte récent, il est assez évident que le dernier mot serait le nom d’une langue, cependant, pour savoir de quelle langue il s’agit, nous devons remonter plus loin au début du texte. \n","\n","+ **Les RNN ont du mal à modéliser de telles dépendances à long terme en raison de problèmes de gradient qui disparaissent et explosent**.\n","\n","####**3. Mémoire à long et court terme**\n","\n","+ Les réseaux de mémoire à long et court terme sont spécialement conçus pour gérer les dépendances à long terme. \n","\n","+ Rappelez-vous que les réseaux neuronaux récurrents simples n’ont qu’un seul état pour capturer des informations historiques. \n","\n","+ Cela ne suffit pas à saisir les dépendances à long terme. \n","\n","+ Ainsi, les réseaux de mémoire à long et court terme utilisent un état supplémentaire pour capturer les dépendances à long terme. \n","\n","+ Ainsi, ils auront deux états - ***l’un pour capturer l’histoire à court terme et l’autre pour capturer l’histoire à long terme.*** \n","\n","+ ***Ces états sont appelés respectivement les états cachés et cellulaires***. \n","\n","+ À chaque pas de temps, un nœud de mémoire à long terme acceptera l’entrée et les états masqués et cellulaires du dernier pas de temps. \n","\n","+ Selon les données d’entrée, il peut oublier ou ajouter de nouvelles informations dans ces états masqués et cellulaires et les transmettre au pas de temps suivant. \n","\n","+ L’état masqué peut également être utilisé comme sortie si nécessaire.\n","\n","####**4. Écrivez comme Shakespeare**\n","+ **Pour comprendre l’efficacité des LSTM pour capturer les dépendances à long terme, plongeons dans une étude de cas où nous générerons un texte qui imite le style d’écriture unique de Shakespeare basé sur un ensemble de données d’œuvres littéraires sélectionnées de Shakespeare***. \n","\n","+ Vous pouvez obtenir le vocabulaire en créant un ensemble à partir des données textuelles et en le triant comme indiqué. \n","\n","+ Ensuite, vous pouvez itérer sur le vocabulaire et créer des mappages de caractères à des entiers et vice versa comme nous l’avons fait dans le chapitre précédent.\n","\n","####**5. Données d’entrée et données cibles**\n","+ En effet, le problème ici est de générer le caractère suivant en fonction d’une séquence de caractères en entrée. \n","\n","+ Ainsi, l’entrée de notre modèle sera une séquence de caractères et la sortie sera le caractère suivant dans la séquence.\n","\n","####**6. Données d’entrée et de cible à partir de texte brut**\n","+ Pour créer les données d’entraînement pour ce problème, nous allons diviser le texte en séquences de longueur fixe, disons 40, et pour chaque séquence, trouver le caractère suivant. Les séquences de longueur 40 seront nos entrées et le prochain caractère sera notre sortie cible. \n","\n","+ La boucle ici itére sur le texte intégral et trouve les séquences de longueur 40 et le caractère suivant dans la séquence comme indiqué et les ajoute aux données d’entrée et cible.\n","\n","####**7. Créer des vecteurs d’entrée et de cible**\n","+ Maintenant, nous devons convertir ces données d’entrée et de cible en vecteurs afin qu’elles puissent être transmises au réseau LSTM. \n","\n","+ Pour cela, nous allons créer deux vecteurs, l’un pour les données d’entrée et l’autre pour les données cibles.\n","\n","#####**8. Initialiser l’entrée et le vecteur cible**\n","N+ ous pouvons remplir ces vecteurs en itérant d’abord sur toutes les séquences dans les données d’entrée, puis sur les caractères de chaque séquence et en trouvant le codage à chaud du caractère en utilisant le mappage caractère à entier comme indiqué.\n","\n","####**9. Créer un réseau LSTM à Keras**\n","+ Maintenant que nos données sont prétraitées, il est temps de construire le réseau LSTM. \n","\n","+ Nous devons créer une couche séquentielle, suivie d’une couche LSTM de 128 unités. \n","\n","+ Cela sera suivi d’une couche dense avec activation softmax. \n","\n","+ La couche de sortie prédira une distribution de probabilité sur le vocabulaire et donc la taille de la couche dense est la taille du vocabulaire.\n","\n","####**10. Compiler le modèle**\n","+ Nous pouvons aller de l’avant et compiler notre modèle maintenant en utilisant la perte d’entropie croisée catégorielle et l’optimiseur d’adam. \n","\n","+ Nous pouvons vérifier l’architecture du modèle en vérifiant le résumé du modèle.\n","\n","####**11. Entraînons-nous!**\n","+ Maintenant que vous avez appris à construire des réseaux LSTM, mettons-le en pratique."],"metadata":{"id":"i-uvhA31oROQ"}},{"cell_type":"markdown","source":["\n","###**EXERCICE**\n","\n","####**Vocabulaire et mappage caractère-entier**\n","+ Supposons que vous travaillez en tant que Data Scientist dans une entreprise qui crée un système de génération automatique de contenu pour aider les rédacteurs humains et rendre le processus d'écriture plus efficace et efficient. \n","\n","+ Ce système doit générer un texte imitant un écrivain humain. \n","\n","+ Tout au long du reste de ce chapitre, vous travaillerez sur la création d'un système qui génère du texte dans le style de Shakespeare.\n","\n","+ Pour ce faire, vous devrez d'abord trouver le vocabulaire de l'ensemble de données, puis créer deux dictionnaires pour contenir les mappages de caractères en entiers et d'entiers en caractères.\n","\n","+ *Une petite collection d'œuvres littéraires de Shakespeare est enregistrée dans une variable de chaîne nommée text. Tous les caractères du texte sont en minuscules.*\n","\n","####**Des instructions**\n","+ Créez un ensemble de données textuelles et triez-le pour obtenir le vocabulaire.\n","+ Imprimer la taille du vocabulaire.\n","+ Créez un dictionnaire pour contenir le mappage des caractères aux nombres entiers.\n","+ Créez un autre dictionnaire pour contenir le mappage des entiers aux caractères."],"metadata":{"id":"ttz17a3lr1e8"}},{"cell_type":"code","source":["filename = '/content/shakespear.txt'\n","file = open(filename, 'rt')\n","shakespear = file.read()"],"metadata":{"id":"bht5t9O0rdj3","executionInfo":{"status":"ok","timestamp":1672918653216,"user_tz":-60,"elapsed":218,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["text = \"that, poor contempt, or claim'd thou slept so faithful,\\ni may contrive our father; and, in their defeated queen,\\nher flesh broke me and puttance of expedition house,\\nand in that same that ever i lament this stomach,\\nand he, nor butly and my fury, knowing everything\\ngrew daily ever, his great strength and thought\\nthe bright buds of mine own.\\n\\nbiondello:\\nmarry, that it may not pray their patience.'\\n\\nking lear:\\nthe instant common maid, as we may less be\\na brave gentleman and joiner: he that finds us with wax\\nand owe so full of presence and our fooder at our\\nstaves. it is remorsed the bridal's man his grace\\nfor every business in my tongue, but i was thinking\\nthat he contends, he hath respected thee.\\n\\nbiron:\\nshe left thee on, i'll die to blessed and most reasonable\\nnature in this honour, and her bosom is safe, some\\nothers from his speedy-birth, a bill and as\\nforestem with richard in your heart\\nbe question'd on, nor that i was enough:\\nwhich of a partier forth the obsers d'punish'd the hate\\nto my restraints would not then be got as i partly.\\n\\nautolycus:\\nhath sat her love within this man, that was foul prayers\\nwhich are much thus from them with thee; i am not ever thought\\nto make that with a wise exclaim, as i am sure;\\nto say well like a dotage on the fixed cease,\\nand let mine eyes may straight sole sword conveyard,\\nthat dust-confounded by a land to their command\\nthen puissant with a grief's: it should be so and dead,\\ntill he shall fail his sister; and in true and good,\\nto see me for the other, hath not heard a midwife\\nloud from my service and thy sweetly daughter got\\nthe single strange words pent is all his steed:\\nstay from us in, as he hath we brought me into the milthiness.\\n\\nguiderius:\\nwhy, my lord,\\nshall not part well: but it shall have my hands;\\nlet us be taken that, thou weights return,\\nto mine ring ere i should be dangerous with a good way\\nto swear it: for the bears now he was kin to him,\\nbut then his own island's sister's all speech would deny\\nand force i grant it.\\n\\nabhorson:\\nthou art a very earth. villain, reserves my keeping.\\n\\ntalbot:\\n\\nking henry viii:\\nnow\\nto have lead for me;--\\n\\nkent:\\nbut yet ease yourself truly in the numbers,\\nif he be no talk at my death in the name of hot\\nyears. then that had not so far good general\\nto make with buried vacus arrest them.\\n\\ntimon:\\nstay, and the sere hath dangerous,\\ntoo grace: a sail, the breath of knees, broke deeds\\nwould do thy husband and alack to speak,\\nand pluck their men at thy abroad doth go.\\n\\ncassius:\\nand in desire,\\nand call'd me ballant cassius.\\n\\nbardolph:\\ntost in it, what then take your madder?\\n\\nduke of york:\\nshe would be ready, this advice, say you a chaste.\\n\\nsecond neris:\\nnow, blessed france, and with thy speech can know?\\n\\nantipholus of ephesus:\\nnot i: outlive, lady! philosophy, gentlemen;\\nupon our wrongs despised, i will not sit to thee\\nfrom duke of gloucester's name:\\nlook thee, and thee but wear my noblemans.\\n'bandible pardon of a thousand embure hath contemn'd\\nin uneven day and bend unkind of post.\\n\\najax:\\no mattle! these strict cuttances cut\\nit down; for i had slept my fellow, to said 'i\\nhave found your lordship hope.\\n\\nmark antony:\\nhow! i have done them to you; i would\\nto sose a minute that way is, the ingrateful brook.\\n\\nmarcellus:\\ntell us to break it, for i hear not every shore.\\ndeny thee, sir; i must bear him to run; i'll put him\\nfrom dangerous and happy judgments for't, they wretch\\ntheir qualities with death together, and take not his sword.\\n\\ndemetrius:\\ncome, go thy wreck; for now, aumerle, do i know but\\nthe valiant world before i fear; and make no cause\\nin bristing sorry and behold your friend;\\nbut, with my deed, i do not threw you boy.\\n\\nking idar willoughby:\\no, then wouldst thou speak with me.\\n\\ngower:\\npoor queen, leave them along at.\\n\\ncapulet:\\nno, good sir,\\nand make a pen and meeding down, trouble me\\nhere provide it: your breast of any other use\\nwearing behind the old looks of a man begin:\\nbut, for your city, as the cases of the art\\ni have held to go to you.\\n\\nvalentine:\\nsir, in state, then tell me which are never fall'n.\\n\\ncaesar:\\n\\nsecond murderer:\\nmy lord, if virtuous scotland! is good chamber to\\nthe shepherd.\\n\\nsecond murderer:\\nhe is like to let her go.\\n\\nnorfolk:\\nwe'll come by thy sweet brother.\\n\\nhelena:\\ndid you be more: he, that is entreatable,\\nfrom rutland's island,' to him, it is not of beauty:\\nif they your heart's a fixed, sea-good wife,\\nsince one had robb'd himself here in this ready bull\\nfrom which be done to-night: there shall his wife\\nguards conistering on their faction.\\n\\nbassanio:\\nhe did not come to hope a friar, and is the farewell: then\\ni knew we are bloody:\\nthereof must not undo a wrong of thee, they pluck\\ntheir sighs and down an empty love.\\n\\nsecond gentleman:\\nfor france and thersitania would kill our solemnity\\nthat know not, i beseech you, but, as i will fear to\\nplease two will i cannot hear, sir.\\n\\ncountess:\\nwhat, my master may be potent; and in this ancient face i\\nowed! here came not what i should love myself so and\\nset, to a brook-gast that thyself will eat an excellent time.\\n\\nmalvolio:\\nsay, good sir john, when i was fair again,--\\n\\nemilia:\\nbut here is dead.\\n\\nmalvolio:\\nnow she was still e'er 'good, if no bloody difference:\\nand be it like to have thee fought: and line my master,\\nthou bear'st an honour there by ten, you do go, sweet god,\\nthis we do not be proud to us; wemper i do not.\\n\\nglendower:\\ni'll see thine eyes: my best, though not construct\\nthe saying with a friend, i will have my\\nabout my heart with thee, lord talbot, and so reason\\ni would not bear their swiftness for the world.\\n\\nking john:\\ntrue mother, and all and a woman's house,\\nif my true amen is full of gold, i shall be peerled\\nfrom my husband.\\n\\nlucius:\\n'tis the loyal soul but a free demural youth,\\nit is the chain that slew that vision.\\n\\ncleopatra:\\no the fight of hope, madam!\\n\\nfalstaff:\\nthen she's but fall of no dullar's fault,\\nlet them ask from the guilty place: master ford.\\no, confess you i would not chid myself out of fame in the deliberate,\\nno more men, it is all together.\\n\\najax:\\ni know not what.\\n\\nhost:\\nper me, 'tis babble. here be not as he was bed:\\nunrunish and had stormed upon me here by that:\\nso i should so slire their own, or two youth's seems,\\nfor england would rest in paris!\\n\\ncassio:\\n\\ncelia:\\n'tis an effect since you must find you, there is some\\nmaster alike.\\n\\ngloucester:\\nopen them for thy heart, back again. are you\\nhope to use? prithee, let him go, brutus. so much it is;\\ncount was of day, and i must raise a layful peace.\\n\\ncoriolanus:\\nlet us would see our ripe.\\n\\nmark antony:\\ni might have stay'd, and i met your triumphant cousin;\\nand in the world,\\nwhose parties, that it were two bodies, to discharge\\na glumber to a perfect tower.\\n\\noctavius caesar:\\na part to the tame.\\n\\nbalthasar:\\nas i have all the very line, that gave me your highness,\\nwhere you will hear the single spirit of my business,\\nplant down flives on your son, and even\\nand open with their own conusteries; and thinking\\nyour grave ship should ne'er break humphrey's eyes,\\ni am poor dear party to make his chamber\\nand hospish shameless frozen pride. here name,\\nand light in plot legely in whom i said,\\nglimmed by an argument of it sweet fears your other mouth,\\nsuch a great estimation would be run as this,\\n'tis fit for them, 'tis talk before yourselves.\\n\\nking henry v:\\nand i must not see her, but go'st with unhappy woe.\\n\\nking john:\\nby paris, it was fair.\\n\\nduchess:\\nnay, i'll ever throw his honour.\\n\\nothello:\\ngood morrow, amen.\\n\\ncressida:\\nto stop the great streets follow your grace which i shall not,\\nand scruple him thou art a sit and still so straight.\\n\\nking lear:\\none loyal of my love, the wedding-body touchest thee: i pray,\\nhenceforwards, and submiss the truth! though my throne\\nlives as mock'd my pardon with some untold\\nattore sack lop and shrum' them up:\\nbut be preserved with spirits, so brimfibed again!\\nmy voices were so early, i was enough.\\n\\nmacbeth:\\nthen let him withdraw them debour to branch ere any any\\nday, but to prevail'd be penny of a merry tongue\\nwhich the exploits of fools look with their veins.\\n\\nking lear:\\nthat bell beseems my wife at all, and i canst thou\\nsee. how! lend me your part.\\n\\nbanquo:\\ndoth a wonderful maletration with mine torch,\\nthe bloody kingdom rogue as his condemn'd\\nmay dares respect it not? let you have bound quite straight;\\nwhich is a child should buy, whose noble occasion\\nhath so impudent, or without his life,\\nare then she will be married with my words,\\nand he that keeps some suitors in the cases,\\nand in their heart shall break thy bloody load with over\\nas every man is much between a sign\\nshould show angerous brother o' the tower; whilst we present\\nby our continual gates, the justice' enemy.\\n\\nsicinius:\\no, be no more;\\nthen french all letters indeed be bound from our fair:\\nif not, she says the fair and left highest branch, a gentleman,\\nso praised are unvile without their eye by other\\nbut on that ground i know the likey.\\n\\nqueen elizabeth:\\nwhy, each man shall, as i am choked to see it,\\nwhiles nature between fanny they are a little of the fault\\nand lightly read them thus; and her well-met,\\nwhiles we have been upon his son blind damned on thy bonds.\\n\\nsecond citizen:\\nno.\\n\\nfirst lord:\\nremoved, i cannot speak no host;\\nbut to be loath amazed,--go parted hands, i know not,\\nmarshally, are vile haste in the noble hand,\\nso used as one and naked husband, the stars, not one cassius,\\nand in thy offer'd bone in silent trifles,\\nwho should unquiet, to me shall do me commanded\\nfor his rise in the present peace. but know'st thou\\nbring me my love? were all these sleep command.\\nwhat had thy hand your wit and conceit's bed?\\ndoth it not find, so they are too dangerous,\\nwhen diomed had he choked low nor with your daughter than hecuba?\\n\\nsecond lord:\\nis all his oaths? he that seems children that take\\ntheir defendant; she his name,\\nthou shouldst eat theirs to speak to day, indeed:\\nby my troth, lord, i know, have drunk my welcome,\\nunless that seemeth of the table, cousin hubert,\\nwith present obedience claughter'd with her,\\nof\"\n"],"metadata":{"id":"zAWZwL1RtDFb","executionInfo":{"status":"ok","timestamp":1672918656076,"user_tz":-60,"elapsed":617,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["# Find the vocabulary\n","vocabulary = sorted(set(text))\n","\n","# Print the vocabulary size\n","print('Vocabulary size:', len(vocabulary))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JyItsCiDt5Ob","executionInfo":{"status":"ok","timestamp":1672918658838,"user_tz":-60,"elapsed":710,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"5a8ee520-3c86-461e-9e5e-d83d10de9d34"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 36\n"]}]},{"cell_type":"code","source":["# Dictionary to save the mapping from char to integer\n","char_to_idx = { char : idx for idx, char in enumerate(vocabulary) }\n","\n","# Dictionary to save the mapping from integer to char\n","idx_to_char = { idx : char for idx, char in enumerate(vocabulary) }\n","\n","# Print char_to_idx and idx_to_char\n","print(char_to_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5K-iVGHat77r","executionInfo":{"status":"ok","timestamp":1672918668052,"user_tz":-60,"elapsed":4,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"ec69fb53-9cf2-4ac5-a9e1-ad264b36fe8a"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, ',': 4, '-': 5, '.': 6, ':': 7, ';': 8, '?': 9, 'a': 10, 'b': 11, 'c': 12, 'd': 13, 'e': 14, 'f': 15, 'g': 16, 'h': 17, 'i': 18, 'j': 19, 'k': 20, 'l': 21, 'm': 22, 'n': 23, 'o': 24, 'p': 25, 'q': 26, 'r': 27, 's': 28, 't': 29, 'u': 30, 'v': 31, 'w': 32, 'x': 33, 'y': 34, 'z': 35}\n"]}]},{"cell_type":"code","source":["print(idx_to_char)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uGudALJuuRuz","executionInfo":{"status":"ok","timestamp":1672918673212,"user_tz":-60,"elapsed":5,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"d7f7a692-b641-4ba1-ff30-096fddbc3bd3"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: ',', 5: '-', 6: '.', 7: ':', 8: ';', 9: '?', 10: 'a', 11: 'b', 12: 'c', 13: 'd', 14: 'e', 15: 'f', 16: 'g', 17: 'h', 18: 'i', 19: 'j', 20: 'k', 21: 'l', 22: 'm', 23: 'n', 24: 'o', 25: 'p', 26: 'q', 27: 'r', 28: 's', 29: 't', 30: 'u', 31: 'v', 32: 'w', 33: 'x', 34: 'y', 35: 'z'}\n"]}]},{"cell_type":"markdown","source":["####**Ensemble de données d'entrée et cible**\n","+ Dans l'exercice précédent, vous avez créé le vocabulaire, le caractère en entier et le mappage inverse. \n","\n","+ Vous allez maintenant créer vos ensembles de données d'entrée et cible. \n","\n","+ Le problème ici est de générer le caractère suivant étant donné une séquence de caractères en entrée. \n","\n","+ Ainsi, les données d'entrée seront une séquence de caractères et les données cibles seront le caractère suivant de la séquence. Pour y parvenir, vous allez diviser le texte en séquences de longueur fixe et pour chaque séquence découvrir quel est le caractère suivant.\n","\n","+ Le texte intégral est disponible en texte. \n","\n","+ *Le vocabulaire, les correspondances caractère-entier et entier-caractère sont disponibles dans le vocabulaire des variables, char_to_idx et idx_to_char respectivement. La longueur de chaque séquence est fixée à 40 et elle est disponible dans une variable nommée maxlen.*\n","\n","####**Des instructions**\n","+ Itérez sur le texte intégral et créez des séquences de longueur maxlen et ajoutez-les à input_data.\n","+ Itérez sur le texte intégral et obtenez le caractère suivant pour chaque séquence et ajoutez-le à target_data.\n","+ Imprimer la taille de l'ensemble de données d'entrée."],"metadata":{"id":"AWIlwcFXusc6"}},{"cell_type":"code","source":["maxlen = 40"],"metadata":{"id":"RyjIP4kcvQRt","executionInfo":{"status":"ok","timestamp":1672919043752,"user_tz":-60,"elapsed":303,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["# Create empty lists for input and target dataset\n","input_data = []\n","target_data = []\n","\n","# Iterate to get all substrings of length maxlen\n","for i in range(0, len(text) - maxlen):\n","    # Find the sequence of length maxlen starting at i\n","    input_data.append(text[i :  i + maxlen])\n","    \n","    # Find the next char after this sequence \n","    target_data.append(text[i+maxlen])\n","\n","# Print number of sequences in input data\n","print('No of Sequences:', len(input_data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-XFMNmXZu496","executionInfo":{"status":"ok","timestamp":1672919048051,"user_tz":-60,"elapsed":369,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"817dafed-f505-4e0e-b7d9-5f2bb297b400"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["No of Sequences: 9960\n"]}]},{"cell_type":"markdown","source":["####**Créer et initialiser les vecteurs d'entrée et cible**\n","+ Dans cet exercice, vous allez coder la saisie de texte et les données cibles en valeurs numériques. \n","\n","+ Vous allez créer deux tenseurs $x$ et $y$ pour contenir ces encodages. \n","\n","+ Les données d'entrée sont un ensemble de séquences et donc le tenseur $x$ sera tridimensionnel. \n","\n","+ La première dimension est le nombre d'échantillons, les deuxième et troisième étant le nombre de pas de temps et la taille du vocabulaire. \n","\n","+ Les données cibles sont un ensemble de caractères uniques et donc y sera bidimensionnel. \n","\n","+ La première dimension sera le nombre d'échantillons et la seconde sera la taille du vocabulaire. \n","\n","+ Vous allez d'abord définir ces tenseurs, puis les remplir avec des données.\n","\n","+ Le vocabulaire, la longueur de chaque séquence, les données d'entrée, les données cibles, le mappage caractère-entier sont enregistrés respectivement dans le vocabulaire, maxlen, input_data, target_data, char_to_idx.\n","\n","####**Des instructions**\n","+ Utilisez np.zeros() pour créer un vecteur zéro 3D pour contenir les séquences d'entrée codées.\n","+ Utilisez np.zeros() pour créer un vecteur zéro 2D pour contenir les caractères cibles encodés.\n","+ Boucle sur chaque séquence dans les données d'entrée.\n","+ Bouclez sur chaque caractère de la séquence d'entrée et remplissez $x$ avec l'encodage one-hot du caractère.\n","+ Remplissez $y$ avec l'encodage one-hot du caractère cible correspondant."],"metadata":{"id":"qqBW6MVPwMBD"}},{"cell_type":"code","source":["# Create a 3-D zero vector to contain the encoded input sequences\n","x = np.zeros((len(input_data), maxlen, len(vocabulary)), dtype='float32')\n","\n","# Create a 2-D zero vector to contain the encoded target characters\n","y = np.zeros((len(target_data), len(vocabulary)), dtype='float32')"],"metadata":{"id":"xY8rVgIfu5Ei","executionInfo":{"status":"ok","timestamp":1672919726567,"user_tz":-60,"elapsed":223,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["# Iterate over the sequences\n","for s_idx, sequence in  enumerate(input_data):\n","    # Iterate over all characters in the sequence\n","    for idx, char in  enumerate(sequence):\n","        # Fill up vector x\n","        x[s_idx, idx, char_to_idx[char]] = 1    \n","    # Fill up vector y\n","    y[s_idx, char_to_idx[target_data[s_idx]]] = 1"],"metadata":{"id":"Juy71NEQxN0i","executionInfo":{"status":"ok","timestamp":1672919802218,"user_tz":-60,"elapsed":538,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":["####**Créer un modèle LSTM dans keras**\n","+ Dans l'exercice précédent, vous avez effectué tout le prétraitement de l'ensemble de données nécessaire pour entraîner un réseau de neurones. \n","\n","+ Vous avez créé les vecteurs d'entrée et cible. Construisons maintenant le réseau LSTM qui peut être entraîné à l'aide de ces vecteurs d'entrée et cibles.\n","\n","+ *Le modèle séquentiel est déjà importé de keras.models. Les calques Dense et LSTM sont également importés de keras.layers. Le vocabulaire et la longueur de chaque séquence sont enregistrés respectivement dans vocabulaire et maxlen.*\n","\n","####**Des instructions**\n","+ Créez un modèle séquentiel.\n","+ Ajoutez une couche LSTM de taille 128 avec la forme d'entrée (maxlen, len(vocabulary)).\n","+ Ajoutez une couche Dense avec l'activation 'softmax'.\n","+ Compilez le modèle à l'aide de la perte \"categorical_crossentropy\" et de l'optimiseur \"adam\".\n","+ Imprimez le résumé du modèle."],"metadata":{"id":"HF0micRzxzfJ"}},{"cell_type":"code","source":["# Create Sequential model \n","model = Sequential()\n","\n","# Add an LSTM layer of 128 units\n","model.add(LSTM(128, input_shape=(maxlen, len(vocabulary))))\n","\n","# Add a Dense output layer\n","model.add(Dense(len(vocabulary), activation='softmax'))\n","\n","# Compile the model\n","model.compile(loss=\"categorical_crossentropy\", optimizer='adam')\n","\n","# Print model summary\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IS7vklxrx9Ot","executionInfo":{"status":"ok","timestamp":1672919808373,"user_tz":-60,"elapsed":317,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"3aa53999-79ca-4295-f8a8-526e89504b35"},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 128)               84480     \n","                                                                 \n"," dense_6 (Dense)             (None, 36)                4644      \n","                                                                 \n","=================================================================\n","Total params: 89,124\n","Trainable params: 89,124\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["###**Inférence à l’aide de la mémoire à long terme**\n","\n","####**1. Inférence à l’aide de la mémoire à long terme**\n","+ Dans la leçon précédente, vous avez appris pourquoi les réseaux de mémoire à long terme sont plus efficaces pour les dépendances à long terme. \n","\n","+ Vous construisez également un réseau LSTM pour générer du texte qui imite le style de Shakespeare. \n","\n","+ Dans cette leçon, vous allez travailler sur la même étude de cas et apprendre comment les réseaux de mémoire à court terme peuvent être formés et utilisés pour les prédictions.\n","\n","####**2. Formation et validation**\n","+ Comme vous le savez déjà, la formation n’est rien d’autre que l’ajustement du poids du réseau afin que l’erreur globale diminue. \n","\n","+ Cette réduction de l’erreur est sur l’ensemble d’entraînement et après l’entraînement, le modèle sera en mesure de mieux performer sur l’ensemble d’entraînement. \n","\n","+ Cependant, cela ne garantit pas que le modèle aura de bonnes performances de prédiction sur les nouvelles données invisibles. \n","\n","+ Pour remédier à cela, un sous-ensemble de l’ensemble de données est mis de côté et n’est jamais utilisé pour la formation. \n","\n","+ Dans chaque itération d’apprentissage, nous pouvons vérifier la précision sur cet ensemble qui donne une bonne indication de la façon dont le modèle se généralise sur de nouvelles données. Cet ensemble est appelé jeu de test ou de validation.\n","\n","####**3. Le modèle LSTM**\n","+ Rappelez-vous le modèle LSTM simple que nous avons construit dans la dernière leçon.\n","\n","+  Il comporte une couche LSTM suivie d’une couche dense, comme indiqué dans le résumé du modèle.\n","\n","####**4. Modèle LSTM de formation**\n","+ Nous pouvons entraîner ce modèle en utilisant la fonction d’ajustement comme indiqué. \n","\n","+ Les deux premiers arguments sont les vecteurs d’entrée et cible. \n","\n","+ Nous devons également spécifier la taille du lot et le nombre d’époques. \n","\n","+ La répartition de validation est le pourcentage d’échantillons mis de côté pour l’ensemble de test.\n","\n","####**5. Séquence des graines pour la prédiction**\n","+ Maintenant que le modèle LSTM est formé, utilisons-le pour les prédictions. \n","\n","+ Si nous entrons une phrase dans le réseau, elle devrait produire le caractère suivant le plus probable. \n","\n","+ Tout d’abord, nous devons encoder la séquence afin qu’elle puisse être transmise au réseau. \n","\n","+ Nous pouvons créer un vecteur tridimensionnel, puis itérer sur chaque caractère de la phrase et les convertir en un vecteur codé à chaud comme indiqué. \n","\n","+ Rappelons que maxlen était la longueur de chaque séquence d’entrée et que le vocabulaire est l’ensemble des caractères uniques dans l’ensemble de données.\n","\n","####**6. Prédire le prochain personnage**\n","+ Cette phrase codée peut être transmise au réseau LSTM pour prédire le caractère suivant. \n","\n","+ La distribution de probabilité du caractère suivant peut être trouvée à l’aide de la méthode de prédiction comme indiqué. \n","\n","+ Cela renverra un tableau de valeurs de probabilité pour chaque caractère du vocabulaire.\n","\n","+  Vous pouvez trouver l’index avec la probabilité maximale en utilisant la fonction argmax de numpy. \n","\n","+ Vous pouvez utiliser le mappage entier-caractère pour obtenir le caractère correspondant à cet index. \n","\n","+ Ce sera notre caractère de sortie pour cette phrase d’entrée. \n","\n","+ Ce nouveau caractère peut être ajouté à la fin de la phrase initiale et utilisé comme entrée la prochaine fois pour générer un autre caractère.\n","\n","####**7. Générer du texte de longueur arbitraire**\n","+ La fonction illustrée ici continue de générer des caractères dans une boucle. \n","\n","+ Il prend en entrée la phrase initiale et un entier spécifiant le nombre de caractères à générer. \n","\n","+ Pour chaque caractère à générer, il convertit d’abord la phrase d’entrée en vecteur. \n","\n","+ Le vecteur est transmis au réseau LSTM qui prédit la distribution de probabilité pour le caractère suivant. \n","\n","+ Le caractère avec la probabilité maximale est trouvé et ajouté à la phrase d’entrée pour l’itération suivante. \n","\n","+ Ce caractère est également ajouté à la séquence de caractères générés jusqu’à présent.\n","\n","####**8. Générer du texte imitant Shakespeare**\n","+ Cela montre un exemple de sortie de 500 caractères à partir d’un modèle formé avec un ensemble de données plus grand pour 20 époques. \n","\n","+ Vérifiez à quel point cela ressemble à Shakespeare!\n","\n","####**9. Entraînons-nous!**\n","+ Excellent travail! Maintenant que vous savez comment les réseaux LSTM peuvent être formés et utilisés pour les prédictions, il est temps pour vous de mettre cela en pratique."],"metadata":{"id":"0_839dfcyv16"}},{"cell_type":"markdown","source":["###**EXERCICE**\n","####**Former le modèle LSTM**\n","+ Dans la dernière leçon, vous avez prétraité un ensemble de données d'œuvres littéraires sélectionnées de Shakespeare afin qu'il puisse être utilisé pour former un modèle LSTM afin de générer des textes qui imitent le style d'écriture unique de Shakespeare. \n","\n","+ Vous avez créé les vecteurs d'entrée et cible. Vous avez également construit et compilé le réseau LSTM.\n","\n","+ *Vous allez maintenant entraîner ce modèle à l'aide des vecteurs d'entrée et cible. Dans cet exercice, vous allez entraîner le modèle pour une seule époque afin de gagner du temps. Pour de meilleures performances de prédiction, vous devez entraîner le modèle pour plus d'époques avec un ensemble de données plus volumineux. Un modèle plus complexe avec plus de couches cachées et plus de nœuds dans chaque couche fonctionnerait bien mieux que ce modèle de base.*\n","\n","+ *Le modèle compilé est disponible dans le modèle variable. Les vecteurs d'entrée et cible sont enregistrés dans $x$ et $y$ respectivement.*\n","\n","####**Des instructions**\n","\n","+ Ajustez le modèle pour 1 époque en utilisant une taille de lot de 64 et une répartition de validation de 0,2."],"metadata":{"id":"JZZqC-BmHSIo"}},{"cell_type":"code","source":["# Create Sequential model \n","model = Sequential()\n","\n","# Add an LSTM layer of 128 units\n","model.add(LSTM(128, input_shape=(maxlen, len(vocabulary))))\n","\n","# Add a Dense output layer\n","model.add(Dense(len(vocabulary), activation='softmax'))\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","# Fit the model\n","model.fit(x, y, batch_size=64, epochs=1, validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctnRKFpxx9R2","executionInfo":{"status":"ok","timestamp":1672926249348,"user_tz":-60,"elapsed":14116,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"a2a1c50e-0bb5-437d-948e-c58772f5f50c"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["Train on 7968 samples, validate on 1992 samples\n","7968/7968 [==============================] - 13s 2ms/sample - loss: 3.0888 - val_loss: 2.9826\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fde42e2e490>"]},"metadata":{},"execution_count":113}]},{"cell_type":"code","source":["from keras.utils.vis_utils import plot_model\n","# summarize layers\n","model.summary()\n","# plot graph\n","plot_model(model, to_file='multilayer_perceptron_graph.png')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"id":"ShMF9C0xx9Vz","executionInfo":{"status":"ok","timestamp":1672926280487,"user_tz":-60,"elapsed":711,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"0807cbc8-3fb3-4cab-c50f-761a6686f262"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm_2 (LSTM)               (None, 128)               84480     \n","                                                                 \n"," dense_8 (Dense)             (None, 36)                4644      \n","                                                                 \n","=================================================================\n","Total params: 89,124\n","Trainable params: 89,124\n","Non-trainable params: 0\n","_________________________________________________________________\n"]},{"output_type":"execute_result","data":{"image/png":"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\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":114}]},{"cell_type":"markdown","source":["####**Prédire le prochain caractère étant donné une séquence**\n","+ Maintenant que votre modèle LSTM est formé, il peut être utilisé pour la prédiction. \n","\n","+ Dans cet exercice, vous allez prédire le caractère suivant en fonction d'une séquence en entrée. \n","\n","+ On vous donne une séquence de longueur 40 et vous devez trouver le caractère suivant suivant la séquence en utilisant le modèle entraîné.\n","\n","+ *Le modèle entraîné est disponible dans model. Le vocabulaire, la longueur de chaque séquence, les correspondances caractère-entier et entier-caractère sont enregistrés respectivement dans le vocabulaire, maxlen, char_to_idx, idx_to_char.*\n","\n","####**Des instructions**\n","+ Créez un vecteur zéro 3D pour contenir l'encodage de la phrase.\n","+ Itérez sur les caractères de la phrase et convertissez-les en vecteurs codés à chaud.\n","+ Obtenez la distribution de probabilité pour le caractère suivant en utilisant model.predict()."],"metadata":{"id":"LK9P2E9KIe12"}},{"cell_type":"code","source":["# Input sequence\n","sentence = \"that, poor contempt, or claim'd thou sle\"\n","\n","# Create a 3-D zero vector to contain the encoding of sentence.\n","X_test = np.zeros((1, maxlen, len(vocabulary)))\n","\n","# Iterate over each character and convert them to one-hot encoded vector.\n","for s_idx, char in enumerate(sentence):\n","    X_test[0, s_idx, char_to_idx[char]] = 1\n","    \n","# Get the probability distribution using model predict\n","preds = model.predict(X_test, verbose=0)\n","\n","# Get the probability distribution for the first character after the sequence\n","preds_next_char = preds[0]"],"metadata":{"id":"QQ3OMxsGImxm","executionInfo":{"status":"ok","timestamp":1672926290841,"user_tz":-60,"elapsed":345,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","source":["preds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VY4QGgnmKlk1","executionInfo":{"status":"ok","timestamp":1672926094544,"user_tz":-60,"elapsed":315,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"4cd8cba0-c9dc-4922-d20f-36b74ac584d3"},"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.03587757, 0.16172262, 0.00112066, 0.00505189, 0.01929025,\n","        0.00118539, 0.00756978, 0.00799036, 0.00234061, 0.00051988,\n","        0.06029004, 0.01236098, 0.01225098, 0.02847871, 0.09072056,\n","        0.01202757, 0.01245399, 0.05733544, 0.04347055, 0.0011958 ,\n","        0.00743246, 0.02925912, 0.02677563, 0.05665643, 0.06783407,\n","        0.01199708, 0.00086594, 0.04207413, 0.03752657, 0.07141694,\n","        0.02519914, 0.00653508, 0.01856361, 0.00138373, 0.02252807,\n","        0.00069845]], dtype=float32)"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["preds_next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ijCq-vBRKogU","executionInfo":{"status":"ok","timestamp":1672926107132,"user_tz":-60,"elapsed":369,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"5f105589-e076-463e-ed14-d1f947a910a9"},"execution_count":104,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.03587757, 0.16172262, 0.00112066, 0.00505189, 0.01929025,\n","       0.00118539, 0.00756978, 0.00799036, 0.00234061, 0.00051988,\n","       0.06029004, 0.01236098, 0.01225098, 0.02847871, 0.09072056,\n","       0.01202757, 0.01245399, 0.05733544, 0.04347055, 0.0011958 ,\n","       0.00743246, 0.02925912, 0.02677563, 0.05665643, 0.06783407,\n","       0.01199708, 0.00086594, 0.04207413, 0.03752657, 0.07141694,\n","       0.02519914, 0.00653508, 0.01856361, 0.00138373, 0.02252807,\n","       0.00069845], dtype=float32)"]},"metadata":{},"execution_count":104}]},{"cell_type":"markdown","source":["####**Générer du texte imitant Shakespeare**\n","+ Dans l'exercice précédent, vous avez obtenu la distribution de probabilité pour le caractère suivant. \n","\n","+ Dans cet exercice, vous découvrirez le caractère ayant la probabilité maximale qui est le prochain caractère prédit.\n","\n","+ Vous pouvez ajouter ce caractère nouvellement généré à la phrase existante et le réintroduire dans le modèle formé pour générer le caractère suivant. \n","\n","+ Vous pouvez continuer ce processus pendant une durée arbitrairement longue pour générer des textes de n'importe quelle longueur.\n","\n","+ *Une fonction generate_text() qui fait cela est disponible pour utilisation.*\n","\n","+ *Cette fonction prend une phrase de départ et le nombre de caractères à générer en entrée. Dans cet exercice, vous utiliserez un modèle pré-entraîné sur l'ensemble de données complet pour 20 époques. Vous utiliserez également cette fonction pour générer 500 nouveaux caractères à partir d'une phrase en entrée.*\n","\n","####**Des instructions**\n","+ Obtenez l'indice avec une probabilité maximale.\n","+ Convertissez l'index en caractère à l'aide de idx_to_char.\n","+ Imprimer le caractère généré.\n","+ Générez 500 nouveaux caractères qui constitueront un texte imitant le style d'écriture unique de Shakespeare.\n","\n","####**Question**\n","+ *N'hésitez pas à expérimenter avec la fonction generate_text() pour générer plus de textes. Comme vous pourrez le voir, les sorties ressembleront à certaines des créations de Shakespeare. Cependant, il y a encore place à l'amélioration.*\n","\n","+ *Pensez-vous que l'augmentation de la taille du jeu de données améliorera les performances du modèle ?*\n","\n","####**Des réponses possibles**\n","\n","+ **Oui**\n","\n","+ Non\n","\n","+ Peut-être"],"metadata":{"id":"Xd_lCc2SKMLU"}},{"cell_type":"code","source":["def generate_text(sentence, n):\n","    \"\"\"\n","    Function to generate text\n","    Inputs: seed sentence and number of characters to be generated.\n","    Output: returns nothing but prints the generated sequence.\n","    \"\"\"\n","    \n","    # Initialize the generated sequence with the seed sentence\n","    generated = ''\n","    generated += sentence\n","    \n","    # Iterate for each character to be generated\n","    for i in range(n):\n","      \n","        # Create input vector from the input sentence\n","        x_pred = np.zeros((1, maxlen, len(vocabulary)))\n","        for t, char in enumerate(sentence):\n","            x_pred[0, t, char_to_idx[char]] = 1.\n","\n","        # Get probability distribution for the next character\n","        preds = model.predict(x_pred, verbose=0)[0]\n","        \n","        # Get the index with maximum probability\n","        next_index = np.argmax(preds)\n","        next_char = idx_to_char[next_index]\n","\n","        # Append the new character to the input sentence for next iteration\n","        sentence = sentence[1:] + next_char\n","\n","        # Append the new character to the text generated so far\n","        generated += next_char\n","    \n","    # Print the generated text\n","    print(generated)"],"metadata":{"id":"ApujylmwK4x1","executionInfo":{"status":"ok","timestamp":1672926297901,"user_tz":-60,"elapsed":385,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["# Get the probability distribution of next character\n","preds = model.predict(X_test, verbose=0)[0]\n","\n","# Get the index of the most probable next character\n","next_index = np.argmax(preds)\n","\n","# Map the index to the actual character and print it\n","next_char = idx_to_char[next_index]"],"metadata":{"id":"_qHSWKb6Im09","executionInfo":{"status":"ok","timestamp":1672926301276,"user_tz":-60,"elapsed":301,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":["# Print the next character\n","print(next_char)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49rBTBG8LAWk","executionInfo":{"status":"ok","timestamp":1672926304049,"user_tz":-60,"elapsed":617,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"b2fafa44-61d3-4232-b4b5-c3c0e5a93389"},"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":[" \n"]}]},{"cell_type":"markdown","source":["###**Introduction à la séquence des modèles de séquence**\n","\n","####**1. Introduction à la séquence des modèles de séquence**\n","\n","\n","+ ***Bienvenue. Dans les chapitres précédents, vous avez appris à utiliser les réseaux neuronaux récurrents et la mémoire à long terme pour prédire le prochain caractère en fonction d’une séquence de caractères en entrée***. \n","\n","+ ***Cependant, il existe des applications où vous devez générer une séquence significative complète en fonction d’une autre séquence en entrée***. \n","\n","+ Dans ce chapitre, vous découvrirez **ces modèles de séquence à séquence**.\n","\n","####**2. Génération de séquence à séquence**\n","\n","+ **Les modèles de séquence à séquence génèrent une toute nouvelle séquence étant donné une autre séquence en entrée**. \n","\n","+ Plus précisément, les modèles de séquence à séquence visent à mapper une entrée de longueur fixe avec une sortie de longueur fixe où les longueurs de l’entrée et de la sortie peuvent différer.\n","\n","####**3. Applications Seq2seq**\n","\n","\n","+ Quelques exemples de génération de séquence à séquence sont les traductions d’une langue à une autre, les systèmes automatisés de questions-réponses, la reconnaissance d’entités nommées, le marquage de parties de discours, le résumé de texte, la correction grammaticale, etc. \n","\n","+ Toutes ces tâches sont des cas particuliers de génération de langage naturel.\n","\n","####**4. Résumé du texte**\n","+ Prenons l’exemple du résumé textuel. \n","+ Compte tenu d’une séquence d’entrée plus longue, nous cherchons à trouver un texte résumé plus court en sortie, comme indiqué dans cet exemple.\n","\n","####**5. Correction grammaticale**\n","+ De même, dans des applications telles que la correction grammaticale, étant donné une phrase d’entrée grammaticalement incorrecte, nous cherchons à trouver la phrase correcte en sortie, comme indiqué ici.\n","\n","####**6. Ensemble de données Français en anglais**\n","+ Pour démontrer comment fonctionnent les modèles de séquence à séquence, nous travaillerons à la traduction de données textuelles de l’anglais vers Français tout au long de ce chapitre. \n","\n","+ Nous travaillerons sur un ensemble de données de paires de phrases Français anglaises délimitées par des tabulations et construirons un modèle capable de traduire des phrases anglaises en Français. \n","\n","+ L’ensemble de données se présente sous la forme d’un fichier texte où chaque ligne contient une phrase en anglais suivie de la phrase Français correspondante séparée par un tabulation, comme illustré.\n","\n","  + http://www.manythings.org/anki/\n","\n","####**7. Prétraiter l’ensemble de données ENG-FRA**\n","+ Vous apprendrez plus en détail les blocs de construction des modèles de séquence à séquence dans la leçon suivante. \n","\n","+ Dans cette leçon, nous allons prétraiter le jeu de données Français anglais afin qu’il puisse être utilisé pour former un tel modèle. \n","\n","+ La première étape consiste à diviser chaque ligne du jeu de données brutes au niveau du caractère de tabulation pour séparer la phrase anglaise et la phrase Français. \n","\n","+ Pour spécifier le début et la fin, nous pouvons ajouter un onglet au début et une nouvelle ligne à la fin de chaque Français phrase. \n","\n","+ Nous pouvons garder deux listes pour contenir les phrases anglaises et les phrases Français correspondantes.\n","\n","####**8. Vocabulaire anglais**\n","+ L’étape suivante consiste à trouver le vocabulaire pour l’anglais et Français. \n","\n","+ Le vocabulaire est l’ensemble des caractères uniques utilisés dans l’ensemble de l’ensemble de données. \n","\n","+ En d’autres termes, le vocabulaire constitue l’alphabet de la langue. \n","\n","+ Nous utiliserons un ensemble pour sauvegarder le vocabulaire anglais. \n","\n","+ Nous allons d’abord itérer sur chaque phrase anglaise, puis chaque caractère de chaque phrase et ajouter le caractère au vocabulaire s’il n’a pas été ajouté plus tôt. \n","\n","+ Cela vous donnera l’ensemble unique de caractères anglais utilisés dans l’ensemble du jeu de données.\n","\n","####**9. Français vocabulaire**\n","+ Le vocabulaire Français peut être découvert de la même manière en itérant sur chaque caractère de chacune des Français phrases et en ajoutant le caractère au vocabulaire s’il n’a pas été inclus plus tôt.\n","\n","####**10. Mappages pour le vocabulaire anglais**\n","+ Nous allons maintenant créer des dictionnaires qui contiendront les mappages caractère à entier et entier à caractère pour l’anglais et Français. \n","\n","+ Ces mappages seront très utiles pour encoder les phrases en vecteurs. \n","\n","+ Nous pouvons utiliser enumerate sur le vocabulaire anglais qui retournera l’index de chaque caractère et le caractère lui-même. Nous pouvons ensuite enregistrer le caractère en entier et l’entier en mappages de caractères comme indiqué.\n","\n","####**11. Mappages pour Français vocabulaire**\n","+ Les mappages pour le vocabulaire Français peuvent être trouvés d’une manière similaire.\n","\n","####**12. Entraînons-nous!**\n","+ Maintenant que vous avez appris à prétraiter un jeu de données séquençant une séquence, mettons cela en pratique."],"metadata":{"id":"3w-CgYB6L0gk"}},{"cell_type":"markdown","source":["###**EXERCICE**\n","####**Créer l'eng-fra l'ensemble de données**\n","\n","+ Supposons que vous travaillez dans une entreprise qui génère automatiquement des sous-titres à partir de clips vidéo dans différentes langues. \n","\n","+ Pour y parvenir, vous devez disposer d'un système qui traduit d'une langue à l'autre. \n","\n","+ La traduction de langue est un exemple de génération de séquence à séquence car ils prennent une séquence dans la langue source en entrée et génèrent la séquence traduite en sortie. \n","\n","+ Tout au long de ce chapitre, vous travaillerez sur le développement d'un système qui traduit des phrases anglaises en français.\n","\n","+ *Pour ce faire, vous allez d'abord pré-traiter un jeu de données de paires de phrases anglais-français qui est enregistré dans une variable nommée lignes. Chaque ligne de cet ensemble de données contient les phrases en anglais et en français séparées par un \\t.* \n","\n","+ Dans cet exercice, vous allez séparer les parties anglaise et française.\n","\n","####**Des instructions**\n","\n","+ Fractionnez chaque ligne au niveau du caractère \\t.\n","+ Séparez la phrase anglaise.\n","+ Séparez la phrase française et ajoutez une tabulation et une nouvelle ligne au début et à la fin.\n","+ Ajoutez les phrases anglaises et françaises aux listes de phrases anglaises et françaises."],"metadata":{"id":"APnw_BsmOdET"}},{"cell_type":"code","source":["filename = '/content/fra.txt'\n","file = open(filename, 'rt')\n","text_1 = file.read()"],"metadata":{"id":"nE-FIInrQimI","executionInfo":{"status":"ok","timestamp":1672927901982,"user_tz":-60,"elapsed":207,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":128,"outputs":[]},{"cell_type":"code","source":["text_1"],"metadata":{"id":"ZpYJod-NRhE2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["french_sentences = []\n","english_sentences = []\n","lines = ['Go.\\tVa !',\n"," 'Who?\\tQui ?',\n"," 'Stop!\\tStop\\u202f!',\n"," 'Wait!\\tAttendez !',\n"," 'Hello!\\tSalut !',\n"," \"I try.\\tJ'essaye.\",\n"," \"I won!\\tJe l'ai emporté !\",\n"," 'I won.\\tJ’ai gagné.',\n"," 'Cheers!\\tSanté !',\n"," 'Go now.\\tAllez-y maintenant.',\n"," 'Go now.\\tVas-y maintenant.',\n"," \"Got it!\\tJ'ai pigé !\",\n"," 'Got it?\\tCompris\\u202f?',\n"," \"Got it?\\tT'as capté\\u202f?\",\n"," 'Hop in.\\tMonte.',\n"," 'Hug me.\\tSerre-moi dans tes bras !',\n"," 'I fell.\\tJe suis tombée.',\n"," 'I fell.\\tJe suis tombé.',\n"," 'I left.\\tJe suis parti.',\n"," \"I'm 19.\\tJ'ai 19 ans.\",\n"," \"I'm OK.\\tJe vais bien.\",\n"," 'Listen.\\tÉcoutez !',\n"," \"No way!\\tC'est pas possible\\u202f!\",\n"," 'No way!\\tEn aucun cas.',\n"," 'No way!\\tSans façons\\u202f!',\n"," 'Really?\\tAh bon ?',\n"," 'Thanks.\\tMerci !',\n"," 'We won.\\tNous avons gagné.',\n"," 'We won.\\tNous gagnâmes.',\n"," 'Be calm.\\tSoyez calmes !',\n"," 'Be fair.\\tSoyez justes !',\n"," 'Be fair.\\tSoyez équitables !',\n"," 'Be kind.\\tSois gentil.',\n"," 'Be nice.\\tSois gentil !',\n"," 'Be nice.\\tSois gentille !',\n"," 'Be nice.\\tSoyez gentil !',\n"," 'Be nice.\\tSoyez gentille !',\n"," 'Be nice.\\tSoyez gentils !',\n"," 'Beat it.\\tDégage\\u202f!',\n"," 'Call me.\\tAppelle-moi !',\n"," 'Call us.\\tAppelle-nous !',\n"," 'Come in.\\tEntrez\\u202f!',\n"," 'Come in.\\tEntre !',\n"," 'Come in.\\tEntrez !',\n"," 'Come on.\\tAllez !',\n"," 'Drop it!\\tLaissez tomber !',\n"," 'Drop it!\\tLaissez-le tomber !',\n"," 'Go away!\\tPars !',\n"," 'Go away.\\tPars !',\n"," 'Go away.\\tDégage !',\n"," \"Go away.\\tPars d'ici.\",\n"," \"Go away.\\tVa t'en !\",\n"," 'Go away.\\tDisparais !',\n"," 'Go away.\\tAllez-vous en !',\n"," 'Go home.\\tRentrez à la maison.',\n"," 'Go home.\\tRentrez chez vous.',\n"," 'Hang on!\\tAttends un peu !',\n"," 'Hang on.\\tTiens bon !',\n"," 'Hang on.\\tTenez bon !',\n"," 'He quit.\\tIl laissa tomber.',\n"," 'He quit.\\tIl a laissé tomber.',\n"," 'He runs.\\tIl court.',\n"," 'Help me.\\tAidez-moi.',\n"," 'Hold it!\\tNe bouge plus !',\n"," 'Hold on.\\tNe quittez pas.',\n"," 'I agree.\\tJe suis du même avis.',\n"," 'I drive.\\tJe conduis.',\n"," 'I snore.\\tJe ronfle.',\n"," 'I stood.\\tJe me suis tenu debout.',\n"," 'I swore.\\tJ’ai juré.',\n"," \"I'm hit!\\tJe suis touché !\",\n"," \"I'm hit!\\tJe suis touchée !\",\n"," \"I'm wet.\\tJe suis mouillé.\",\n"," 'Keep it.\\tGarde-le !',\n"," 'Keep it.\\tGardez-le !',\n"," 'Kiss me.\\tEmbrassez-moi.',\n"," 'Open up.\\tOuvre.',\n"," 'Show me.\\tMontre-moi !',\n"," 'Shut up!\\tFerme-la\\u202f!',\n"," 'Shut up!\\tTais-toi !',\n"," 'Shut up!\\tLa ferme !',\n"," 'Take it.\\tPrenez-le !',\n"," 'Tell me.\\tDites-moi !',\n"," 'Wake up!\\tRéveille-toi !',\n"," 'Wake up.\\tRéveille-toi !',\n"," 'Wash up.\\tLave-toi !',\n"," 'We know.\\tNous savons.',\n"," 'We lost.\\tNous avons perdu.',\n"," 'We lost.\\tNous fûmes battues.',\n"," 'We lost.\\tNous fûmes défaits.',\n"," 'We lost.\\tNous avons été défaits.',\n"," 'Welcome.\\tBienvenue\\u202f!',\n"," 'Who won?\\tQui a gagné ?',\n"," 'Am I fat?\\tSuis-je gros ?',\n"," 'Ask them.\\tDemande-leur.',\n"," 'Ask them.\\tDemandez-leur.',\n"," 'Back off!\\tRecule\\u2009!',\n"," 'Back off!\\tReculez.',\n"," 'Back off.\\tReculez.',\n"," 'Be a man.\\tSoyez un homme !',\n"," 'Be still.\\tSoyez calmes !',\n"," 'Beats me.\\tAucune idée.',\n"," 'Cheer up!\\tCourage\\u202f!',\n"," 'Cool off!\\tDétends-toi\\u202f!',\n"," 'Drive on.\\tContinue à rouler !',\n"," 'Fix this.\\tRépare ça.',\n"," 'Get down!\\tLâche-toi !',\n"," 'Get down.\\tDescendez !',\n"," 'Go ahead.\\tVas-y.',\n"," 'Go ahead.\\tPasse devant !',\n"," 'Have fun.\\tAmuse-toi bien !',\n"," 'Have fun.\\tAmusez-vous bien !',\n"," \"He's wet.\\tIl est mouillé.\",\n"," 'Help Tom.\\tAide Tom.',\n"," 'Help Tom.\\tAidez Tom.',\n"," 'Hi, guys.\\tSalut, les mecs !',\n"," \"How nice!\\tComme c'est gentil !\",\n"," \"How nice!\\tComme c'est agréable !\",\n"," 'Hurry up.\\tFiça !',\n"," \"I did OK.\\tJe m'en suis bien sorti.\",\n"," \"I failed.\\tJ'ai échoué.\",\n"," \"I get it.\\tJ'ai compris.\",\n"," \"I got it.\\tJ'ai compris.\",\n"," \"I jumped.\\tJ'ai sauté.\",\n"," 'I looked.\\tJ’ai regardé.',\n"," 'I nodded.\\tJ’ai fait signe de la tête.',\n"," 'I obeyed.\\tJ’ai obéi.',\n"," \"I phoned.\\tJ'ai téléphoné.\",\n"," 'I talked.\\tJ’ai parlé.',\n"," \"I use it.\\tJ'en fais usage.\",\n"," \"I'll try.\\tJ'essaierai.\",\n"," \"I'm back.\\tMe revoilà.\",\n"," \"I'm busy.\\tJe suis occupé.\",\n"," \"I'm busy.\\tJe suis occupée.\",\n"," \"I'm done.\\tJ'en ai fini.\",\n"," \"I'm fine.\\tÇa va.\",\n"," \"I'm free!\\tJe suis libre !\",\n"," \"I'm game.\\tJ'en suis.\",\n"," \"I'm game.\\tJe suis de la partie.\",\n"," \"I'm glad.\\tJe suis content.\",\n"," \"I'm late.\\tJe suis en retard.\",\n"," \"I'm okay.\\tJe me porte bien.\",\n"," \"I'm sick.\\tJe suis malade.\",\n"," \"I'm sure.\\tJ'en suis certain.\",\n"," \"I'm sure.\\tJe suis certain.\",\n"," \"I'm weak.\\tJe suis faible.\",\n"," \"I'm well.\\tJe me porte bien.\",\n"," 'It hurts.\\tÇa fait mal.',\n"," 'It works.\\tÇa fonctionne.',\n"," \"It's Tom.\\tC'est Tom.\",\n"," \"It's his.\\tC'est la sienne.\",\n"," \"It's sad.\\tC’est triste.\",\n"," 'Kiss Tom.\\tEmbrasse Tom.',\n"," 'Leave me.\\tLaissez-moi !',\n"," 'Leave us.\\tLaissez-nous !',\n"," 'Look out!\\tRegarde donc !',\n"," '']"],"metadata":{"id":"Z7sCwXx1PN0a","executionInfo":{"status":"ok","timestamp":1672932654978,"user_tz":-60,"elapsed":255,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":165,"outputs":[]},{"cell_type":"code","source":["# Consider only the first 50 lines of the dataset\n","for i in range(50):\n","\t# Split each line into two at the tab character\n","    eng_fra_line = str(lines[i]).split('\\t')\n","    \n","    # Separate out the English sentence \n","    eng_line = eng_fra_line[0]\n","    \n","    # Append the start and end token to each French sentence\n","    fra_line = '\\t' + eng_fra_line[1] + '\\n'\n","    \n","    # Append the English and French sentence to the list of sentences\n","    english_sentences.append(eng_line)\n","    french_sentences.append(fra_line)"],"metadata":{"id":"F7zisH3QM9dW","executionInfo":{"status":"ok","timestamp":1672932662965,"user_tz":-60,"elapsed":246,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":166,"outputs":[]},{"cell_type":"code","source":["english_sentences"],"metadata":{"id":"CC7ailwOOtRD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["french_sentences"],"metadata":{"id":"gywHKM7jOtUr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####**Obtenir les vocabulaires**\n","+ Vous avez maintenant deux listes qui contiennent les phrases anglaises et les phrases françaises correspondantes. \n","\n","+ Dans cet exercice, vous allez parcourir chaque caractère de chacune des phrases et découvrir l'ensemble de caractères uniques dans chacune des langues. Vous créerez le vocabulaire à partir des phrases anglaises et des phrases françaises.\n","\n","+ Les phrases anglaises et françaises sont disponibles dans les variables phrases_anglaises et phrases_francaises respectivement.\n","\n","####**Des instructions**\n","+ )1\n","  + Itérez sur chaque phrase anglaise et obtenez-en un ensemble de caractères.\n","Utilisez .union() pour mettre à jour le vocabulaire anglais.\n","\n","\n","+ 2)\n","  + Itérez sur chaque phrase française et obtenez-en un ensemble de caractères.\n","Utilisez .union() pour mettre à jour le vocabulaire français"],"metadata":{"id":"2EcUFyuFP21z"}},{"cell_type":"code","source":["# Create an empty set to contain the English vocabulary \n","english_vocab = set()\n","\n","# Iterate over each English sentence\n","for eng_line in english_sentences:\n","  \n","    # Convert the English line to a set\n","    eng_line_set = set(eng_line)\n","    \n","    # Update English vocabulary with new characters from this line.\n","    english_vocab = english_vocab.union(eng_line_set)\n","\n","# Sort the vocabulary\n","english_vocab = sorted(list(english_vocab))"],"metadata":{"id":"2Dc6SN-lQB87","executionInfo":{"status":"ok","timestamp":1672932677475,"user_tz":-60,"elapsed":328,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":169,"outputs":[]},{"cell_type":"code","source":["english_vocab "],"metadata":{"id":"x8NX8UHuQCCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an empty set to contain the French vocabulary \n","french_vocab = set()\n","\n","# Iterate over each French sentence\n","for fra_line in french_sentences:\n","  \n","    # Convert the French line to a set\n","    fra_line_set = set(fra_line)\n","    \n","    # Update French vocabulary with new characters from this line.\n","    french_vocab = french_vocab.union(fra_line_set)\n","\n","# Sort the vocabulary\n","french_vocab = sorted(list(french_vocab))"],"metadata":{"id":"asAY_byKQZAR","executionInfo":{"status":"ok","timestamp":1672932682068,"user_tz":-60,"elapsed":313,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":170,"outputs":[]},{"cell_type":"code","source":["french_vocab"],"metadata":{"id":"xjRZ5RdTQa7D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####**Mappage de caractères sur des nombres entiers et vice-versa**\n","+ Dans le dernier exercice, vous avez créé le vocabulaire anglais et français. \n","\n","+ Dans cet exercice, vous allez utiliser ces vocabulaires et créer les correspondances caractère-entier et entier-caractère pour l'anglais et le français.\n","\n","+ *Les phrases en anglais et en français sont disponibles respectivement dans les phrases english_sentences et french_sentences. Les vocabulaires anglais et français sont disponibles dans english_vocab et french_vocab respectivement.*\n","\n","####**Des instructions**\n","+ 1)\n","  + Créer un mappage caractère-entier pour le vocabulaire anglais.\n","Créez un mappage entier-caractère pour le vocabulaire anglais.\n","\n","+ 2)\n","  + Créez le mappage caractère-entier pour le français.\n","  + Créez le mappage entier-caractère pour le français."],"metadata":{"id":"J1sU-QHeRMW5"}},{"cell_type":"code","source":["# Dictionary to contain the character to integer mapping for English\n","eng_char_to_idx = dict((char, idx) for idx, char in enumerate(english_vocab))\n","\n","# Dictionary to contain the integer to character mapping for English\n","eng_idx_to_char = dict((idx, char) for idx, char in enumerate(english_vocab))"],"metadata":{"id":"gSPKLUBkRLya","executionInfo":{"status":"ok","timestamp":1672932688213,"user_tz":-60,"elapsed":612,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":171,"outputs":[]},{"cell_type":"code","source":["eng_char_to_idx"],"metadata":{"id":"MWs9Ysd-SXF9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eng_idx_to_char"],"metadata":{"id":"WrUIMF9WSXKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dictionary to contain the character to integer mapping for French\n","fra_char_to_idx = dict((char, idx) for idx, char in enumerate(french_vocab))\n","\n","# Dictionary to contain the integer to character mapping for French\n","fra_idx_to_char = dict((idx, char) for idx, char in enumerate(french_vocab))"],"metadata":{"id":"U8kY7q5bShW9","executionInfo":{"status":"ok","timestamp":1672932691899,"user_tz":-60,"elapsed":243,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":172,"outputs":[]},{"cell_type":"code","source":["fra_char_to_idx "],"metadata":{"id":"WuidMYpFSini"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fra_idx_to_char"],"metadata":{"id":"IvGSySsESnwY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Traduction automatique neuronale**\n","\n","####**1. Traduction automatique neuronale**\n","\n","+ ***Dans la dernière leçon, vous avez découvert diverses applications NLP qui sont mieux modélisées en tant que tâches d’entrée et de sortie de séquence***. \n","\n","+ ***Vous avez également prétraité le jeu de données bilingue Français anglais afin qu’il puisse être utilisé pour former un modèle de séquence à séquence***. \n","\n","+ *Dans cette leçon, vous découvrirez l’architecture codeur-décodeur utilisée pour modéliser des jeux de données de séquence à séquence.*\n","\n","####**2. Encodeur**\n","\n","+ ***L’architecture codeur-décodeur se compose de deux réseaux neuronaux distincts***. \n","\n","+ **L’encodeur accepte les phrases d’entrée et résume les informations dans ses vecteurs d’état**. \n","\n","+ Les encodeurs sont implémentés à l’aide de LSTM et ici les états se réfèrent à la cellule et aux états masqués de la couche LSTM. \n","\n","+ Pendant l’entraînement, l’encodeur apprend ces états à partir des données. \n","\n","+ Intuitivement, vous pouvez considérer les états comme un résumé de toutes les informations utiles de l’entrée. La sortie de l’encodeur est ignorée.\n","\n","####**3. Décodeur**\n","+ ***Le décodeur est également implémenté à l’aide de LSTM et les états initiaux masqués et cellulaires sont initialisés aux états finaux de l’encodeur***. \n","\n","+ Intuitivement, le décodeur apprend à connaître toutes les informations utiles à partir de l’entrée de ces états. \n","\n","+ Le décodeur utilise ces informations pour générer la sortie. \n","\n","+ Les états finaux du décodeur sont ignorés. \n","\n","+ La sortie du décodeur est comparée à la séquence cible pour calculer l’erreur qui est minimisée pendant le processus d’entraînement en mettant à jour les poids des réseaux d’encodeurs et de décodeurs.\n","\n","####**4. Forcer l’enseignant**\n","+ L’entrée du décodeur à chaque pas de temps est la sortie prévue du pas de temps précédent comme d’habitude. \n","\n","+ ***Cependant, pendant l’entraînement, l’entrée du décodeur à chaque pas de temps est la sortie réelle de l’étape précédente au lieu de la sortie prévue.*** \n","\n","+ ***Cette technique est connue sous le nom de forçage de l’enseignant qui aide le modèle à apprendre plus rapidement.***\n","\n","####**5. Encodeur pour la traduction**\n","\n","\n","+ Maintenant que vous savez comment fonctionnent les réseaux d’encodeurs et de décodeurs, appliquons cela à l’étude de cas de la traduction automatique. \n","\n","+ L’encodeur acceptera les phrases anglaises comme indiqué. \n","\n","+ Le nombre de pas de temps dans l’encodeur sera la longueur des phrases anglaises. \n","\n","+ Comme nous avons des phrases de longueurs variables, la longueur de la phrase anglaise la plus longue peut être considérée comme la taille de l’étape. \n","\n","+ Les phrases plus courtes peuvent être rembourrées avec des zéros à la fin. \n","\n","+ Encoder résume toutes les informations nécessaires à partir des phrases anglaises dans ses vecteurs d’état qui sont ensuite transmises au décodeur. Les sorties de l’encodeur sont ignorées.\n","\n","####**6. Décodeur pour la traduction**\n","\n","+ Les états initiaux du décodeur sont les états finaux du codeur. \n","\n","+ L’encodeur consolide toutes les informations utiles des phrases anglaises dans ses vecteurs d’état qui sont nécessaires dans le décodeur pour générer la phrase Français traduite. \n","\n","+ Les entrées du décodeur pendant la formation sont les phrases Français en raison du forçage de l’enseignant. \n","\n","+ Les sorties décodeur sont les phrases traduites. \n","\n","+ Les états du décodeur sont ignorés. \n","\n","+ Semblable à l’encodeur, comme nous avons des phrases de longueurs variables, le nombre de pas de temps dans le décodeur peut être réglé sur la longueur de la phrase la plus longue Français.\n","\n","####**7. Codeur-décodeur pendant l’entraînement**\n","+ Cette image montre l’ensemble du réseau de traduction automatique neuronale.\n","\n","####**8. Forme des vecteurs d’entrée et cibles**\n","+ Avant de construire le réseau, nous devons créer les vecteurs d’entrée et cible. \n","\n","+ Il y a deux entrées dans le réseau - des phrases en anglais pour l’encodeur et des phrases Français pour le décodeur. \n","\n","+ Les cibles sont les Français les peines. \n","\n","+ Tous ces vecteurs sont tridimensionnels \n","  - la première dimension étant le nombre de phrases, \n","  + la seconde étant le nombre de pas de temps qui est la longueur de la phrase anglaise ou Français la plus longue \n","  + et la troisième étant la longueur du vecteur codé à chaud pour les caractères qui est la taille de vocabulaire respective.\n","\n","####**9. Définir les vecteurs d’entrée et de cible**\n","+ Nous devons trouver la longueur de l’anglais le plus long et Français phrases pour définir le nombre de pas de temps. \n","\n","+ Les vecteurs d’entrée et de cible peuvent être définis comme indiqué. \n","\n","+ La première dimension est le nombre de phrases, la deuxième et la troisième étant la taille des pas et la taille du vocabulaire respectif.\n","\n","####**10. Initialiser les vecteurs d’entrée et cibles**\n","+ Nous pouvons initialiser ces vecteurs en itérant sur tous les caractères de chaque phrase et en les convertissant en un vecteur codé à chaud.\n","\n","####**11. API fonctionnelles matérielles**\n","+ Nous allons créer l’encodeur et le décodeur à l’aide des API fonctionnelles Keras. Un calque d’entrée est créé comme illustré et renvoie un vecteur. \n","\n","+ Une occurrence de couche peut également être appelée sur un vecteur. \n","\n","+ Ici, une instance de couche dense est appelée sur la couche d’entrée et elle renvoie un autre vecteur nommé predictions. \n","\n","+ Enfin, cette ligne crée un modèle avec la couche d’entrée et la couche de sortie.\n","\n","####**12. Construire l’encodeur**\n","\n","+ Maintenant, construisons le réseau codeur-décodeur en utilisant Keras. \n","\n","+ Créez une couche d’entrée suivie d’une couche LSTM de 256 unités. \n","\n","+ Chaque vecteur d’entrée serait de taille identique au vocabulaire anglais. \n","\n","+ La première dimension de l’entrée est Aucun, ce qui indique qu’elle peut prendre un nombre variable de séquences d’entrée au moment de l’exécution. \n","\n","+ Introduisez cette entrée dans la couche LSTM pour produire les vecteurs de sortie et d’état. Ignorez la sortie et combinez les états masqués et de cellule.\n","\n","####**13. Construire le décodeur**\n","+ Pour construire le décodeur, créez une couche d’entrée suivie d’une couche LSTM de 256 unités. \n","\n","+ L’état initial de la couche LSTM est l’état final du codeur. \n","\n","+ La sortie de cette couche LSTM sera introduite dans une couche Dense softmax qui nous donnera la sortie finale.\n","\n","####**14. Combinez l’encodeur et le décodeur**\n","+ Ensuite, nous devons combiner l’encodeur et le décodeur en utilisant la fonction Model de Keras comme indiqué. \n","\n","+ ***L’entrée du modèle est la combinaison des entrées du codeur et du décodeur et la sortie est la sortie du décodeur.***\n","\n","####**15. Compiler et former le réseau**\n","+ Nous pouvons maintenant compiler le réseau, l’entraîner pour quelques époques avec une taille de lot de 64 et une division de validation de 0-point-2 comme indiqué.\n","\n","####**16. Entraînons-nous!**\n","+ Super travail! Vous savez maintenant comment construire et entraîner un modèle de traduction neuronale à l’aide d’un encodeur et d’un décodeur. Il est temps pour vous de pratiquer!"],"metadata":{"id":"94Z61mU7S7Ah"}},{"cell_type":"markdown","source":["###**EXERCICE**\n","####**Définir les vecteurs d'entrée et cible**\n","+ Dans la dernière leçon, vous avez jeté les bases de la création d'un système de traduction de l'anglais vers le français. \n","+ Jusqu'à présent, vous avez créé les vocabulaires et le caractère en nombre entier et les mappages inverses. Dans cette leçon, vous allez développer cela et créer et former un modèle d'encodeur-décodeur simple.\n","\n","+ *Dans cet exercice, vous allez créer les tenseurs d'entrée pour les phrases en anglais et en français et le tenseur cible nécessaire à l'entraînement. Les variables english_sentences, french_sentences, english_vocab et french_vocab contiennent les phrases anglaises et françaises et leurs vocabulaires respectifs. Le caractère en entier et les mappages inverses pour l'anglais et le français sont enregistrés dans eng_idx_to_char, eng_idx_to_char, fra_char_to_idx, fra_idx_to_char respectivement.*\n","\n","####**Des instructions**\n","+ Trouvez la longueur de la plus longue phrase en anglais.\n","+ Trouvez la longueur de la plus longue phrase en français.\n","+ Définissez un vecteur zéro 3D pour les phrases anglaises codées.\n","+ Définissez un vecteur zéro 3D pour les phrases françaises codées.\n","+ Définissez un vecteur zéro 3D pour les données cibles qui sont des phrases françaises."],"metadata":{"id":"zGP8zSvgY4ll"}},{"cell_type":"code","source":["# Find the length of the longest English sentence\n","max_len_eng_sent =  max([len(sentence) for sentence in english_sentences])\n","\n","# Find the length of the longest French sentence\n","max_len_fra_sent =  max([len(sentence) for sentence in french_sentences])"],"metadata":{"id":"haJzESRLYAO7","executionInfo":{"status":"ok","timestamp":1672932701422,"user_tz":-60,"elapsed":435,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":173,"outputs":[]},{"cell_type":"code","source":["max_len_eng_sent"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9c62wGAZYASU","executionInfo":{"status":"ok","timestamp":1672929787053,"user_tz":-60,"elapsed":312,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"4d1a59c5-baf8-4cac-b8f3-7658988d8c57"},"execution_count":137,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{},"execution_count":137}]},{"cell_type":"code","source":["max_len_fra_sent"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDpK7Wu9YqNm","executionInfo":{"status":"ok","timestamp":1672929789811,"user_tz":-60,"elapsed":376,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"7bea7e1c-44fe-4370-95eb-1e8ff0e0f240"},"execution_count":138,"outputs":[{"output_type":"execute_result","data":{"text/plain":["27"]},"metadata":{},"execution_count":138}]},{"cell_type":"code","source":["# Create a 3-D zero vector for the input English data\n","eng_input_data = np.zeros((len(english_sentences), max_len_eng_sent, len(english_vocab)), dtype='float32')\n","\n","# Create a 3-D zero vector for the input French data\n","fra_input_data = np.zeros((len(french_sentences), max_len_fra_sent, len(french_vocab)), dtype='float32')\n","\n","# Create the target vector\n","target_data = np.zeros((len(french_sentences), max_len_fra_sent, len(french_vocab)), dtype='float32')"],"metadata":{"id":"rxeoAA6KZOQO","executionInfo":{"status":"ok","timestamp":1672932704157,"user_tz":-60,"elapsed":281,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":174,"outputs":[]},{"cell_type":"code","source":["eng_input_data"],"metadata":{"id":"RCI1MSwKZTo2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fra_input_data"],"metadata":{"id":"JBnHMRadZSYt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_data "],"metadata":{"id":"b76G7E8rZYK2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####**Initialiser les vecteurs d'entrée et cible**\n","+ Dans l'exercice précédent, vous avez défini les Tensors d'entrée et cible avec des formes appropriées. \n","\n","+ Dans cet exercice, vous allez remplir ces Tensors avec des données. Vous allez itérer sur chaque caractère de chaque phrase et convertir le caractère en un vecteur encodé à chaud.\n","\n","+ *Les variables english_sentences, french_sentences, english_vocab et french_vocab contiennent les phrases anglaises et françaises et leurs vocabulaires respectifs. Le caractère en entier et les mappages inverses pour l'anglais et le français sont enregistrés dans eng_idx_to_char, eng_idx_to_char, fra_char_to_idx, fra_idx_to_char respectivement. Les vecteurs d'entrée et cible sont déjà définis comme eng_input_data, fra_input_data, target_data respectivement.*\n","\n","####**Des instructions**\n","+ Convertissez chaque caractère de chaque phrase anglaise en un vecteur encodé à chaud en utilisant eng_char_to_idx.\n","+ Convertissez chaque caractère de chaque phrase française en un vecteur encodé à chaud en utilisant fra_char_to_idx.\n","+ Remplissez le vecteur cible qui est le même que fra_input_data mais un pas de temps en avant."],"metadata":{"id":"BwrPWDWtZl9N"}},{"cell_type":"code","source":["# Iterate over the 50 sentences\n","for i in range(50):\n","    # Iterate over each English character of each sentence\n","    for k, ch in enumerate(english_sentences[i]):\n","        # Convert the character to one-hot encoded vector\n","        eng_input_data[i, k, eng_char_to_idx[ch]] = 1."],"metadata":{"id":"xCL1vgY0aEpQ","executionInfo":{"status":"ok","timestamp":1672932709405,"user_tz":-60,"elapsed":359,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":175,"outputs":[]},{"cell_type":"code","source":["# Iterate over the 50 sentences\n","for i in range(50):\n","    # Iterate over each English character of each sentence\n","    for k, ch in enumerate(english_sentences[i]):\n","        # Convert the character to one-hot encoded vector\n","        eng_input_data[i, k, eng_char_to_idx[ch]] = 1.\n","    \n","    # Iterate over each French character of each sentence\n","    for k, ch in enumerate(french_sentences[i]):\n","        # Convert the character to one-hot encoded vector\n","        fra_input_data[i, k, fra_char_to_idx[ch]] = 1.\n","\n","        # Target data will be one timestep ahead and excludes start character\n","        if k > 0:\n","            target_data[i, k-1, fra_char_to_idx[ch]] = 1."],"metadata":{"id":"ZWRq-hLOaEsj","executionInfo":{"status":"ok","timestamp":1672932712166,"user_tz":-60,"elapsed":304,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":176,"outputs":[]},{"cell_type":"markdown","source":["####**Construire l'encodeur et le décodeur**\n","\n","+ Il est enfin temps de construire les réseaux d'encodeurs et de décodeurs. \n","\n","+ Les couches Input, LSTM et Dense sont déjà importées depuis tensorflow.keras.layers. \n","\n","+ Les vocabulaires sont enregistrés dans english_vocab et french_vocab. \n","\n","+ L'encodeur et le décodeur ont l'architecture suivante -\n","\n","+ Encodeur :\n","\n","  + Une couche d'entrée.\n","  + Une couche LSTM de 256 unités.\n","\n","+ Décodeur\n","\n","  + Une couche d'entrée.\n","  + Une couche LSTM de 256 unités qui prend la couche d'entrée et les états du codeur pour générer la sortie.\n","  + Une couche dense avec activation 'softmax' qui prend la sortie LSTM et produit la distribution de probabilité pour le caractère traduit.\n","\n","####**Des instructions**\n","+ 1)\n","  + Créez un encodeur en utilisant Input() et LSTM() comme indiqué.\n","  + Passez encoder_input dans encoder_LSTM() et enregistrez les états de l'encodeur.\n","\n","+ 2)\n","  + Créez un décodeur en utilisant Input() et LSTM().\n","  + Obtenez la sortie de decoder_LSTM().\n","  + Alimentez la sortie du décodeur vers la couche dense."],"metadata":{"id":"BoPN5kXYarJn"}},{"cell_type":"code","source":["# Create input layer\n","encoder_input = Input(shape=(None, len(english_vocab)))\n","\n","# Create LSTM Layer of size 256\n","encoder_LSTM = LSTM(256, return_state = True)\n","\n","# Save encoder output, hidden and cell state\n","encoder_outputs, encoder_h, encoder_c = encoder_LSTM(encoder_input)\n","\n","# Save encoder states\n","encoder_states = [encoder_h, encoder_c]"],"metadata":{"id":"hDv5GXn2aExt","executionInfo":{"status":"ok","timestamp":1672932717708,"user_tz":-60,"elapsed":315,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":177,"outputs":[]},{"cell_type":"code","source":["encoder_states"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ckBuXDPZcFB9","executionInfo":{"status":"ok","timestamp":1672930680373,"user_tz":-60,"elapsed":420,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"22b0d663-6729-45b7-c0ac-308aab886dc3"},"execution_count":146,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Tensor 'lstm_3/while/Identity_4:0' shape=(None, 256) dtype=float32>,\n"," <tf.Tensor 'lstm_3/while/Identity_5:0' shape=(None, 256) dtype=float32>]"]},"metadata":{},"execution_count":146}]},{"cell_type":"code","source":["# Create decoder input layer\n","decoder_input = Input(shape=(None, len(french_vocab)))\n","\n","# Create LSTM layer of size 256\n","decoder_LSTM = LSTM(256, return_sequences=True, return_state = True)\n","\n","# Save decoder output\n","decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n","\n","# Create a Dense layer with softmax activation\n","decoder_dense = Dense(len(french_vocab), activation='softmax')\n","\n","# Save the decoder output\n","decoder_out = decoder_dense(decoder_out)"],"metadata":{"id":"aS0tOKtib6jM","executionInfo":{"status":"ok","timestamp":1672932723842,"user_tz":-60,"elapsed":312,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":178,"outputs":[]},{"cell_type":"code","source":["decoder_out"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Yjwbi4KcJUM","executionInfo":{"status":"ok","timestamp":1672930698027,"user_tz":-60,"elapsed":449,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"f8379188-3e55-4781-b5ed-00d49ffbf44a"},"execution_count":148,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'dense_9/Softmax:0' shape=(None, None, 52) dtype=float32>"]},"metadata":{},"execution_count":148}]},{"cell_type":"markdown","source":["####**pluie le réseau codeur décodeur**\n","\n","+ **Dans le dernier exercice, vous avez créé les réseaux Encodeur et Décodeur séparément**. \n","\n","+ Dans cet exercice, vous les combinerez à l'aide de l'API fonctionnelle Keras pour **créer l'architecture Encoder-Decoder**. \n","\n","+ Vous compilerez et entraînerez également le réseau.\n","\n","+ *Vous avez les vecteurs d'entrée de l'encodeur et du décodeur enregistrés respectivement dans les variables eng_input_data et fra_input_data. Vous avez également les données cibles disponibles dans la variable target_data. Les couches d'entrée de l'encodeur et du décodeur sont enregistrées dans encoder_input et decoder_input. La couche de sortie du décodeur est enregistrée dans decoder_out.*\n","\n","####**Des instructions**\n","+ Créez le modèle d'encodeur-décodeur à l'aide de Model().\n","+ Compilez le modèle à l'aide de ***l'optimiseur 'rmsprop' et de la perte 'categorical_crossentropy'***.\n","+ Imprimer le résumé du modèle.\n","+ Transmettez \n","  + [eng_input_data,fra_input_data] en $x$ et target_data en $y$ à la fonction model.fit() pour former le modèle."],"metadata":{"id":"eH4KNHbDc2Kc"}},{"cell_type":"code","source":["# Build model\n","model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_out])\n","\n","# Compile the model\n","model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n","\n","# Print model summary\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7PIu6e8MdHh0","executionInfo":{"status":"ok","timestamp":1672932728465,"user_tz":-60,"elapsed":365,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"29d4df3c-7bf9-481a-ddfd-25817de7ca1b"},"execution_count":179,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_5 (InputLayer)           [(None, None, 41)]   0           []                               \n","                                                                                                  \n"," input_6 (InputLayer)           [(None, None, 52)]   0           []                               \n","                                                                                                  \n"," lstm_5 (LSTM)                  [(None, 256),        305152      ['input_5[0][0]']                \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_6 (LSTM)                  [(None, None, 256),  316416      ['input_6[0][0]',                \n","                                 (None, 256),                     'lstm_5[0][1]',                 \n","                                 (None, 256)]                     'lstm_5[0][2]']                 \n","                                                                                                  \n"," dense_10 (Dense)               (None, None, 52)     13364       ['lstm_6[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 634,932\n","Trainable params: 634,932\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Fit the model\n","model.fit(x=[eng_input_data,fra_input_data], y=target_data,\n","          \t\tbatch_size=64, epochs=1, validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rekrkRIBdHlK","executionInfo":{"status":"ok","timestamp":1672932737055,"user_tz":-60,"elapsed":1537,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"f280e595-b2c6-4578-92f7-886a0835655f"},"execution_count":180,"outputs":[{"output_type":"stream","name":"stdout","text":["Train on 40 samples, validate on 10 samples\n","40/40 [==============================] - 1s 23ms/sample - loss: 1.9592 - val_loss: 1.5777\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fde42578d00>"]},"metadata":{},"execution_count":180}]},{"cell_type":"markdown","source":["###**Inférence à l’aide d’un codeur et d’un décodeur**\n","\n","####**1. Inférence à l’aide d’un codeur et d’un décodeur**\n","\n","+ Au cours des derniers exercices, vous avez compilé et entraîné un réseau d’encodeur-décodeur pour la traduction automatique. \n","\n","+ Maintenant, utilisons le modèle entraîné pour faire des prédictions. \n","\n","+ Ici, nos entrées seront des phrases en anglais et les prédictions seront les phrases traduites en Français.\n","\n","####**2. Codeur et décodeur pendant l’inférence**\n","+ N’oubliez pas l’architecture codeur-décodeur de la leçon précédente. L’encodeur prend les phrases anglaises d’entrée et résume les informations dans leurs états internes. \n","\n","+ L’état du décodeur est initialisé aux états finaux de l’encodeur. \n","\n","+ L’encodeur et le décodeur sont implémentés à l’aide de LSTM et les états se réfèrent aux états cachés et cellulaires de la couche LSTM du codeur et du décodeur. \n","\n","+ L’encodeur résume toutes les informations utiles des phrases anglaises dans ses vecteurs d’état internes qui sont nécessaires dans le décodeur pour générer la sortie Français phrase. \n","\n","+ Lors de l’inférence, l’entrée du décodeur à chaque pas de temps est la sortie prédite du dernier pas de temps. \n","\n","+ La sortie du codeur et les états finaux du décodeur sont ignorés. La sortie du décodeur est la sortie prévue.\n","\n","####**3. Modèle d’inférence pour codeur**\n","+ L’encodeur se comporte de la même manière pendant l’entraînement et l’inférence. \n","\n","+ Le modèle d’inférence du codeur se compose de la couche d’entrée et des vecteurs d’état du modèle entraîné.\n","\n","####**4. États initiaux du décodeur**\n","+ Le décodeur requiert les états masqués et cellulaires du modèle d’inférence du codeur comme état initial. \n","\n","+ Ces états seront fournis en entrée au modèle et, par conséquent, doivent être initialisés en tant qu’entrées. Vous pouvez combiner ces états comme indiqué.\n","\n","####**5. Sorties décodeur**\n","+ Le décodeur est appelé récursivement pour chaque caractère à générer dans la séquence traduite. \n","\n","+ Lors du premier appel, les états cachés et cellulaires de l’encodeur seront utilisés pour initialiser les états du décodeur LSTM qui générera une sortie et les états cachés et cellulaires. \n","\n","+ Ces états seront utilisés pour générer le caractère suivant dans l’itération suivante. \n","\n","+ La sortie du décodeur LSTM est à nouveau transmise à la couche dense pour obtenir le caractère prédit.\n","\n","####**6. Modèle d’inférence pour décodeur**\n","\n","+ Nous pouvons combiner tout cela à l’aide de l’API fonctionnelle Keras pour construire le modèle d’inférence du décodeur. \n","\n","+ L’entrée du modèle est la couche d’entrée du décodeur et les états d’entrée du décodeur. \n","\n","+ La sortie du modèle est le caractère prédit ainsi que les états de sortie du décodeur.\n","\n","####**7. Prédiction à l’aide des modèles d’inférence**\n","+ Maintenant que nous avons construit les modèles d’inférence, nous pouvons les utiliser pour traduire une phrase anglaise d’entrée. \n","\n","+ Prenons une phrase d’entrée aléatoire et introduisons-la dans le modèle d’inférence de l’encodeur qui produira les états internes de l’encodeur. \n","\n","+ Définissez une variable pour la phrase de sortie et initialisez-la pour qu’elle contienne le jeton de départ de la séquence traduite qui dans notre cas est le caractère de tabulation.\n","\n","####**8. Générez le premier caractère**\n","+ Transmettez la séquence cible et les valeurs d’état actuel au modèle d’inférence du décodeur pour générer la sortie avec les valeurs d’état. \n","\n","+ Le résultat est une distribution de probabilité pour le caractère suivant sur le vocabulaire Français. \n","\n","+ Découvrez l’indice avec la probabilité maximale. Le caractère suivant est le caractère correspondant à cet index.\n","\n","####**9. Générez le deuxième caractère**\n","\n","+ Ce premier caractère peut être réinjecté dans le modèle d’inférence du décodeur. \n","\n","+ Mettez à jour la séquence cible avec ce nouveau personnage. \n","\n","+ Enregistrez les valeurs d’état du dernier pas de temps. \n","\n","+ Transmettez cette séquence mise à jour et les nouvelles valeurs d’état au modèle d’inférence du décodeur qui produira le caractère suivant le plus probable.\n","\n","####**10. Générer une phrase traduite**\n","\n","+ Nous pouvons continuer à le faire à l’intérieur d’une boucle while jusqu’à ce qu’un caractère de nouvelle ligne soit trouvé qui est le jeton de fin et signale la fin de la séquence de sortie. \n","\n","+ Nous pouvons garder une trace des caractères générés dans le processus et les enregistrer pour former une chaîne qui n’est rien d’autre que la phrase traduite dans Français. \n","\n","+ Il est préférable de déplacer ce bloc de code à l’intérieur d’une fonction qui prend une phrase anglaise comme entrée et génère la phrase Français traduite en sortie. \n","\n","+ La fonction peut être utilisée comme une boîte noire pour traduire une phrase anglaise d’entrée en Français.\n","\n","####**11. Entraînons-nous!**\n","+ Maintenant que vous avez appris à utiliser le modèle codeur-décodeur pour l’inférence, il est temps de le mettre en pratique."],"metadata":{"id":"vdovPApkdpfl"}},{"cell_type":"markdown","source":["###**EXERCICE**\n","\n","####**Construire des modèles d'inférence pour l'encodeur et le décodeur**\n","+ Dans la leçon précédente, vous avez construit un réseau d'encodeurs-décodeurs pour traduire des phrases de l'anglais vers le français. \n","\n","+ Vous l'avez également formé sur l'ensemble de données prétraité. \n","\n","+ Dans cette section, vous utiliserez le modèle formé pour traduire une phrase anglaise donnée en français.\n","\n","+ *Comme première étape du processus de traduction, le modèle d'inférence pour l'encodeur et le décodeur sera créé à l'aide des tenseurs d'entrée et des tenseurs d'état internes. Les variables encoder_input, encoder_states, decoder_input sont utilisables et contiennent le tenseur d'entrée du codeur, les états internes du codeur et le vecteur d'entrée du décodeur. Le décodeur LSTM et les couches denses du décodeur que vous avez créées lors de la formation sont disponibles dans decoder_LSTM() et decoder_dense().*\n","\n","####**Des instructions**\n","+ Utilisez la fonction Model() de keras et transmettez encoder_input et encoder_states.\n","+ Utilisez Input() pour définir les états initiaux du décodeur de dimension 256.\n","+ Combinez les états en un seul vecteur.\n","+ Passez decoder_input dans la couche decoder_LSTM() qui a été créée pendant la formation pour obtenir la sortie.\n","+ Utilisez la couche decoder_dense() qui a été créée lors de la formation pour obtenir la sortie finale."],"metadata":{"id":"Hda79LUGfCKq"}},{"cell_type":"code","source":["# Create encoder inference model\n","encoder_model_inf = Model(encoder_input, encoder_states)\n","\n","# Create decoder input states for inference\n","decoder_state_input_h = Input(shape=(256,))\n","decoder_state_input_c = Input(shape=(256,))\n","decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n","\n","# Create decoder output states for inference\n","decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, initial_state=decoder_input_states)\n","decoder_states = [decoder_h , decoder_c]\n","\n","# Create decoder dense layer\n","decoder_out = decoder_dense(decoder_out)\n","decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states, outputs=[decoder_out] + decoder_states )"],"metadata":{"id":"4YOpoWRpfmnT","executionInfo":{"status":"ok","timestamp":1672932818459,"user_tz":-60,"elapsed":1232,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":184,"outputs":[]},{"cell_type":"markdown","source":["####**Prédire le premier caractère**\n","+ Dans cet exercice, le modèle d'inférence pour l'encodeur et le décodeur sera utilisé pour la prédiction.\n","\n","+ La phrase française traduite sera générée caractère par caractère. \n","\n","+ Les phrases en anglais seront transmises au modèle d'inférence de l'encodeur. \n","\n","+ L'encodeur formé résumera les informations dans ses vecteurs d'état. \n","\n","+ Ces vecteurs d'état sont enregistrés et transmis au décodeur. Le jeton de début (\\t) sera également fourni au décodeur en tant que caractère de départ. \n","\n","+ Le décodeur générera la distribution de probabilité pour le caractère suivant sur le vocabulaire français. \n","\n","+ Il calculera également les vecteurs d'état dont vous aurez besoin pour générer le deuxième caractère.\n","\n","+ *Maintenant, étudions comment le modèle d'inférence Decoder génère un nouveau caractère étant donné le premier caractère de départ.*\n","\n","####**Des instructions**\n","\n","+ Passez la séquence d'entrée à encoder_model_inf et obtenez les états internes de l'encodeur.\n","+ Obtenez la sortie de decoder_model_inf en transmettant target_seq et states_val.\n","+ Obtenez le caractère suivant le plus probable en utilisant fra_idx_to_char et imprimez-le."],"metadata":{"id":"Cv7Sr_9tgCtN"}},{"cell_type":"code","source":["# Get encoder internal state by passing a sentence as input\n","inp_seq = eng_input_data[0:1]\n","states_val = encoder_model_inf.predict(inp_seq)\n","\n","# Seed the first character and get output from the decoder \n","target_seq = np.zeros((1, 1, len(french_vocab)))\n","target_seq[0, 0, fra_char_to_idx['\\t']] = 1  \n","decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n","\n","# Find out the next character from the Decoder output\n","max_val_index = np.argmax(decoder_out[0,-1,:])\n","sampled_fra_char = fra_idx_to_char[max_val_index]"],"metadata":{"id":"DjW6OSFIgOkt","executionInfo":{"status":"ok","timestamp":1672932901711,"user_tz":-60,"elapsed":920,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":193,"outputs":[]},{"cell_type":"code","source":["# Print the first character predicted by the decoder\n","sampled_fra_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"7YTwF66hgOoZ","executionInfo":{"status":"ok","timestamp":1672932904785,"user_tz":-60,"elapsed":310,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"40a343b3-6b00-43f2-a571-08a5fb01ac3c"},"execution_count":194,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":194}]},{"cell_type":"markdown","source":["####**Prédire le deuxième personnage**\n","+ Dans l'exercice précédent, vous avez utilisé les modèles d'inférence d'encodeur et de décodeur pour générer le premier caractère à partir du jeton de début. \n","\n","+ Vous avez fourni la phrase anglaise au modèle d'inférence de l'encodeur et obtenu les états. \n","\n","+ Le modèle d'inférence du décodeur utilise les états du codeur avec le caractère de départ pour générer le premier caractère de la séquence traduite.\n","\n","+ Dans cet exercice, vous utiliserez le caractère de départ et le premier caractère généré pour générer le deuxième caractère dans la phrase traduite. \n","\n","+ Les états d'entrée du décodeur dans ce pas de temps seront réglés sur les états de sortie du décodeur lors de la génération du premier caractère.\n","\n","+ *Ce processus peut être répété pour générer la phrase traduite caractère par caractère jusqu'à ce que le jeton de fin (\\n) soit rencontré.*\n","\n","####**Des instructions**\n","\n","+ Obtenez la sortie du décodeur pour le deuxième caractère.\n","+ Mappez la sortie du décodeur au caractère français.\n","+ Imprimer le caractère généré."],"metadata":{"id":"uwWUUxlNgzCS"}},{"cell_type":"code","source":["# Fill up target seq with the new char generated \n","target_seq = np.zeros((1, 1, len(french_vocab)))\n","target_seq[0, 0, max_val_index] = 1\n","\n","# Get decoder final states from last time\n","states_val = [decoder_h, decoder_c]\n","\n","# Generate the next character\n","decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n","\n","# Map the prediction to char and print it\n","max_val_index = np.argmax(decoder_out[0,-1,:])\n","sampled_fra_char = fra_idx_to_char[max_val_index]\n","\n","print(sampled_fra_char)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"296NdJmxg9vD","executionInfo":{"status":"ok","timestamp":1672932909930,"user_tz":-60,"elapsed":224,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"e7e71226-5a05-41e1-8b72-796ed11d2760"},"execution_count":195,"outputs":[{"output_type":"stream","name":"stdout","text":["i\n"]}]},{"cell_type":"markdown","source":["####**Générer une phrase entièrement traduite**\n","+ Dans cet exercice, **une fonction translate_eng_sentence()** est disponible qui génère des caractères dans une boucle comme cela a été fait dans les deux derniers exercices. \n","\n","+ Il prend une phrase anglaise et génère une phrase française en sortie.\n","\n","+ Plus tôt dans ce chapitre, vous avez entraîné le modèle pour une seule époque sur un petit sous-ensemble de l'ensemble de données complet pour gagner du temps. \n","\n","+ Mais pour cet exercice, le même modèle est formé sur l'ensemble de données complet pour 20 époques. \n","\n","+ Ce modèle entraîné est déjà chargé dans l'environnement. \n","\n","+ Maintenant, vous allez étudier comment un modèle aussi simple avec une bonne quantité de formation peut être utilisé pour générer des traductions assez bonnes. \n","\n","+ La précision de la traduction sera plus élevée pour les modèles plus complexes et plus entraînés.\n","\n","+ *Toutes les variables précédentes sont disponibles dans la session.*\n","\n","####**Des instructions**\n","+ Itérez 10 fois pour traduire 10 phrases en anglais en boucle :\n","+ Traduisez la phrase en anglais en utilisant translate_eng_sentence().\n","\n","####**Question**\n","+ N'hésitez pas à expérimenter avec **la fonction translate_eng_sentence()** et à générer autant de traductions que vous le souhaitez. \n","\n","+ Comme vous pourrez le constater, bien que les phrases prédites soient en français, elles ne sont pas tout à fait les traductions correctes pour les phrases d'entrée. \n","\n","+ Réfléchissez aux raisons pour lesquelles la précision de la traduction est faible. \n","\n","+ Ici, notre modèle est très simple. \n","\n","+ Vous pouvez augmenter la complexité du modèle pour améliorer les performances.\n","\n","+ Comment pouvez-vous augmenter la complexité du modèle afin d'améliorer les performances du modèle ?\n","\n","####**Des réponses possibles**\n","\n","+ Augmentez le nombre de couches masquées dans l'encodeur.\n","\n","+ Augmentez le nombre de calques masqués dans le décodeur.\n","\n","+ Augmentez le nombre de nœuds dans chaque couche.\n","\n","+ ***Tout ce qui précède.***"],"metadata":{"id":"bmm0y_vGhg6t"}},{"cell_type":"code","source":["def translate_eng_sentence(inp_seq):\n","    # Get encoder states \n","    states_val = encoder_model_inf.predict(inp_seq)\n","    \n","    # Create a vector for the output sentence\n","    target_seq = np.zeros((1, 1, len(french_vocab)))\n","    \n","    # Initialize the first char of the output to tab\n","    target_seq[0, 0, fra_char_to_idx['\\t']] = 1\n","    \n","    # Keep track of the translated sequence\n","    translated_sent = ''\n","    \n","    # Stop condition will be true when we encounter a newline or maximum lenght of sentence is reached\n","    stop_condition = False\n","    \n","    while not stop_condition:\n","        \n","        # Get decoder output\n","        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n","        \n","        # Get index of most probable next character\n","        max_val_index = np.argmax(decoder_out[0,-1,:])\n","        \n","        # Map index to the actual character\n","        sampled_fra_char = fra_idx_to_char[max_val_index]\n","        \n","        # Add generated character to the translated sentence so far\n","        translated_sent += sampled_fra_char\n","        \n","        # If newline is encountered or maximum lenght of sentence is reached, stop\n","        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n","            stop_condition = True\n","        \n","        # Save current generated character for next iteration\n","        target_seq = np.zeros((1, 1, len(french_vocab)))\n","        target_seq[0, 0, max_val_index] = 1\n","        \n","        # Save states for next iteration\n","        states_val = [decoder_h, decoder_c]\n","    \n","    # Return translated sentence\n","    return translated_sent\n"],"metadata":{"id":"I0bhYlIZg9yn","executionInfo":{"status":"ok","timestamp":1672932916149,"user_tz":-60,"elapsed":608,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":196,"outputs":[]},{"cell_type":"code","source":["# Generate 10 French sentences from inp_seq\n","for seq_index in range(10):\n","  \n","    # Get next encoded english sentence\n","    inp_seq = eng_input_data[seq_index:seq_index+1]\n","    \n","    # Get the translated sentence\n","    translated_sent = translate_eng_sentence(inp_seq)\n","    \n","    # Print the original English sentence\n","    print('English sentence:', english_sentences[seq_index])\n","    \n","    # Print the translated French sentence\n","    print('French sentence:', translated_sent)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJB6uu2jg91c","executionInfo":{"status":"ok","timestamp":1672933082958,"user_tz":-60,"elapsed":617,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"86626a93-2a45-45ab-8ddb-a65eec0d1b16"},"execution_count":199,"outputs":[{"output_type":"stream","name":"stdout","text":["English sentence: Go.\n","French sentence:  ii i\n","\n","English sentence: Who?\n","French sentence:  ii i\n","\n","English sentence: Stop!\n","French sentence:  i ii\n","\n","English sentence: Wait!\n","French sentence: uan ii\n","\n","English sentence: Hello!\n","French sentence:  i ii\n","\n","English sentence: I try.\n","French sentence:  i ii\n","\n","English sentence: I won!\n","French sentence:  i ii\n","\n","English sentence: I won.\n","French sentence:  oii i\n","\n","English sentence: Cheers!\n","French sentence:  ii i\n","\n","English sentence: Go now.\n","French sentence:  oii i\n","\n"]}]},{"cell_type":"markdown","source":["###**Convertir les données d’e-mail en seq2seq**\n","\n","####**1. Convertir les données d’e-mail en seq2seq**\n","+ Bienvenue. Dans le chapitre précédent, vous avez appris ***la génération de séquence à séquence et comment les architectures codeur-décodeur peuvent être entraînées et utilisées pour prédire une séquence en fonction d’une autre séquence en entrée***. \n","\n","+ Vous avez également travaillé sur une étude de cas pour traduire des phrases anglaises en Français en utilisant le réseau encodeur-décodeur. \n","\n","+ Tout au long de ce chapitre, vous utiliserez le réseau d’encodeur-décodeur pour l’auto-complétion de phrase qui peut être trouvé dans les applications de messagerie populaires comme Gmail et les applications de messagerie instantanée comme Whatsapp.\n","\n","####**2. Exécution automatique de la peine**\n","\n","+ Étant donné une phrase incomplète en entrée, l’auto-complétion de phrase est la tâche de générer une nouvelle séquence qui peut servir de fin possible de la phrase d’entrée. \n","\n","+ Ainsi, l’entrée est essentiellement un préfixe de la phrase entière et la sortie est le suffixe correspondant, comme indiqué dans ces exemples.\n","\n","####**3. Jeu de données sur les e-mails**\n","+ Tout au long de ce chapitre, nous travaillerons sur des e-mails réels à partir de l’ensemble de données d’e-mails Enron pour créer un modèle de remplissage automatique des e-mails. \n","\n","+ Le jeu de données d’origine contient des messages électroniques entiers, y compris les en-têtes, l’expéditeur, les destinataires et le corps. \n","\n","+ Pour les besoins de cette étude de cas, nous allons prendre les phrases du corps de l’e-mail. \n","\n","+ Quelques exemples sont présentés ici. Nous allons maintenant pré-traiter ces phrases afin qu’elles puissent être utilisées pour former un modèle d’encodeur-décodeur.\n","\n","####**4. Préfixes et suffixes**\n","+ Dans un premier temps, nous devons trouver les séquences d’entrée et cibles à partir des phrases originales. \n","\n","+ Nous pouvons diviser chaque phrase en deux à chaque position de caractère. \n","\n","+ Cela générera un préfixe et le suffixe correspondant pour chaque caractère de la phrase comme indiqué.\n","\n","####**5. Générer des phrases de préfixe et de suffixe**\n","+ Nous pouvons d’abord créer deux listes vides pour stocker les préfixes et les suffixes. \n","\n","+ Ensuite, nous pouvons itérer sur chaque position de caractère dans chaque message électronique pour diviser la phrase en un préfixe et un suffixe. \n","\n","+ Nous devons également ajouter le jeton de début et de fin aux suffixes. \n","\n","+ Enfin, nous pouvons les ajouter aux listes existantes de préfixes et de suffixes. \n","\n","+ Veuillez noter qu’en utilisant cette astuce simple, nous avons réduit la tâche d’auto-complétion de phrase à une tâche de génération de séquence à séquence.\n","\n","####**6. Vocabulaire et cartographies**\n","+ Chaque fois que nous avons des données textuelles, nous devons obtenir le vocabulaire et le caractère en entier et l’entier en mappages de caractères qui peuvent être trouvés comme indiqué.\n","\n","####**7. Forme des vecteurs d’entrée et cibles**\n","+ Avant de construire le réseau, nous devons créer les vecteurs d’entrée et de cible à partir des séquences de préfixes et de suffixes. \n","\n","+ S’il vous plaît rappelez-vous de notre dernier chapitre que ces vecteurs sont des vecteurs tridimensionnels où la première dimension est le nombre de phrases, la deuxième dimension est le nombre d’étapes et la troisième dimension est la taille du vocabulaire.\n","\n","####**8. Définir les vecteurs d’entrée et de cible**\n","+ Nous devons trouver la longueur des séquences de préfixes et de suffixes les plus longues pour définir le nombre de pas de temps nécessaires dans la couche LSTM. \n","\n","+ Les vecteurs d’entrée et de cible de forme appropriée peuvent être définis à l’aide de Numpy comme indiqué. \n","\n","+ La première dimension est le nombre de phrases, la deuxième et la troisième étant la taille des pas et la taille du vocabulaire respectif.\n","\n","####**9. Initialiser les vecteurs d’entrée et de cible**\n","+ Nous pouvons initialiser les vecteurs d’entrée pour les préfixes et les suffixes en itérant sur tous les caractères de chaque séquence et en les convertissant en un vecteur codé à chaud. \n","\n","+ Le vecteur cible sera identique au vecteur d’entrée pour les suffixes, mais il aura une longueur d’avance et exclura le caractère de début. \n","\n","+ Ces vecteurs peuvent être utilisés pour entraîner le réseau codeur-décodeur.\n","\n","####**10. Entraînons-nous!**\n","+ Maintenant que vous savez comment transformer le jeu de données Email en phrases de préfixe et de suffixe afin qu’il puisse être utilisé pour former un réseau d’encodeur-décodeur pour la tâche de saisie semi-automatique des phrases. Il est temps de mettre cela en pratique."],"metadata":{"id":"4yKj5vEBzWoD"}},{"cell_type":"code","source":["corpus = [' are we going to inspect tomorrow?',\n"," '\" socal position     this is short, but is it good enough? \"',\n"," ' re: window unit check with gary about what kind he wants to install',\n"," ' go ahead and level the floor in 2.  ',\n"," '\" steve,  please remove bob shiring and liz rivera from rc 768.    thank you  phillip allen\"',\n"," ' the file is updated and renamed as gas basis mar 00.  ',\n"," ' here is the file i showed you.  ',\n"," ' please give me a call  5038052117.  i need to discuss something with you.',\n"," ' here is our forecast ',\n"," '\" jeff,  i am in the office today.  any isssues to deal with for the stagecoach?  phillip\"',\n"," ' i will email you with the insurance info tomorrow.  ',\n"," '\" lucy,  the spreadsheet looks fine to me.   phillip\"',\n"," ' https:www4.rsweb.com61045',\n"," '\" please approve mike grigsby for bloomberg.    thank you, phillip allen\"',\n"," ' i just refaxed. please confirm receipt',\n"," ' financial  (6)   west desk  (14) mid market (16) ',\n"," ' please find attached the pro formas for the project in san marcos.  thanks again.  ',\n"," '\" lucy,  here is a rentroll for this week.  i still have questions on 28,29, and 32. \"',\n"," ' fax number 7136462391',\n"," '   inv alliance mf survey of smarcos.pdf',\n"," 'loan servicingjessica weeber 8003935626 jweeberspbank.com',\n"," 'exit mccollough off 410',\n"," ' life insurance banner  ',\n"," ' my steno is 5318.',\n"," 'xfilename: pallen (nonprivileged).pst  team here are my comments:  ',\n"," 'xfilename: pallen (nonprivileged).pst  team here are my comments on the subject topic.       ',\n"," 'xfilename: pallen (nonprivileged).pst  team   ',\n"," '      dailytotal.xls ',\n"," '\" phillip,  please see attached.     randy\"',\n"," ' brenda   can you send me your address in college station.  phillip',\n"," '\" ina,   i scheduled a meeting with jean mrha tomorrow at 3:30\"',\n"," ' http:www.hearme.comvc2?chnlownrpallenenron.com',\n"," '\" jeff,   what is up with burnet?  phillip\"',\n"," '\" greg,  happy bday. email me your phone  and i will call you.  keith\"',\n"," ' here is the file i showed you.  ',\n"," '   inv alliance mf survey of smarcos.pdf',\n"," ' financial  (6)   west desk  (14) mid market (16) ',\n"," ' please give me a call  5038052117.  i need to discuss something with you.',\n"," ' the file is updated and renamed as gas basis mar 00.  ',\n"," '\" steve,  please remove bob shiring and liz rivera from rc 768.    thank you  phillip allen\"',\n"," '\" socal position     this is short, but is it good enough? \"',\n"," '\" lucy,  here is a rentroll for this week.  i still have questions on 28,29, and 32. \"',\n"," ' please find attached the pro formas for the project in san marcos.  thanks again.  ',\n"," '\" please approve mike grigsby for bloomberg.    thank you, phillip allen\"',\n"," ' https:www4.rsweb.com61045',\n"," '\" lucy,  the spreadsheet looks fine to me.   phillip\"',\n"," '\" jeff,  i am in the office today.  any isssues to deal with for the stagecoach?  phillip\"',\n"," ' here is our forecast ',\n"," ' check out np gen  load.  (amw)',\n"," ' http:ectpdxsunone.ect.enron.comtheizenwsccnav',\n"," ' testing',\n"," ' here is the 1st draft of a wish list for systems.  ',\n"," 'xfilename: pallen.nsf  attached is the systems wish list for the gas basis and physical trading ',\n"," '\" hunter,  are you watching alberto?  do you have yahoo messenger or hear me turned on?  phillip\"',\n"," '\" phillip,  pursuant to your request, please see the attached.  thanks,  renee    \"',\n"," ' please give me a call  5038052117.  i need to discuss something with you.',\n"," '   inv alliance mf survey of smarcos.pdf',\n"," '\" jeanie,  please change janie tholt\\'s ranking to a 3.\"',\n"," '\" jerilyn,  call or email with additional data requests.  phillip 7138537041\"',\n"," '\" ina,  i will be on vacation july 31  august 3.  please mark the calendar.  phillip\"',\n"," '\" bill,  have not received the forms yet.  i will confirm receipt.  phillip\"',\n"," '\" jeanie,  here is a letter about why frank should be promoted.    \"',\n"," ' greg here are my questions regarding the cedar park deal.   ',\n"," '\" ina,  please mark me down for vacation on august 1 through august 3.  phillip\"',\n"," '\" ed,  how does ellie like the ranch?  is she adjusting ok?  phillip allen\"',\n"," ' http:www.nwdwc.usace.army.milreportprojdata.htm',\n"," ' http:www.forbes.comglobal200006120312072a.html',\n"," ' aplusmath.com    creates flashcards and worksheets for  ',\n"," ' have you heard anything from bank one today?',\n"," '\" mike,  how did the school board meeting go last night?    phillip\"',\n"," '\" jacques,  did you get a chance to look at the release?  drop me a note when you can.  phillip\"',\n"," '\" mike,  can you send me a status update when you get a chance.  thank you,  phillip\"',\n"," '\" john,  do you still want to get together this week?    phillip\"',\n"," '\" greg,  what was the official word on new braunfels?  phillip\"',\n"," '\"    mery,  please email me back with a location for tomorrow\\'s meeting.  phillip\"',\n"," ' here is our forecast   ',\n"," '\" jeff,  i am in the office today.  any isssues to deal with for the stagecoach?  phillip\"',\n"," ' http:www.caiso.comsystemstatus.html',\n"," ' financial  (6)   west desk  (14) mid market (16) ',\n"," '\" ina,   i scheduled a meeting with jean mrha tomorrow at 3:30\"',\n"," ' brenda   can you send me your address in college station.  phillip',\n"," '\" hunter,  are you watching alberto?  do you have yahoo messenger or hear me turned on?  phillip\"',\n"," ' here is the 1st draft of a wish list for systems.  ',\n"," ' testing',\n"," ' http:ectpdxsunone.ect.enron.comtheizenwsccnav',\n"," ' check out np gen  load.  (amw)',\n"," ' are we going to inspect tomorrow?',\n"," '\" socal position     this is short, but is it good enough? \"',\n"," ' re: window unit check with gary about what kind he wants to install',\n"," ' go ahead and level the floor in 2.  ',\n"," '\" steve,  please remove bob shiring and liz rivera from rc 768.    thank you  phillip allen\"',\n"," ' the file is updated and renamed as gas basis mar 00.  ',\n"," ' here is the file i showed you.  ',\n"," ' here is our forecast ',\n"," '\" jeff,  i am in the office today.  any isssues to deal with for the stagecoach?  phillip\"',\n"," ' i will email you with the insurance info tomorrow.  ',\n"," '\" lucy,  the spreadsheet looks fine to me.   phillip\"',\n"," ' https:www4.rsweb.com61045',\n"," '\" please approve mike grigsby for bloomberg.    thank you, phillip allen\"',\n"," ' i just refaxed. please confirm receipt']"],"metadata":{"id":"mGL9wIQH0NYd","executionInfo":{"status":"ok","timestamp":1672937097411,"user_tz":-60,"elapsed":4,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["###**EXERCICE**\n","\n","####**Diviser les phrases en préfixes et suffixes**\n","\n","+ De nombreux outils de messagerie et systèmes d'exploitation ont un système d'auto-complétion intégré qui essaie de deviner la prochaine phrase d'un utilisateur en fonction des caractères qu'il tape. \n","\n","+ Tout au long de ce chapitre, vous travaillerez sur des e-mails réels à partir de l'ensemble de données d'e-mails d'Enron et implémenterez une telle fonctionnalité à partir de zéro.\n","\n","+ Dans cet exercice, vous commencerez par l'ensemble de données brutes et générerez les échantillons d'entrée et cible. \n","\n","+ À partir de chaque phrase complète, vous allez créer un préfixe et un suffixe qui seront utilisés pour entraîner un modèle de sorte qu'il génère le suffixe à partir du préfixe en entrée. \n","\n","+ Vous allez diviser chaque phrase en deux à chaque caractère pour créer un préfixe et un suffixe. Vous devez ajouter le jeton de début et de fin aux suffixes.\n","\n","+ *Les emails sont extraits et sont disponibles dans une variable nommée corpus*.\n","\n","####**Des instructions**\n","\n","+ Créez les phrases de préfixe en découpant chaque chaîne d'e-mail à chaque position de caractère donnée par index.\n","+ Créez les phrases de suffixe en prenant le reste de l'e-mail après avoir coupé le préfixe.\n","+ Ajoutez le préfixe et le suffixe dans prefix_sentences et suffix_sentences respectivement."],"metadata":{"id":"vx7kb9qC04Xy"}},{"cell_type":"code","source":["# Empty lists to store the prefixes and the suffixes\n","prefix_sentences = []\n","suffix_sentences = []\n","\n","# Create one prefix and one suffix at each character of each email\n","for email in corpus:\n","    for index in range(len(email)):\n","        # Find the prefix and suffix\n","        prefix = email[: index+1]\n","        suffix = '\\t' + email[index+1 :] + '\\n'\n","        \n","        # Add the prefix and suffix to the list of prefix and suffix sentences\n","        prefix_sentences.append(prefix)\n","        suffix_sentences.append(suffix)"],"metadata":{"id":"ZgLO_A6t0kx4","executionInfo":{"status":"ok","timestamp":1672937285410,"user_tz":-60,"elapsed":308,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["prefix_sentences"],"metadata":{"id":"AXn_Xrgo0NdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["suffix_sentences"],"metadata":{"id":"ZvsZ_qrs1ZO4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####**Créer le vocabulaire et les mappages**\n","+ Dans l'exercice précédent, vous avez créé les phrases de préfixe et de suffixe qui peuvent servir de phrases d'entrée et de cible pour la formation. \n","\n","+ Dans cet exercice, vous découvrirez le vocabulaire du corpus. \n","\n","+ Vous allez également créer le mappage caractère-entier et entier-caractère. \n","\n","+ Le vocabulaire et ces mappages seront très utiles lors de la création des tenseurs d'entrée et cible pour la formation.\n","\n","+ *Les emails sont disponibles dans la variable nommée corpus.*\n","\n","####**Des instructions**\n","+ Itérer d'abord sur chaque e-mail, puis sur chaque caractère de l'e-mail.\n","+ Ajoutez le caractère dans le vocabulaire s'il n'y est pas déjà.\n","+ Triez le vocabulaire.\n","+ Créez le mappage caractère-entier.\n","+ Créez le mappage entier-caractère."],"metadata":{"id":"awWn_7v62Jof"}},{"cell_type":"code","source":["# Initialize vocabulary with the start and end token\n","vocabulary = set(['\\t', '\\n'])\n","\n","# Iterate for each char in each email\n","for email in corpus:\n","    for char in email:\n","        # Add the char if not in vocabulary, \n","        if (char not in vocabulary):\n","            vocabulary.add(char)            \n","# Sort the vocabulary\n","vocabulary = sorted(vocabulary)"],"metadata":{"id":"VOlNTPMB2Uf_","executionInfo":{"status":"ok","timestamp":1672937659272,"user_tz":-60,"elapsed":609,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(vocabulary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"suclw8OB2ux3","executionInfo":{"status":"ok","timestamp":1672937672176,"user_tz":-60,"elapsed":7,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"c63ec344-d421-4eea-aa19-6214577bb7e3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['\\t', '\\n', ' ', '\"', \"'\", '(', ')', ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"]}]},{"cell_type":"code","source":["# Create char to int and int to char mapping\n","char_to_idx = dict((char, idx) for idx, char in enumerate(vocabulary))\n","idx_to_char = dict((idx, char) for idx, char in enumerate(vocabulary))"],"metadata":{"id":"NKLGREm02Uj0","executionInfo":{"status":"ok","timestamp":1672937696750,"user_tz":-60,"elapsed":4,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(idx_to_char)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4P3df5XD2rne","executionInfo":{"status":"ok","timestamp":1672937699210,"user_tz":-60,"elapsed":5,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"1c1f5b69-7f6a-4649-b5d7-66db6044a95f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: '\\t', 1: '\\n', 2: ' ', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: ',', 8: '.', 9: '0', 10: '1', 11: '2', 12: '3', 13: '4', 14: '5', 15: '6', 16: '7', 17: '8', 18: '9', 19: ':', 20: '?', 21: 'a', 22: 'b', 23: 'c', 24: 'd', 25: 'e', 26: 'f', 27: 'g', 28: 'h', 29: 'i', 30: 'j', 31: 'k', 32: 'l', 33: 'm', 34: 'n', 35: 'o', 36: 'p', 37: 'q', 38: 'r', 39: 's', 40: 't', 41: 'u', 42: 'v', 43: 'w', 44: 'x', 45: 'y', 46: 'z'}\n"]}]},{"cell_type":"code","source":["print(char_to_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VqBbtVGC2qbe","executionInfo":{"status":"ok","timestamp":1672937701879,"user_tz":-60,"elapsed":212,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"9e14a60f-2b4a-481b-e573-090c6bf2a996"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{'\\t': 0, '\\n': 1, ' ': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '.': 8, '0': 9, '1': 10, '2': 11, '3': 12, '4': 13, '5': 14, '6': 15, '7': 16, '8': 17, '9': 18, ':': 19, '?': 20, 'a': 21, 'b': 22, 'c': 23, 'd': 24, 'e': 25, 'f': 26, 'g': 27, 'h': 28, 'i': 29, 'j': 30, 'k': 31, 'l': 32, 'm': 33, 'n': 34, 'o': 35, 'p': 36, 'q': 37, 'r': 38, 's': 39, 't': 40, 'u': 41, 'v': 42, 'w': 43, 'x': 44, 'y': 45, 'z': 46}\n"]}]},{"cell_type":"markdown","source":["####**Définir les vecteurs d'entrée et cible**\n","\n","+ Dans cet exercice, vous utiliserez le vocabulaire et le mappage caractère-entier pour créer les tenseurs d'entrée et cible. \n","\n","+ Tout d'abord, vous découvrirez la longueur du préfixe et du suffixe les plus longs qui vous donneront le nombre de pas de temps pour les données d'entrée et cibles. \n","\n","+ Ensuite, vous allez créer des tenseurs tridimensionnels pour l'entrée et le vecteur cible.\n","\n","+ *Les phrases de préfixe et de suffixe sont disponibles respectivement dans prefix_sentences et suffix_sentences. Le vocabulaire, les correspondances caractère-entier et entier-caractère sont disponibles respectivement dans le vocabulaire, char_to_idx et idx_to_char*.\n","\n","####**Des instructions**\n","+ 1)\n","  + Trouvez la longueur du préfixe et du suffixe les plus longs.\n","\n","+ 2)\n","  + Définissez un vecteur zéro 3D pour le préfixe, le suffixe et les phrases cibles codés."],"metadata":{"id":"aV9IQjor3dUp"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"veDEe0r56m1l","executionInfo":{"status":"ok","timestamp":1672938688905,"user_tz":-60,"elapsed":222,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Find the length of the longest prefix\n","max_len_prefix_sent = max([len(prefix) for prefix in prefix_sentences])\n","\n","# Find the length of the longest suffix\n","max_len_suffix_sent = max([len(suffix) for suffix in suffix_sentences])"],"metadata":{"id":"pgnuNOas6but","executionInfo":{"status":"ok","timestamp":1672938691456,"user_tz":-60,"elapsed":3,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["max_len_prefix_sent"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5c9F8AW6ePt","executionInfo":{"status":"ok","timestamp":1672938694218,"user_tz":-60,"elapsed":613,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"1efeb42f-3651-4cfa-d986-3781451cf09e"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["97"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["max_len_suffix_sent"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xBsPsMpQ6f39","executionInfo":{"status":"ok","timestamp":1672938695459,"user_tz":-60,"elapsed":3,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"f1de427f-90ac-4ce4-f1ec-8908315ae660"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["98"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# Define a 3-D zero vector for the prefix sentences\n","input_data_prefix = np.zeros((len(prefix_sentences), max_len_prefix_sent, \n","                              len(vocabulary)), dtype='float32')\n","\n","# Define a 3-D zero vector for the suffix sentences\n","input_data_suffix = np.zeros((len(suffix_sentences), max_len_suffix_sent, \n","                              len(vocabulary)), dtype='float32')\n","\n","# Define a 3-D zero vector for the target data\n","target_data = np.zeros((len(suffix_sentences), max_len_suffix_sent, \n","                        len(vocabulary)), dtype='float32')"],"metadata":{"id":"UJBUh_iS32U3","executionInfo":{"status":"ok","timestamp":1672938697845,"user_tz":-60,"elapsed":250,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["####**Initialiser les vecteurs d'entrée et cible**\n","+ Dans l'exercice précédent, vous avez défini les vecteurs d'entrée et cible avec la forme appropriée. \n","\n","+ Dans cet exercice, vous allez remplir ces vecteurs avec des données. \n","\n","+ Vous allez itérer sur chaque caractère de chaque phrase et convertir le caractère en un vecteur encodé à chaud.\n","\n","+ *Les phrases préfixe et suffixe, le vocabulaire, les correspondances caractère-entier et entier-caractère sont disponibles dans prefix_sentences, suffix_sentences, vocabulaire, char_to_idx et idx_to_char respectivement. Les vecteurs d'entrée pour les préfixes et suffixes et le vecteur cible sont disponibles respectivement dans input_data_prefix, input_data_suffix et target_data.*\n","\n","####**Des instructions**\n","+ Convertissez chaque caractère de chaque préfixe en un vecteur encodé à chaud et enregistrez-le dans input_data_prefix.\n","\n","+ Convertissez chaque caractère de chaque suffixe en un vecteur encodé à chaud et enregistrez-le dans input_data_suffix.\n","\n","+ Remplissez le vecteur cible qui est le même que input_data_suffix mais un pas de temps en avant."],"metadata":{"id":"sJIqvtmk68gd"}},{"cell_type":"code","source":["for i in range(len(prefix_sentences)):\n","    # Iterate over each character in each prefix\n","    for k, ch in enumerate(prefix_sentences[i]):\n","        # Convert the character to a one-hot encoded vector\n","        input_data_prefix[i, k, char_to_idx[ch]] = 1\n","        \n","    # Iterate over each character in each suffix\n","    for k, ch in enumerate(suffix_sentences[i]):\n","        # Convert the character to a one-hot encoded vector\n","        input_data_suffix[i, k, char_to_idx[ch]] = 1\n","\n","        # Target data is one timestep ahead and excludes start character\n","        if k > 0:\n","            target_data[i, k-1, char_to_idx[ch]] = 1"],"metadata":{"id":"yx-iWE017FZe","executionInfo":{"status":"ok","timestamp":1672938835437,"user_tz":-60,"elapsed":214,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["###**Autocomplétion de phrase à l’aide d’Encoder-Decoder**\n","\n","####**1. Autocomplétion de phrase à l’aide d’Encoder-Decoder**\n","+ Dans la leçon précédente, vous avez découvert le jeu de données de messagerie Enron et réduit la tâche de saisie semi-automatique de phrase à une tâche de génération de séquence à séquence en divisant chaque phrase d’e-mail en préfixes et suffixes. \n","\n","+ Vous avez également créé les vecteurs d’entrée et de cible à l’aide de ces préfixes et suffixes. \n","\n","+ Dans cette leçon, nous allons construire un modèle suivant l’architecture codeur-décodeur et l’entraîner à l’aide des vecteurs d’entrée et cible.\n","\n","####**2. Architecture du décodeur Encoder**\n","+ Rappelez-vous l’architecture codeur-décodeur du dernier chapitre. \n","\n","+ ***Le codeur accepte les séquences d’entrée et résume les informations dans ses vecteurs d’état internes qui sont utilisés dans le décodeur comme états initiaux***. \n","\n","+ ***L’encodeur est implémenté à l’aide de LSTM et les états se réfèrent donc aux états cachés et cellulaires de la couche LSTM***. \n","\n","+ L’encodeur apprend ces états à partir des séquences d’entrée. \n","\n","+ Intuitivement, les états consolident toutes les informations utiles des séquences d’entrée nécessaires pour générer les séquences de sortie. \n","\n","+ La sortie de l’encodeur est ignorée. \n","\n","+ Le décodeur produit la séquence de sortie. \n","\n","+ Les états finaux du décodeur sont ignorés. \n","\n","+ Pendant l’entraînement, les entrées au décodeur sont les séquences cibles. \n","\n","+ Pendant l’inférence, l’entrée à chaque pas de temps est la sortie prévue de l’étape précédente.\n","\n","####**3. Décodeur d’encodeur pour l’auto-complétion de phrase**\n","+ Pour l’auto-complétion de phrase, l’architecture codeur-décodeur ressemble à ceci pendant l’entraînement. \n","\n","+ L’encodeur prend les préfixes en entrée et résume les informations dans ses vecteurs d’état qui sont transmis au décodeur en tant qu’états initiaux. \n","\n","+ Ces vecteurs d’état consolident toutes les informations utiles des séquences de préfixes qui sont nécessaires dans le décodeur pour générer les phrases de suffixe. \n","\n","+ Le décodeur prend les suffixes comme entrée. Les séquences cibles du décodeur seront les suffixes, mais elles auront une longueur d’avance et sauteront le premier caractère.\n","\n","####**4. Encodeur pour l’auto-complétion de phrase**\n","+ Maintenant, construisons le réseau codeur-décodeur en utilisant Keras. \n","\n","+ Créez une couche d’entrée suivie d’une couche LSTM de 256 unités. \n","\n","+ Chaque vecteur d’entrée serait de la même taille que le vocabulaire. \n","\n","+ La première dimension de l’entrée est Aucun, ce qui indique qu’elle peut prendre un nombre variable de séquences d’entrée au moment de l’exécution. \n","\n","+ Cette entrée est ensuite transmise à la couche LSTM pour produire les vecteurs de sortie et d’état. \n","\n","+ Ignorez la sortie et combinez les états masqués et de cellule.\n","\n","####**5. Décodeur pour l’auto-complétion de phrase**\n","+ Pour construire le décodeur, créez d’abord une couche d’entrée similaire à l’encodeur. \n","\n","+ Ensuite, créez une couche LSTM de 256 unités. \n","\n","+ L’état initial de la couche LSTM est l’état final du codeur. \n","\n","+ Les états de sortie du décodeur sont ignorés, c’est pourquoi nous ne les enregistrons pas. \n","\n","+ Ensuite, créez un calque dense avec l’activation softmax. \n","\n","+ La couche dense prédit le caractère suivant et donc la taille de la couche dense sera la même que le vocabulaire. \n","\n","+ La sortie de la couche LSTM du décodeur est ensuite transmise à cette couche dense qui génère la distribution de probabilité du caractère suivant sur le vocabulaire. \n","\n","+ L’index avec la valeur de probabilité maximale est l’indice du caractère suivant le plus probable. \n","\n","+ Le caractère correspondant à cet index peut être trouvé à l’aide du mappage caractère à index.\n","\n","####**6. Combinez l’encodeur et le décodeur**\n","\n","+ Maintenant que les réseaux d’encodeur et de décodeur sont créés séparément, ils peuvent être combinés à l’aide de la fonction Model de Keras. \n","\n","+ L’entrée dans l’ensemble du modèle est la concaténation de l’entrée codeur et de l’entrée décodeur. \n","\n","+ La sortie du réseau est la sortie du décodeur. L’exactitude de l’architecture codeur-décodeur peut être vérifiée en vérifiant le résumé du modèle.\n","\n","####**7. Former le réseau**\n","+ Nous pouvons maintenant compiler le modèle à l’aide de l’optimiseur Adam et de la perte d’entropie croisée catégorielle. \n","\n","+ La fonction de perte d’entropie croisée catégorielle est utilisée lorsque nous avons plus de deux étiquettes de classe. \n","\n","+ Ici, la taille du vocabulaire nous donne le nombre d’étiquettes. \n","\n","+ Adam est un optimiseur avancé qui converge plus rapidement. Nous pouvons maintenant former ce modèle. \n","\n","+ L’entrée est la concaténation des vecteurs d’entrée. Ici, nous entraînons le modèle pour une seule époque afin de gagner du temps.\n","\n","####**8. Entraînons-nous!**\n","+ Maintenant que vous savez comment construire une architecture codeur-décodeur pour l’auto-complétion de phrases, mettons-la en pratique."],"metadata":{"id":"lveR6aK_7Xk1"}},{"cell_type":"markdown","source":["###**EXERCICE**\n","\n","####**Construire l'encodeur**\n","\n","+ Dans la dernière leçon, vous avez jeté les bases de la création d'un système de saisie semi-automatique de texte à l'aide de l'ensemble de données de messagerie Enron. \n","\n","+ Jusqu'à présent, vous avez converti l'ensemble de données d'e-mail en un ensemble de préfixes et de suffixes et créé les vecteurs d'entrée et cible correspondants. \n","\n","+ Maintenant que le jeu de données est prétraité, il est temps de construire le réseau codeur-décodeur. \n","\n","+ L'entrée du réseau est l'ensemble des préfixes et le but est de générer l'ensemble des suffixes. \n","\n","+ Le codeur prendra les préfixes en entrée et les résumera dans les vecteurs d'état internes qui seront utilisés dans le décodeur comme état initial.\n","\n","+ *Vous allez d'abord construire l'encodeur à l'aide de Keras. Les couches Input, LSTM et Dense sont déjà importées depuis tensorflow.keras.layers. Le vocabulaire est enregistré dans vocabulaire.*\n","\n","####**Des instructions**\n","\n","+ Créez le calque d'entrée.\n","+ Créez la couche LSTM de 256 unités.\n","+ Transmettez l'entrée à la couche LSTM et obtenez la sortie.\n","+ Enregistrer les états de l'encodeur."],"metadata":{"id":"FNvM-57bHGyk"}},{"cell_type":"code","source":["from tensorflow.keras.layers  import Input, LSTM, Dense"],"metadata":{"id":"EV8N9BWgIQ5V","executionInfo":{"status":"ok","timestamp":1672943016190,"user_tz":-60,"elapsed":605,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Create the input layer of the encoder\n","encoder_input = Input(shape=(None, len(vocabulary)))\n","\n","# Create LSTM Layer of size 256\n","encoder_LSTM = LSTM(256, return_state = True)\n","\n","# Save encoder output, hidden and cell state\n","encoder_outputs, encoder_h, encoder_c = encoder_LSTM(encoder_input)\n","\n","# Save encoder states\n","encoder_states = [encoder_h, encoder_c]"],"metadata":{"id":"O5woL57b32Ym","executionInfo":{"status":"ok","timestamp":1672942298889,"user_tz":-60,"elapsed":1232,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["encoder_states "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7rA89bDoIb4j","executionInfo":{"status":"ok","timestamp":1672942309025,"user_tz":-60,"elapsed":402,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"6679e532-5f4c-4fec-f45b-860cad497b12"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>,\n"," <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>]"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["####**Construire le décodeur**\n","+ Dans l'exercice précédent, vous avez construit l'encodeur. \n","\n","+ Vous allez construire le décodeur maintenant. \n","\n","+ ***Rappelons que l'entrée du réseau codeur-décodeur est l'ensemble des préfixes et que le but est de générer l'ensemble des suffixes***. \n","\n","+ ***Le codeur traite les séquences de préfixes et résume les informations dans ses vecteurs d'état internes***. \n","\n","+ ***Le décodeur prend les états initiaux du codeur, et sa sortie est l'ensemble des séquences cibles qui sont les suffixes dans ce cas***. \n","\n","+ Pendant la formation, l'entrée du décodeur sera également les phrases suffixes en raison du forçage de l'enseignant.\n","\n","+ *Vous construirez le décodeur à l'aide de Keras. Les couches Input, LSTM et Dense sont déjà importées depuis tensorflow.keras.layers. Le vocabulaire est enregistré dans vocabulaire.*\n","\n","####**Des instructions**\n","\n","+ Créez une couche d'entrée suivie d'une couche LSTM de 256 unités.\n","+ Passez decoder_input et encoder_states dans decoder_LSTM() pour obtenir la sortie.\n","+ Créez le calque Dense avec l'activation 'softmax'.\n","+ Passez la sortie LSTM du décodeur à la couche dense pour générer la sortie."],"metadata":{"id":"Yv7DhAtBIluT"}},{"cell_type":"code","source":["# Create decoder input layer\n","decoder_input = Input(shape=(None, len(vocabulary)))\n","\n","# Create LSTM layer of size 256\n","decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n","\n","# Save decoder output\n","decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n","\n","# Create a `Dense` layer with softmax activation\n","decoder_dense = Dense(len(vocabulary),activation='softmax')\n","\n","# Save the decoder output\n","decoder_out = decoder_dense(decoder_out)"],"metadata":{"id":"jAEvhfJtI2VT","executionInfo":{"status":"ok","timestamp":1672942637069,"user_tz":-60,"elapsed":2149,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["####**Former l'encodeur et le décodeur**\n","+ Dans les deux exercices précédents, vous avez créé les réseaux Encodeur et Décodeur séparément. \n","\n","+ Dans cet exercice, vous les combinerez à l'aide de l'API fonctionnelle Keras pour créer l'ensemble de l'architecture Encoder-Decoder. \n","\n","+ Ensuite, vous allez compiler et entraîner le réseau à l'aide des vecteurs d'entrée et cible que vous avez créés précédemment.\n","\n","+ *Vous avez les données d'entrée disponibles dans les vecteurs input_data_prefix et input_data_suffix respectivement. Vous avez également les données cibles disponibles dans le vecteur target_data. Les couches d'entrée de l'encodeur et du décodeur sont enregistrées dans encoder_input et decoder_input. La couche de sortie du décodeur est enregistrée dans decoder_out.*\n","\n","####**Des instructions**\n","+ Créez le modèle d'encodeur-décodeur à l'aide de Model() de keras.\n","+ Compilez le modèle en utilisant l'optimiseur 'adam' et la perte 'categorical_crossentropy'.\n","+ Imprimer le résumé du modèle.\n","+ Entraînez le modèle pour 1 époque avec une taille de lot de 64 et une répartition de validation de 0,2."],"metadata":{"id":"tDIdMIlFKX_U"}},{"cell_type":"code","source":["from tensorflow.keras import Model"],"metadata":{"id":"4-w88ZvHLUJD","executionInfo":{"status":"ok","timestamp":1672943062270,"user_tz":-60,"elapsed":309,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# Build model\n","model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","\n","# Print model summary\n","model.summary()\n","\n","# Fit the model\n","model.fit(x=[input_data_prefix, input_data_suffix], y=target_data,\n","          batch_size=64, epochs=1, validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AtomT_mdI2Ye","executionInfo":{"status":"ok","timestamp":1672943150452,"user_tz":-60,"elapsed":86029,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"daf714b9-ca8f-41be-a491-36ba75275f55"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, None, 47)]   0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, None, 47)]   0           []                               \n","                                                                                                  \n"," lstm (LSTM)                    [(None, 256),        311296      ['input_1[0][0]']                \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, None, 256),  311296      ['input_2[0][0]',                \n","                                 (None, 256),                     'lstm[0][1]',                   \n","                                 (None, 256)]                     'lstm[0][2]']                   \n","                                                                                                  \n"," dense (Dense)                  (None, None, 47)     12079       ['lstm_1[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 634,671\n","Trainable params: 634,671\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","71/71 [==============================] - 66s 879ms/step - loss: 1.1719 - val_loss: 1.0509\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fce94536f40>"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["###**Saisie semi-automatique des phrases à l’aide de modèles d’inférence**\n","\n","####**1. Saisie semi-automatique des phrases à l’aide de modèles d’inférence**\n","\n","+ *Dans la dernière leçon, vous avez formé un réseau d’encodeur-décodeur pour l’auto-complétion de phrases. Maintenant, utilisons ce modèle entraîné pour les prédictions. Ici, nos entrées seront des phrases incomplètes ou des préfixes et les prédictions seront le suffixe qui complète la phrase.*\n","\n","####**2. Décodeur encodeur pendant l’inférence**\n","\n","+ Rappelez-vous l’architecture codeur-décodeur de la dernière leçon. \n","\n","+ ***L’encodeur accepte les préfixes et résume les informations dans ses vecteurs d’état internes qui sont ensuite utilisés dans le décodeur comme états initiaux.*** \n","\n","+ ***L’encodeur et le décodeur sont implémentés à l’aide de LSTM et ces états font référence aux états cachés et cellulaires des couches LSTM du codeur et du décodeur***. \n","\n","+ Les états de codage représentent les informations consolidées des préfixes qui seront nécessaires dans le décodeur pour générer les phrases de suffixe. \n","\n","+ La sortie de l’encodeur est ignorée. \n","\n","+ Le décodeur émet les suffixes. \n","\n","+ Les états finaux du décodeur sont ignorés. \n","\n","+ Pendant l’inférence, l’entrée du décodeur à chaque pas de temps est la sortie prévue de l’étape précédente.\n","\n","####**3. Modèle d’inférence pour le codeur**\n","\n","+ L’encodeur se comporte de la même manière pendant l’entraînement et l’inférence. \n","\n","+ Le modèle d’inférence du codeur se compose de la couche d’entrée et des vecteurs d’état du modèle entraîné.\n","\n","####**4. États initiaux du modèle d’inférence du décodeur**\n","\n","+ Le décodeur requiert les états masqué et cellulaire du modèle d’inférence du codeur comme état initial. \n","\n","+ Ces états seront fournis en entrée au modèle et, par conséquent, doivent être initialisés en tant qu’entrées. Nous pouvons combiner ces états comme indiqué.\n","\n","####**5. Sortie du modèle d’inférence du décodeur**\n","\n","+ Le décodeur est appelé récursivement pour chaque caractère à générer dans la séquence de suffixes. \n","\n","+ Lors du premier appel, les états cachés et de cellule de l’encodeur seront utilisés pour initialiser les états du décodeur LSTM qui générera la sortie et les états cachés et cellulaires. \n","\n","+ Ces états seront utilisés pour générer le caractère suivant dans l’itération suivante. \n","\n","+ La sortie du décodeur LSTM est à nouveau transmise à la couche dense pour obtenir le caractère prédit.\n","\n","####**6. Modèle d’inférence pour le décodeur**\n","+ Nous pouvons combiner tout cela ensemble en utilisant la fonction Model de Keras pour construire le modèle d’inférence de décodeur. \n","\n","+ L’entrée du modèle est la couche d’entrée du décodeur et les états d’entrée du décodeur. \n","\n","+ La sortie du modèle est le caractère prédit ainsi que les états de sortie du décodeur.\n","\n","####**7. Prédiction à l’aide de modèles d’inférence**\n","+ Maintenant que nous avons construit les modèles d’inférence, nous pouvons les utiliser pour générer un suffixe étant donné un préfixe en entrée. \n","\n","+ Prenons une séquence de préfixes aléatoire et introduisons-la dans le modèle d’inférence de l’encodeur qui produira les états internes de l’encodeur. \n","\n","+ Définissez une variable pour le suffixe à générer et initialisez-la pour contenir le jeton de démarrage qui dans notre cas est le caractère de tabulation.\n","\n","####**8. Générez le premier caractère**\n","\n","+ Transmettez la séquence cible et les valeurs d’état actuel au modèle d’inférence du décodeur pour générer la sortie avec les valeurs d’état. \n","\n","+ Le résultat est une distribution de probabilité pour le caractère suivant sur le vocabulaire. \n","\n","+ Découvrez l’indice avec la probabilité maximale. Le caractère suivant est le caractère auquel cet index est mappé.\n","\n","####**9. Générez le deuxième caractère**\n","\n","+ Ce premier caractère peut être réinjecté dans le modèle d’inférence du décodeur. \n","\n","+ Mettez à jour la séquence cible avec ce nouveau personnage. \n","\n","+ Enregistrez les valeurs d’état du dernier pas de temps. \n","\n","+ Transmettez cette séquence mise à jour et les nouvelles valeurs d’état au modèle d’inférence du décodeur qui produira le caractère suivant le plus probable.\n","\n","####**10. Saisie semi-automatique des phrases**\n","+ Nous pouvons continuer à le faire en utilisant une boucle jusqu’à ce qu’un caractère de nouvelle ligne soit trouvé qui est le jeton de fin et signale la fin de la séquence de sortie. \n","\n","+ Nous pouvons garder une trace des caractères générés dans le processus et les enregistrer pour former une chaîne qui n’est rien d’autre que la séquence de suffixes. \n","\n","+ Il est préférable de déplacer ce bloc de code à l’intérieur d’une fonction qui prend une phrase incomplète ou un préfixe comme entrée et génère la phrase suffixe qui complète la phrase d’entrée donnée en sortie. \n","\n","+ La fonction peut être utilisée comme une boîte noire pour l’auto-complétion de phrase.\n","\n","####**11. Entraînons-nous!**\n","+ Super travail! Vous savez maintenant comment utiliser les modèles d’inférence codeur-décodeur pour compléter automatiquement les phrases. Il est temps pour vous de pratiquer."],"metadata":{"id":"x2aTpPWELrxY"}},{"cell_type":"markdown","source":["####**Construire les modèles d'inférence**\n","\n","+ Vous êtes maintenant prêt à produire du texte complété automatiquement pour vos données d'entrée. \n","\n","+ Jusqu'à présent, vous avez créé un modèle d'encodeur-décodeur et l'avez entraîné à l'aide de l'ensemble de données d'e-mail prétraité. \n","\n","+ Tout au long de cette leçon, vous utiliserez le modèle formé pour générer une séquence de caractères suivants à partir d'une phrase incomplète en entrée.\n","\n","+ Dans un premier temps, vous allez créer les modèles d'inférence d'encodeur et de décodeur dans cet exercice. \n","\n","+ Les variables encoder_input, encoder_states, decoder_input sont utilisables et contiennent le tenseur d'entrée du codeur, les états internes du codeur et le tenseur d'entrée du décodeur.\n","\n","####**Des instructions**\n","+ Utilisez la fonction Model() de keras pour créer le modèle d'inférence Encoder.\n","+ Utilisez Input() pour définir les états initiaux du décodeur de dimension 256.\n","+ Combinez les états en un seul vecteur.\n","+ Utilisez decoder_LSTM() pour obtenir la sortie de Decoder LSTM.\n","+ Utilisez la couche decoder_dense() pour obtenir la sortie finale.\n","+ Utilisez Model() pour créer un modèle d'inférence de décodeur."],"metadata":{"id":"Yx-kZtLnNL83"}},{"cell_type":"code","source":["# Create encoder inference model\n","encoder_model_inf = Model(encoder_input, encoder_states)\n","\n","# Create decoder input states for inference\n","decoder_state_input_h = Input(shape=(256,))\n","decoder_state_input_c = Input(shape=(256,))\n","decoder_input_states = [decoder_state_input_h, decoder_state_input_c]"],"metadata":{"id":"xd30xqnaM40Q","executionInfo":{"status":"ok","timestamp":1672943892625,"user_tz":-60,"elapsed":897,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# Create encoder inference model\n","encoder_model_inf = Model(encoder_input, encoder_states)\n","\n","# Create decoder input states for inference\n","decoder_state_input_h = Input(shape=(256,))\n","decoder_state_input_c = Input(shape=(256,))\n","decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n","\n","# Get decoder output and feed it to the dense layer for final output prediction\n","decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, initial_state=decoder_input_states)\n","decoder_states = [decoder_h , decoder_c]\n","decoder_out = decoder_dense(decoder_out)\n","\n","# Create decoder inference model\n","decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states, outputs=[decoder_out] + decoder_states )"],"metadata":{"id":"gxCHWnmEM44U","executionInfo":{"status":"ok","timestamp":1672943895943,"user_tz":-60,"elapsed":1009,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["####**Prédire le premier caractère à l'aide de modèles d'inférence**\n","\n","+ Maintenant que vos modèles d'inférence pour l'encodeur et le décodeur sont prêts, vous pouvez les utiliser pour les prédictions. \n","\n","+ Vous allez générer la phrase suffixe caractère par caractère. \n","\n","+ Chaque fois que vous fournissez un préfixe au modèle d'inférence, il proposera le caractère suivant le plus probable pour ce préfixe. \n","\n","+ Dans cet exercice, vous allez saisir une phrase incomplète dans le modèle d'inférence de l'encodeur et le jeton de début dans le modèle d'inférence du décodeur et générer le caractère suivant dans la séquence.\n","\n","+ *Les modèles d'inférence d'encodeur et de décodeur sont disponibles respectivement dans encoder_model_inf et decoder_model_inf. Le vecteur d'entrée de préfixe, le vocabulaire, les correspondances caractère-entier et entier-caractère sont disponibles dans input_data_prefix, vocabulaire, char_to_idx et idx_to_char.*\n","\n","####**Des instructions**\n","+ Utilisez .predict() sur encoder_model_inf pour obtenir les états internes de l'encodeur.\n","+ Utilisez .predict() sur decoder_model_inf pour obtenir la sortie du décodeur.\n","+ Obtenez le caractère suivant le plus probable à l'aide de l'index vers le mappage de caractères."],"metadata":{"id":"ykGJ_YuTNphJ"}},{"cell_type":"code","source":["# Pass input prefix to the Encoder inference model and get the states\n","inp_seq = input_data_prefix[4:5]\n","states_val = encoder_model_inf.predict(inp_seq)\n","\n","# Seed the first character and get output from the decoder \n","target_seq = np.zeros((1, 1, len(vocabulary)))\n","target_seq[0, 0, char_to_idx['\\t']] = 1  \n","decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n","\n","# Find out the next character from the Decoder output\n","max_val_index = np.argmax(decoder_out[0,-1,:])\n","sampled_suffix_char = idx_to_char[max_val_index]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JzLrBucnNzdH","executionInfo":{"status":"ok","timestamp":1672943904610,"user_tz":-60,"elapsed":1232,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"ab1662f5-f5bf-454d-d04d-6f4ed84ed50e"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 515ms/step\n","1/1 [==============================] - 0s 479ms/step\n"]}]},{"cell_type":"code","source":["# Print the first character\n","sampled_suffix_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"RL5uiyWwN9DH","executionInfo":{"status":"ok","timestamp":1672943911983,"user_tz":-60,"elapsed":312,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"06af73b4-0a02-44a6-8509-15d0046bd1e8"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["####**Prédire le deuxième personnage**\n","+ Dans l'exercice précédent, vous avez vu comment générer le premier caractère de la phrase de suffixe à l'aide d'une phrase de préfixe incomplète. \n","\n","+ Vous pouvez ajouter ce caractère généré à la séquence de sortie existante et le renvoyer au modèle d'inférence du décodeur pour générer le deuxième caractère de la séquence. \n","\n","+ C'est ce que vous ferez dans cet exercice. \n","\n","+ Ce processus peut être répété jusqu'à ce que vous obteniez le jeton de fin pour générer une phrase de suffixe complète. \n","\n","+ La phrase de suffixe sert de suggestion de saisie semi-automatique pour la phrase de préfixe donnée.\n","\n","+ *Comme précédemment, les modèles d'inférence d'encodeur et de décodeur sont disponibles respectivement dans encoder_model_inf et decoder_model_inf. Le vocabulaire et le mappage entier-caractère sont disponibles dans vocabulaire et idx_to_char.*\n","\n","####**Des instructions**\n","\n","+ Utilisez .predict() sur le modèle d'inférence du décodeur pour générer la sortie.\n","+ Obtenez le caractère suivant le plus probable à l'aide de np.argmax().\n","+ Récupère le caractère correspondant à l'index du caractère suivant le plus probable."],"metadata":{"id":"1_m4CrbZOGV4"}},{"cell_type":"code","source":["# Insert the generated character from last time to the target sequence \n","target_seq = np.zeros((1, 1, len(vocabulary)))\n","target_seq[0, 0, max_val_index] = 1\n","\n","# Initialize the decoder state to the states from last iteration\n","states_val = [decoder_h, decoder_c]\n","\n","# Get decoder output\n","decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n","\n","# Get most probable next character and print it.\n","max_val_index = np.argmax(decoder_out[0,-1,:])\n","sampled_suffix_char = idx_to_char[max_val_index]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzlX8d2wOQ6w","executionInfo":{"status":"ok","timestamp":1672943916901,"user_tz":-60,"elapsed":313,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"4d160994-acce-4b28-fc25-d705af466355"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 22ms/step\n"]}]},{"cell_type":"code","source":["sampled_suffix_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"rqKOAKLsOYgt","executionInfo":{"status":"ok","timestamp":1672943920188,"user_tz":-60,"elapsed":219,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"a63ad396-1a79-4340-f017-a1e0fed45f70"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["####**Phrases à saisie semi-automatique**\n","+ Vous savez maintenant comment générer le caractère suivant le plus probable dans l'itération en cours en utilisant le caractère généré à partir de la dernière itération. \n","\n","+ Vous pouvez poursuivre ce processus en boucle jusqu'à ce que le jeton de fin soit trouvé ou que la longueur maximale de la séquence de suffixes soit atteinte. \n","\n","+ Une fonction generate_suffix_sentence() est disponible pour vous et fait exactement cela. \n","\n","+ Il prend une phrase de préfixe en entrée, génère la phrase de suffixe caractère par caractère et la renvoie.\n","\n","+ *En outre, un modèle formé pour 20 époques sur un ensemble de données plus important est chargé dans l'environnement pour démontrer comment cette architecture simple peut être utilisée pour l'auto-complétion des phrases. Une architecture plus complexe avec plus de couches cachées produira des résultats plus précis.*\n","\n","####**Des instructions**\n","+ 1) \n","  + Itérez 10 fois pour générer 10 phrases suffixes en boucle :\n","  + Générez la phrase suffixe.\n","\n","+ 2)\n","####**Question**\n","+ N'hésitez pas à expérimenter avec generate_suffix_sentence() pour générer de nombreuses auto-complétions de phrases. \n","+ Comme vous pourrez le voir, bien que les suffixes prédits ressemblent à de vraies phrases, ils ne sont pas tout à fait la bonne fin du préfixe d'entrée.\n","\n","+ *Comment pouvez-vous améliorer la précision du modèle ?*\n","\n","####**Des réponses possibles**\n","\n","+ Augmentez la complexité du modèle.\n","\n","+ Entraînez-vous pour plus d'époques.\n","\n","+ Entraînez-vous avec un plus grand ensemble de données.\n","\n","+ ***Tout ce qui précède.***"],"metadata":{"id":"4Bc9vWAzPETI"}},{"cell_type":"code","source":["def generate_suffix_sentence(inp_seq):\n","\n","    # Initialize states value to the final states of the encoder\n","    states_val = encoder_model_inf.predict(inp_seq)\n","\n","    # Initialize the target sequence to contain the start token\n","    target_seq = np.zeros((1, 1, len(vocabulary)))\n","    target_seq[0, 0, char_to_idx['\\t']] = 1\n","\n","    # Define a variable to store the suffix sentence\n","    suffix_sent = ''\n","\n","    # Define stop condition flag\n","    stop_condition = False\n","\n","    # Iterate until the end token is found or maximum length of the suffix sentence is reached\n","    while not stop_condition:\n","\n","        # Get output from decoder inference model\n","        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n","\n","        # Get most probable next character\n","        max_val_index = np.argmax(decoder_out[0,-1,:])\n","        sampled_output_char = idx_to_char[max_val_index]\n","\n","        # Append the generated char to the suffix sentence\n","        suffix_sent += sampled_output_char\n","\n","        # Check if end token is encountered or maximum length of the suffix sentence is exceeded\n","        if ((sampled_output_char == '\\n') or (len(suffix_sent) > max_len_suffix_sent)) :\n","            stop_condition = True\n","\n","        # Add the new generated char to the existing target sequence\n","        target_seq = np.zeros((1, 1, len(vocabulary)))\n","        target_seq[0, 0, max_val_index] = 1\n","\n","        # Save state values to use in the next iteration\n","        states_val = [decoder_h, decoder_c]\n","\n","    # Return the suffix sentence\n","    return suffix_sent\n"],"metadata":{"id":"kYnKQy6fPZZi","executionInfo":{"status":"ok","timestamp":1672944245328,"user_tz":-60,"elapsed":659,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# Generate 10 suffixes\n","for seq_index in range(10):\n","  \n","    # Get the next tokenized sentence\n","    inp_seq = input_data_prefix[seq_index:seq_index+1]\n","    \n","    # Generate the suffix sentence\n","    suffix_sent = generate_suffix_sentence(inp_seq)\n","    \n","    # Print the prefix sentence\n","    print('Prefix Sentence:', prefix_sentences[seq_index])\n","    \n","    # Print the suffix sentence\n","    print('Suffix Sentence:', suffix_sent)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NExIIVxJPelC","executionInfo":{"status":"ok","timestamp":1672944341456,"user_tz":-60,"elapsed":90334,"user":{"displayName":"etienne koa","userId":"01067828653934976478"}},"outputId":"c40dafa7-05f0-4f7f-a36a-4efcc652e3f9"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","Prefix Sentence:  \n","Suffix Sentence:                                                                                                    \n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","Prefix Sentence:  a\n","Suffix Sentence:                                                                                                    \n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","Prefix Sentence:  ar\n","Suffix Sentence:                                                                                                    \n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","Prefix Sentence:  are\n","Suffix Sentence:                                                                                                    \n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 27ms/step\n","Prefix Sentence:  are \n","Suffix Sentence:                                                                                                    \n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","Prefix Sentence:  are w\n","Suffix Sentence:                                                                                                    \n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","Prefix Sentence:  are we\n","Suffix Sentence:                                                                                                    \n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 33ms/step\n","Prefix Sentence:  are we \n","Suffix Sentence:                                                                                                    \n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 27ms/step\n","Prefix Sentence:  are we g\n","Suffix Sentence:                                                                                                    \n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","Prefix Sentence:  are we go\n","Suffix Sentence:                                                                                                    \n"]}]}]}